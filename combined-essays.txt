A HYDRA MADE OF COAL


“One observer writing in the 1830s called this new class of workers “but a Hercules in the cradle,” brought together into dense masses by the steam engine...”
-Barbara Freese, quoting Elizabeth Gaskell

“What would have become of Hercules do you think if there had been no lion, hydra, stag or boar - and no savage criminals to rid the world of? What would he have done in the absence of such challenges?


Obviously he would have just rolled over in bed and gone back to sleep. So by snoring his life away in luxury and comfort he never would have developed into the mighty Hercules.”
 -Epicticus
Safety After the End of History
From 1900 to 2017, the fatality rate in the American coal mining industry fell by 97%.[1] How was this change possible? How did it affect society? Will it continue?


Despite a neverending supply of new hazards, the world  of commerce[a] has become increasingly safe. This can be partly explained by advances in technology, legislation, and/or wealth. However, these explanations favor macro-level factors to the detriment of the human element. 


The missing part of the story: workplace safety protocols – constraints set on human behavior in order to reduce work-related injury, disease, and death. These protocols have emerged, evolved, and expired in step with changes in the industrial environment. Using the coal mining industry as a case study, I propose i) a theory of protocolization (figure 1) and ii) a theory of protocol evolution (figure 2).[2]
i)   ii)  


Why coal? By venturing into the coal pits, we kicked off a world-changing process. On the one hand, coal “would raise up not only our civilization but our very souls.”[3] and on the other hand, coal “helped create a new kind of savage existence not controlled by nature but virtually severed from it.”[4] 


Human ingenuity has solved problem after problem in the coal mining industry. But these solutions create their own problems. Technological advances in coal extraction “reduced some risks while increasing others”.[5] Like how the electrification of coal-cutting machinery reduced the number of dangerous mechanical parts but increased the risk of electrocution.[6] Risks like that were perhaps predictable, but at many points we did not know – nor could we afford to care – what we were getting into. How were early coal miners supposed to know that burning coal would lead to the accumulation of atmospheric carbon dioxide? How could 18th-century laborers have avoided fatal levels of air pollution if their survival depended on burning coal for warmth? [7] 


The effects of technologies are unpredictable. It’s easy to anticipate some of the health and safety problems (hazards) that they eliminate and create, but it’s impossible to predict all of them. Our predictions can either contain errors, be ignorant, or be superseded by our more immediate interests in survival or profit.[8] It takes time to uncover new hazards through experience, knowledge, or shifting cultural norms. The mix of hazards that we must mitigate continually waxes and wanes in different directions. To deal with this unstable mix, humans need a reliable source of short-term solutions.


Protocols emerge as a first line of defense against newfound hazards. To do so, protocols limit the scope of human behavior in some way. Social distancing in the office during a pandemic, for example, requires no technology[b], significantly limits human behavior, and reduces workplace injuries and fatalities. Norms, distributions of authority, and hierarchies emerge to enforce such constraints – resulting in a new ‘protocol-based social order’[9]. Organizations built upon dangerous technology have clear, protocol-based social orders. Nuclear power plants, commercial airlines, and railroad companies[10] are all built upon powerful technologies, which require protocols, which often require a change to the social order. Atom-splitting (nuclear) technology begets a centralized, authoritarian social order to tightly control its downside risks.[11]


We have been in a centuries-long battle against the externalities of rapidly evolving technology. Over time, the protocolization of the coal industry led to the near-total elimination of fatal workplace incidents. This success is a testament to the power of protocol. 


In the conclusion of the essay, I make a case that we are overdue for a shift of focus in the area of safety. There are diminishing returns to safety protocols. Several strong cases have been made arguing that the pursuit of absolute safety is economically unoptimal[12] and morally suspect[13]. 


I believe the obsession with an antiquated view of workplace safety is costing many people their lives. There is an absence of workplace health protocols that address the top causes of workplace fatalities in the 21st century: Chronic Obstructive Pulmonary Disorder, Ischaemic Heart Disease, and Stroke.[14] After the end of history of safety, health should be the new focus.


Key Points
1. Technological progress is the main driver of protocol creation and destruction; technology and protocols are entangled at a deep level.
2. Protocols evolve via meme-like replication errors and environmental selection pressures. 
3. We are approaching the “end of history” for safety – health should be the new focus.
4. Protocol tinkering is more effective than protocol design.
5. Safety protocols often decrease the safety of an adjacent outgroup.
6. There are diminishing – and eventually negative – returns to safety protocols.
7. There is a specific set of environmental factors that allow safety protocols to compete, including wealth, low social tolerance for risk displacement, and a dense social network.
Unanswered Questions
1. Is there a perceived decline in safety? What are the implications of that?
2. How quickly do we adapt our expectations when objective levels of safety increase?
3. What are the negative externalities of health protocols? How can we avoid risk displacement?
4. Is worker insurance a protocol?
5. Could, and should, liability for workers’ long-term health rest with heir employer(s)?
6. What is the nature of the entanglement between protocols and technology?
Safety as a Dynamic[c] Non-Event[15]
Our brain has finite bandwidth. To deal with limited memory and processing powers, we get used to things going well. So, over time, we only notice when things go wrong – when there is an ‘event’. We do not notice the ‘non-events’ (i.e. the status quo, situation normal, the day-to-day). But events and non-events are both consequences of human actions. No actions, no events. Action performance varies, putting the ‘dynamic’ in “dynamic non-event”. Safety is a dynamic non-event. It can be difficult to talk about safety (and health) because safety is an absence of events, not an event itself.


When you use a stapler it is highly unlikely that you will stick yourself. In safety terms, stapling yourself is the event. Stapling the paper without stapling yourself is the non-event. The risk of an event is always there – you might even get really close to stapling your own thumb to that freshly printed pdf. Hence, the action is dynamic; you never do it precisely the same. But you probably won’t notice these near misses[d]. We only detect outcomes that fall below the “limit of unacceptable performance”[16] (see figure 3).


We can count how many times we’ve stapled ourselves by accident (the event), but we can’t count how many times we haven’t stapled ourselves (the dynamic non-event). The process remains above the limit of unacceptable performance – and therefore our awareness – due to “moment-to-moment adjustments and compensations”[17] thanks to you, the operator of the stapler. Ultimately, you don’t notice when you’re safe.
  

For that reason, the benefits of safety protocols are often taken for granted. We tend to pay attention to the cost of following protocol. It can be tiresome and appear inefficient. This is because the benefits – the dynamic non-events that safety protocols create and sustain – fall below our radar. 


Safety protocols don’t just benefit individuals. They also benefit peers and groups. Reducing the rate of disease and injuries reduces the societal burden of grief [citation], improves our ability to create knowledge, and increases economic productivity. In other words, “The less energy expended merely in keeping a society alive, the more is available for change…”[18]


A few examples of workplace safety protocols, and their individual and emergent benefits:






Mining Safety Protocol
	Individual Benefit
	Emergent Group Benefit
	Group meeting and risk review before entering mines[19]
	Increased knowledge of risks and how to avoid them
	Reduced chance of one member compromising group safety
	Annual Mine Emergency Response Development exercise[20]
	Faster and better response to well-known types of mining emergencies
	Reduces the total harm in the case of an emergency
	Reporting workplace accidents and near misses[21]
	Root cause of the incident is fixed
	Enhanced ability to allocate investments 
	Proactively alerting coworkers of your presence
	Many potential accidents (collision, exposure) are averted
	Operations are uninterrupted due to lost time
	Using signs to indicate the presence of a hazard
	Worker can rely less on memory
	First-timers know to avoid area
	Rotating inspection and monitoring duties
	Workers spend less time on cognitively draining tasks[22]
	Performance goes up as a result of heightened attentiveness[23]
	

Reporting accidents and near misses improves the safety of a worker by being the first step to eliminating a hazard from their workplace. The emergent effect is the accumulation of data that can be used to pool investments and direct them toward preventing the top causes of worker fatalities, such as chronic obstructive pulmonary disorder. 


Safety protocols are also a part of everyday life:


Safety Protocol
	Individual Benefit
	Emergent Group Benefit
	Looking both ways before crossing a street or railroad
	Potential collisions are averted
	Traffic flow is uninterrupted; infrastructure is undamaged
	Hiking in a group rather than hiking solo
	Injuries affecting mobility are less likely to result in death
	Decreased cost of search and rescue operations
	Using “bear bins” to while camping.
	Reduced risk of bear encounters
	Avoids sensitizing bears to human presence
	

These protocols all directly benefit individuals. But they have positive emergent effects on safety as well. Using bear bins keeps bears from coming to your campsite and ruining your trip. This is great – and it’s a public service![e] Bears that have eaten human food are more likely to harass future campers. By preventing an initial encounter, you are improving the safety of campers whom you might never meet.


Health vs. Safety
Occupational health and safety are often bunched together under the acronym OHS. But why aren’t public safety and public health bunched together? Health, like safety, is a dynamic non-event. Why do we talk about health and safety separately in our day-to-day lives?


The most obvious difference between safety and health is the timeline in question. A coal miner not using a hardhat is doing something unsafe. A coal miner inhaling silica dust is doing something unhealthy. A rock wall won’t gradually fall onto one’s head over the course of 10 years. It happens in a moment. Inhaling silica dust won’t kill you instantly, but the damage is cumulative. 


In both situations, the long run outcome is the same – fatal. Long enough exposure to either unsafety or unhealth will result in the death. Safety is about avoiding risks of instant injury or death. Health is about avoiding cumulative damage. Being unsafe is like having a pet rattlesnake. Being unhealthy is like having a stressful job. 


Health is also about avoiding cumulative risk factors that increase your overall chances of mortality (such as muscle loss, high blood pressure, insulin resistance, obesity, smoking, a sedentary lifestyle, etc.)[24]. In that sense, safety and health overlap. Healthier = safer. Getting injured can destroy your ability to lead a healthy lifestyle. So, safer = healthier.[25]


The same memory mechanics that makes safety (and non-events in general) difficult to notice are even more pertinent when it comes to health. Because health often deteriorates slowly, it’s hard to be aware of changes. The limit of acceptable performance drifts downward as we adjust our expectations to new average levels of wellbeing. 


Because the benefits of health and safety protocols are unseen, it can be difficult for such protocols to be adopted. From my experience, we operate more proactively when the risk is obvious, probable, and has the possibility of immediate harm. 


This subtle nature of health had consequences with issues like black lung[26] and asphestos. There were several sources of friction to problem solving in both cases. First, medical knowledge takes time to develop. Second, the scientific community can be captured. Corporations funding scientific research might have an interest in suppressing certain findings, as happened with miner’s silicosis.[27] Third, causality is difficult to prove given i) long timespan between exposure and disease (asbestosis has a latency period of 15-30 years[28]), and ii) a mechanism of harm is often below the level of human perception. We can see someone lose an arm in a gearbox, but we cannot use our bare eyes to observe cancer form in someone’s lungs.


Today, we face a similar set of problems. According to the World Health Organization and the International Labor Organization, in 2016, only 25% of workplace fatalities globally were caused by accidents.[29] The rest were caused by diseases. Today, the top three causes of death globally are the same top three work-related causes of death.[30] Chronic obstructive pulmonary disease, stroke, and ischemic heart disease account for ~63.6% of work-related causes of death in 2016.[31] Long working hours alone were estimated to account for 39.5% of workplace fatalities in 2016.[32] 


Emotional Ergonomics
Human safety and health are generally understood to be physical phenomena, governed by the same laws as the world and technology. “How do we preserve our biological constitution?” is a difficult problem, but it is tractable. Safety protocols, in conjunction with technological progress, have come a long way toward solving that problem. For that reason, the United States has observed a near-total elimination of fatalities from occupational incidents (a 90% reduction from 1933-1997).[33]


However, what we consider to be safety is changing. Our standards – minimum levels of acceptable performance – have gotten higher. In about two hundred years, the bar has been raised from “don’t die” to “don’t get hurt” to “don’t burn out” to “love your job”. One of the difficulties faced by occupational health and safety professionals today is how to protocolize emotional health. Issues like depression, anxiety, and PTSD in the workplace comprise another head of the hydra. While treatments are available, Western societies do not yet have consensus on the best protocols prevent these issues[34]. 


Increasing minimum levels of acceptable performance indicate that the protocolization of industrial safety was a success. We are beginning to see a distinct shift from safety to health. A short history of the coal mining industry illustrates how protocols took us from A to B.


Case Study: The Protocolization of Safety in the Coal Mining Industry
Pre-Industrial Safety Protocols (< 1700 A.D.)
As early as the 13th century, Europeans started to mine coal in bell pits[35]. Bell pits were only a few feet wide at the mouth, then dug out wider underground – up to about 30 feet below the surface.[36] They were typically operated by a serf family, and owned by a lord.[37] It’s hypothesized that a bell pit would be excavated until the walls or roof appeared to be unstable, then it would be abandoned.[38] The family would then dig a new shaft nearby to access the same coal vein.[39] Here we have one of the first records of a workplace safety protocol, more or less: establish a sufficient distance between bell pits so that the structural integrity of one bell pit doesn’t affect the other.[40]


Drift mines were another early method of coal extraction.[41] A coal vein would be accessed from the side of a bank or cliff, rather than by a vertical shaft. This allowed a mining family to move coal with less effort. Dragging coal up or down a slope was probably safer than hauling it up a ladder. The first drift mines were uncomplicated and shallow.[42] As they were sunk deeper into the earth, coal miners began to encounter a new killer: gas. Methane is the most well-known. There were all kinds of gas. Miners at the time might not have known the chemical compounds of these gasses, but each gas was identified by how it killed, giving them a series of grim names.[43]


* Chokedamp a.k.a stythe, damp, or blackdamp (carbonic acid gas): an asphyxiant gas.
* Firedamp (methane gas / carburetted hydrogen): a highly explosive gas.
* Afterdamp (a mix of carbon monoxide, carbon dioxide, firedamp, and nitrogen): an asphyxiant and poisonous gas.
* Stinkdamp (hydrogen sulfide): a poisonous gas.


The discovery and understanding of these gasses led to new safety protocols. Chokedamp is heavier than air; miners would hold a candle near the floor. If it dimmed or went out, it was time to go. Firedamp was “solved” by preemptively igniting buildups. This was initially done by a ‘Penitent’ – a convict– who, instead of going to jail, assumed an extremely dangerous job igniting gas build-ups before the other miners went into the cave.[44] Miners also tried to find nonflammable ways of illuminating their workplaces – they “even experimented with bringing phosporescent fish”.[45] Birds, such as canaries, were used to detect Afterdamp and Stinkdamp. They would die sooner than a miner, giving the miner a heads up that there was an asphyxiant or poisonous gas. Hence, “canary in the coal mine,” was an early detection protocol.


Pre-industrial mines were powered by humans and animals. Because iron was expensive, cranks, pulleys, tools, wheels, and carts were made primarily from wood. The available materials imposed limits on the scale of mining operations. Things changed dramatically with the introduction of steam engines and with the societal changes harkened by the industrial revolution. 


The Big Bang (1700 - 1850 A.D.)
With the industrial revolution came steam engines, massive boilers and coal furnaces, new machinery, and a greater supply of steel. These technologies enabled a new flywheel of productivity in the coal industry. Pulley systems operated by steam engines and cheaper steel rails could move coal faster. Steam engines were powered by coal furnaces. The demand for coal skyrocketed and new technology improved access to the supply. Things started to move very quickly. Britain’s annual production was about ~3 million tons of coal in 1700. About 200 years later, U.S. annual production hit 680 million tons.[46]


Mines were worked near-constantly and miners were often paid on a piece wage[47] – by the amount of coal they produced, rather than the time they spent mining. In North America, coal mines were shallow and workers were highly distributed across a large area.[48] For this reason, supervision was logistically challenging and expensive. Because of a lack of supervision and the incentive of being on a piece wage, coal miners regularly traded off safety in exchange for productivity. (We see similar issues today with semitruck drivers falling asleep at the wheel as a result of trying to finish their routes faster – necessitating the need for rest protocols.)


With the increased rate of production, dust became a greater issue.[49] Air in mines became saturated with coal dust, which is flammable, and silica dust, which has been long known to cause tuberculosis.[50] Boilers, furnaces, and explosives like dynamite and Emulex were also extremely hazardous. Heavy steel rail carts and locomotives could crush coal miners. Personal protective equipment (PPE) was virtually nonexistent by today’s standards.[51] 


After several hundred years of incremental development, the coal industry had a big bang. The combination of these factors would lead to disasters like the Monongah mine explosion in 1907, killing tens or hundreds of miners at a time. Respiratory diseases were rampant. Workplace safety protocols were not yet keeping up with the pace of the industry. Workers were highly replaceable, and unions were illegal in most of the West until at least the late 1800s.[52] Thus, workers couldn’t afford to take sufficient precautions in their jobs. 


Peak Employment (1850 - 1950 A.D.)
I could not find evidence of coal mining safety records prior to about 1850 in the United States. In the mid-late 18th century[f], municipal and state governments began collecting data on fatalities.[53] Fatality records are grim but tell an important story about how the philosophy of accidents has changed over time. The first recorded deaths didn’t even consider responsibility – accidents were considered inevitable; a cost of doing business.[54]


The late 1800s to early-mid 1900s saw great changes in occupational health and safety. These years coincided with two early peaks in US coal production.[55] This time also coincides with peak employment in the coal industry.[56] The interaction between technological development and safety protocols was rapid and productive. Mining ventilation standards were established by MSHA.[57] Mines had to ensure that the presence of particulate and gas fell below a certain threshold. Unionization was legalized.[58] Equipment was designed with operator safety in mind and safety protocols around training, ventilation, and explosives became widespread.[59] Despite this, terrible workplace accidents like Monongah happened.


The Monongah disaster in 1907 killed at least 362 men and boys.[60] Despite being touted as a “model mine”, it exploded. Rescuers’ attempts were stalled because the collapsed mine had filled with chokedamp (carbonic acid gas). And because of the newfound scale, complexity, and interdependence of mining operationsT, the cause of the accident was not determined.


Events from this era highlight the political nature of safety and health. It might seem obvious to someone today that smoking in an active coal mine is a dangerous thing to do. But in the early 1900s there were several labor strikes when the U.S. government sought to ban smoking cigarettes in coal mines.[61] Mark Aldrich hypothesized that this was a byproduct of the traditional view of coal miners’ role and responsibilities, saying that miners were viewed as “craftsmen” and “independent contractors” who were “responsible for their own safety”.[62] This social status, combined with a piece wage, suggests that there was both an economic incentive (revenue) and a social incentive (valor) to value productivity over safety.


Production Peaks, Complexity Accumulates (1950 - 2000 A.D.)
From 1950 to the early 2000s was peak production in the U.S. coal industry.[63] At the same time, fewer and fewer people were employed.[64] New technology enabled per-worker productivity to rise quickly, but it increased the complexity of operations. In a few hundred years, the industry went from pickaxe-wielding families to multinational megacorporations.


Accident and fatality rates in the industry plunged.[65] Ventilation, detection, and respiration equipment became more effective, and protocols regarding their use ensured that dust and gas levels were kept at benign levels. From the 70’s onwards, mining companies used atmospheric monitoring systems and automatic alerts to prevent build-ups of methane, carbon monoxide, and dust. The protocol for deploying these systems was to space the units fewer than 2,000 feet apart, and as high in the tunnels/chambers as possible. This protocolization reduced the likelihood of explosions, cave-ins, and the burden of common mining-related diseases, like black lung.[66]


But with so many moving parts, operations became unpredictable in and of themsleves. Weather, human behavior, geography, and geology were no longer the sole sources of uncertainty. Technology had become engineered to be safe, but some accidents still occured. This had implications for for how safety was managed, and new approaches were rapidly deployed. 


First, root cause analysis led safety experts to assume incidents were being caused by human error.[67] After a couple decades of trying, it was discovered that eliminating human error was basically impossible.[68] The focus then turned to sociology instead of psychology. Protocols around improving “safety culture” came about.[69] Safety First, Take Five, Safety Walk, etc. emerged as a way to fight the diminishing returns on traditional approaches to safety.


Finally, systems theory[70] came along – theorizing that “unexpected interactions”[71] between “tightly coupled”[72] components within a system caused accidents, and that these accidents very unpredictable. Charles Perrow argued that a certain number of accidents are “normal”.[73] No matter how hard we try to stop all accidents, some will happen. It is simultaneously true that i) all accidents are preventable, and ii) we cannot prevent all accidents. Safety is full of beautiful and tragic paradoxes.


By the year 2000, improvements in coal mining safety began to plateau. The protocolization cycle of mining safety had run its course and was, as we will see, at risk of being taken too far. Demand for coal would soon start to decline, and competition would increase.


The New Heads of The Hydra (2000 A.D. - Present
A 2006 study on the U.S. Mine Safety and Health Administration (MSHA) concluded:


“Almost 700,000 life years could be gained for typical miners if a quarter of MSHA’s enforcement budget were reallocated to other programs (more heart disease screening or defibrillators at worksites).”[74] 


With increased life expectancy and increasingly sedentary jobs, workers today face a new set of hazards, like heart disease. One of the problems with safety institutions is that their existence depends on them pursuing problems far beyond pareto optimal solutions. 


We’re reaching the end of safety, and the dawn of “health”. Highly social, communicative, administration-based companies seem to be generating a new set of protocols that are directed at sustaining and scaling the dynamic non-events of psychological safety and mental health. These areas don’t benefit from the same scientific basis as biological safety.


That being said, safety is still an issue in many places. Declining demand for coal in the Western world will generate (and perhaps already has) safety issues as companies seek to cut costs. Global capitalism still has pinchpoints. Industries in less-developed economies are not as safe.[75] But when the top work-related causes of death are health issues, a focus on safety can be deadly. It’s time to focus on creating and spreading new workplace health protocols. To do that, we need a better understanding of what protocols are and how we can manipulate them for good. 


The Role of Protocol
In the coal industry, protocols emerged as a first response to newfound hazards. Cave-ins were met with minimum spacing protocols. Gasses were met with detection protocols. Explosive dust and substances were met with ventilation protocols. These protocols provided the basis for “harder” defenses of safety, such as personal protective equipment, engineering, and legislation. 


Protocolization of the coal industry led to the near-total elimination of fatal accidents. Accelerating the process of protocolization in future industries could save many lives. In order to accelerate the cycle, we need to understand two things. First, the high-level dynamics (protocolization) we seek to influence. Second, the low-level mechanics (protocols) which we can directly influence. Therefore, we need i) a grand theory of protocolization and ii) a practical theory of protocol evolution. 


In the next section, I make an early attempt at formulating each of these things. Because this theory of protocolization is derived from industrial safety protocols, it has a narrow scope. However, if it is true that, “The chief motive of all human action is the desire to avoid anxiety,”[76] this theory is a promising starting point for a more general model.   


A Grand Theory of Protocolization
Summary
This model is meant to explain the process of how industrial safety becomes protocolized. Protocols are born as a first response to new hazards, which arise from several sources (technological progress, environmental change, and changes to the social order). Because protocols limit human behavior in some way, protocols that catch on influence social hierarchies, soft power gradients, norms, and values. The social order affects the direction of technological progress. Technological progress changes the environment. And so on. This is my proposed model of the grand theory of protocolization.


Underlying this model is a theory of protocol evolution, which is based on principles of natural selection and mimetic theory. “Fitness” is approximated to protocol usability. Poor usability, which is usually determined by environmental factors, leads to rejection by agents. Protocols change as a result of two forces: i) Mutation, which is primarily caused by memetic error, and ii) Selection Pressures, imposed by the environment, which include social, technological, physical, and computational factors. 


A complete theory of protocol evolution must provide mechanisms for four processes: Birth, Mutation, Selection, and Death. 


Process 1: Protocol Birth
Safety protocols emerge directly as a response to the discovery of hazards. Protocols are, first of all, a means of self-preservation. They protect individuals from hazards. When new hazards are created or discovered, new protocols are made. 


New hazards are created all the time. The first source of hazard creation is environment change, like natural disasters, which can constitute a dramatic change to the set of hazards present in a workplace. Heavy rains create structural instabilities in the landscape. Landslides and floods become hazards.


Another major source of hazard creation is technological change. Having evolved with the experience of constant environmental change, we are somewhat better adapted to handle those hazards. Technological change, on the other hand, creates a tremendously wide, potent, and novel set of hazards.[77] Steam engines created a fantastic variety of hazards, like mechanical machines which could harm workers’ ears, locomotives which could crush workers, and water pumps, which allowed coal mines to be dug deeper, exposing workers to risks like gas, explosions, and cave-ins.


A third source of hazard creation is changes in social order. Safety protocols can arise as a response to shifts in hierarchy, power distribution, or social network size/density. If an elite class in an economic system begins displacing risk onto the working class, collectivist protocols like unionization, labor strikes, and collective bargaining can emerge to correct this behavior. Another example could be workplace hazing or initiation rituals, which create hazards unrelated to environmental or technological change.


The final source of hazard creation is changes to the definitions of health, safety, or hazard. For example, the recent addition of emotional health to the domain of occupational health and safety. It’s likely that this was not a concern worth discussing much 200 years ago. However, today, preventing illnesses like post-traumatic stress disorder, anxiety, and depression now falls under the mandate of some health and safety teams. 


To sum up, safety protocols are created as a reaction to hazards arising from four different sources:
* Environmental change,
* Technological progress (including increasing knowledge),
* Societal change, and
* Changes to the definition of health, safety, or hazard.


Process 2: Protocol Mutation
The main way that protocols mutate is via mimetic error. To follow protocol is to perform two or three unique actions: i) perceive another person following protocol, ii) store the protocol in memory, and iii) replicate the protocol. This is a process of mimicry. Errors can occur during any one of three actions.


  The other two ways that protocols mutate is through tinkering and design. To tinker is to intentionally mutate a protocol in some way. For example, one could add an extra step to a workplace inspection checklist. Because it is an intentional process, the pace of mutation by tinkering is slower than by mimetic error. But it is faster than protocol design, which is creating or changing a protocol so that it is substantially different from existing protocols. Because selection pressures are so strong, there is a very limited ability to design protocols. The strength of mutations from design easily launches protocols into the realm of unfeasibility, where they will be rejected by participants. For example, I have seen a corporate health and safety team design an incident reporting protocol, which was ignored by departments on account of the time-consuming nature of the reports. The designed protocol landed outside of acceptable efficiency-thoroughness selection pressures… Suffering an embarrasing and ignoble death.


The three sources of protocol mutation can be characterized by their pace and strength:


  



The differences between these sources are critical for application of protocolization theory, especially if we want to accelerate the process. This is discussed in more detail in the conclusion.


Process 3: Protocol Selection


If protocols merely mutated, there would be no progress. However, we saw protocols improve in the coal mining industry alongside technological development. Early anti-explosion protocols like candle-based detection were replaced by preventative protocols, like maximum thresholds for dust and gas in mines. This didn’t happen immediately. Environmental factors had to change for more effective safety protocols to emerge. In the table below, I describe what I believe are the strongest protocol selection pressures.


Selection Pressure
	Description
	Efficiency-Thoroughness Trade-Off[78]
	Safety protocols that greatly reduce accidents will be more likely to propagate. Safety protocols with steep efficiency costs face more resistance to propagation. 
	Power/Influence
	High-power actors can coerce other actors into following a protocol that benefits them. Low-power actors cannot.
	Agency/Freedom
	High-agency actors can easily opt-in and opt-out of protocols. Low-agency actors cannot.
	Bandwidth (Cognition, Memory, Physical)
	Agents’ abilities to perceive, remember, and replicate protocols are limited by cognition, memory, and physical bandwidth. Protocols typically compete for these resources in a zero-sum fashion. 


This selects for protocols that are time-efficient and easy to mimic accurately.
	Technology
	Technology pressures selection in several ways:


First, it can augment worker bandwidth by encoding protocols into technology, thereby freeing up bandwidth for other protocols. Gas detection using candles was an active protocol, which was encoded into passive Atmospheric Monitoring Systems. More tech = more protocols.


Second, technology can create hazards that change the ETTO (cost/benefit) of existing protocols. More effective insulation for residential houses led to problems with radon, which increased the value of radiation detection protocols.[79]


Third, technology can allow us to discover and understand new hazards, like the role of lipoproteins as a risk factor for heart disease. 




	Benefits 
* Immediacy
	

Protocols with immediate benefits are more likely to replicate. Short-term interests, like survival and status games, select for protocols with immediate benefits. Health protocols face lots of competition from immediately beneficial protocols.
	* Legibility
	Protocols with benefits that are clear are more likely to replicate. In the case of safety protocols, obvious cause-and-effect makes some protocols easier to follow than others. 
	* Generality
	Protocols that benefit a larger variety of workers are more likely to replicate. 
	Ethics
	The ethics of an individual affect their acceptance or rejection of protocols. E.g. Early 20th century coal miners, viewing themselves as entrepreneurs, might have had a work ethic that favored risk taking.
	Horizon of Concern
	Longer horizons of concern, unlocked by longevity and or wealth, allow protocols with non-immediate benefits to compete. Workers focused on meeting near-term needs like food, water, and shelter don’t have the luxury to focus on protocols that will benefit them only in 20+ years. 
	Wealth
	The level of wealth in an industry will affect workers’ horizons of concern and the amount of time and other resources that can be reasonably spent of safety protocols. As the value of a human life increases, so do the value of accident prevention measures.
	Displacement
	Some safety protocols displace costs from one group onto another. E.g. SUV’s are safe, but reduce safety of other drivers and pedestrians. The more risk a protocol displaces, the more power its participants need.
	Public Attention
	Protocols that target well-publicized risks might replicate better. For example, concerns over consumer food poisoning led to improvements in factory sanitation protocols, achieving the target objective and reducing the rate of occupational disease at meatpacking plants in the process.[80]
	

Protocol Expiration (Death)
The “birth” and “death” of protocols are intimately related. The same forces that drive protocol creation drive protocol destruction. New technologies can quickly make old protocols obsolete. Many protocols will experience slower “deaths”, however. If a protocol doesn’t have a favorable set of characteristics relative to its surrounding environment, the protocol could die. For a protocol to die, it simply needs to not be used. Protocols are similar to muscles in this sense; without use, they will atrophy away. 


Generalizing the Protocol Cycle
When we create new technologies or decommission old technologies, that has unanticipated or unintended consequences. There can also be willful ignorance of the consequences, as with the DuPont corporation’s use of ethyl lead[81] to reduce engine knock or the Monsanto corporation’s use of Roundup.[82] New health and safety issues arise all the time. The speed at which we can protocolize solutions affects how many lives are saved.


To accelerate the process of protocolization, we need to increase the volume and pace at which we tinker with existing protocols. Designing new protocols has a high failure rate. It probably makes sense only in extreme situations, because designed protocols have a high probability of being in conflict with existing norms and habits. Therefore, they require enforcement, which is expensive in many ways. Tinkering, on the other hand, is cheaper and can be done at a faster rate. 


Tinkering with protocols along the various contours of the selection pressures listed in this article is a start. Notice a helpful protocol? How can it be adjusted to be simpler? Can it be augmented by information technology? Is there an existing dense network that it can be introduced to?






Bibliography


Index




________________
[1] Mining Safety and Health Association (2018) ‘Coal Fatalities for 1900 Through 2017’. https://www.msha.gov/coal-fatalities-1900-through-2017
[2] If you want to skip to the theoretical component, see page 12. :-)
[3] Coal: a human history page 11
[4] Coal: a human history page 72
[5] Safety First page 58
[6] Safety First
[7] Coal: a human history
[8] Merton
[9] Nathan Schneider https://beta.briefideas.org/ideas/7c294f66fae2352765ec79bf5b0c7966
[10] The Whale and the Reactor
[11] The Whale and the Reactor
[12] Searching for Safety
[13] Searching for Safety
[14] ILO WHO joint report
[15] Weick, K.E. (1987) 'Organizational culture as a source of high reliability.' California Management Review, 29: 112-127. AND Hollnagel
[16] Hollnagel - diagram is also from Hollnagel
[17] Ebrary ‘A Dynamic Non-Event’. https://ebrary.net/29719/sociology/dynamic_event
[18] Boulding and Waldavsky (see Searching for Safety, double reference)
[19] Peabody Energy Company (2021) ‘ESG Report’: page 26.
[20] Arch Resources (2023) https://www.archrsc.com/sustainability/safety/
[21] Government of Yukon, Transportation Maintenance Branch
[22] Ironies of Automation
[23] Ironies of Automation
[24] Attia: https://peterattiamd.com/peter-on-the-four-horsemen-of-chronic-disease/
[25] However, Safety and Health are not perfect substitutes. Becoming completely safe will not make you completely healthy. In the absence of external stressors (negative events) internal feedback loops will destroy a system (you).
[26] https://www.sunnisidelocalhistorysociety.co.uk/bellpit.html
[27] Source: A short history of occupational health
[28] https://scdhec.gov/environment/your-home/asbestos/asbestos-effects-health
[29] ILO WHO Report
[30] ILO and WHO Joint Report 
[31] ILO WHO report
[32] ILO WHO report
[33] https://www.cdc.gov/mmwr/preview/mmwrhtml/mm4822a1.htm &&& National Safety Council. Accident facts, 1998 edition. Itasca, Illinois: National Safety Council, 1998.
[34] World Health Organization https://apps.who.int/iris/bitstream/handle/10665/42539/9241562161.pdf
[35]  https://www.sunnisidelocalhistorysociety.co.uk/bellpit.html
[36]  https://www.sunnisidelocalhistorysociety.co.uk/bellpit.html
[37] Coal: a human history
[38] Durham Mining Museum http://www.dmm.org.uk/pitwork/html/history1.htm
[39] Durham Mining Musuem http://www.dmm.org.uk/pitwork/html/history1.htm
[40] Bob Bradley http://www.healeyhero.co.uk/rescue/individual/Bob_Bradley/Bk-1/1300.html
[41] Encyclopedia Brittanica https://www.britannica.com/technology/coal-mining
[42] The Age and Depth of Mines, Department of the Interior https://www.science.org/doi/pdf/10.1126/science.60.1541.viii.s
[43] https://www.healeyhero.co.uk/rescue/individual/Bob_Bradley/Bk-1/1300.html
[44] https://www.healeyhero.co.uk/rescue/individual/Bob_Bradley/Bk-1/1300.html
[45] Coal: a human history
[46] Bruce C. Netschert and Sam H. Schurr, Energy in the American Economy, 1850-1975: An Economic Study of Its History and Prospects. pp 60-62.
[47] Safety first, a short history of occupational health
[48] A Short History of Occupational Health
[49] Safety first
[50] A short history of occupational health
[51]National Museum of American History, Smithsonian https://americanhistory.si.edu/collections/object-groups/mining-lights-and-hats
[52] https://www.britannica.com/topic/history-of-Europe/The-rise-of-organized-labour-and-mass-protests
[53] Registers of Mine Accidents (PA) for the Anthracite Districts, 1899-1972
[54] Registers of Mine Accidents (PA) for the Anthracite Districts, 1899-1972
[55] https://en.wikipedia.org/wiki/Coal_mining_in_the_United_States
[56] https://en.wikipedia.org/wiki/Coal_mining_in_the_United_States
[57] Safety First
[58] https://en.wikipedia.org/wiki/Labor_unions_in_the_United_States
[59] Safety First
[60] MSHA: https://web.archive.org/web/20061229061425/http://www.msha.gov/DISASTER/MONONGAH
[61] Safety First
[62] Safety First
[63] https://en.wikipedia.org/wiki/Coal_mining_in_the_United_States
[64] https://en.wikipedia.org/wiki/Coal_mining_in_the_United_States
[65] https://en.wikipedia.org/wiki/Coal_mining_in_the_United_States#/media/File:Coal_Fatalities_-_US.png
[66] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7885287/
[67] Hollnagel
[68] Hollnagel
[69] Hollnagel
[70] Hollnagel
[71] Weick
[72] Weick
[73] Perrow
[74] https://www.jstor.org/stable/41761169
[75] From Searching for Safety: “”In summary,” Sagan and Afifi conclude, “economic development has been shown to add approximately 30 years to life expectancy.””
[76] Ibn Hazm / erikhollnagel.com
[77] Coal: A Human History
[78] https://en.wikipedia.org/wiki/Efficiency%E2%80%93thoroughness_trade-off_principle
[79] Searching for Safety page 227
[80] A Short History of Occupational Health, page 49
[81] Cautionary Tales, The Inventor Who Ended the World
[82] Cautionary Tales, The Inventor Who Ended the World
[a]commerce? or industry? or economic activity? or the workplace? or life?
[b]inclined to say here that social distancing *is* the technology (to the extent it can be written down and reproduced elsewhere) but requires no additional technological apparatus
[c]also "sustained"
[d]may be interesting to make a tangential remark on the aviation near-miss database in relation to the actual accidents
[e]the emergent socialized benefit is a really interesting point that I want to keep in mind as I take another look at the other drafts. Certainly it bears some relevance to the commons management literature
[f]19th?

Addressable Space
Chenoe Hart
Version 0.65


NOTE: This is an early draft. “[ ]” brackets around writing indicate notes regarding areas of future development.


[Alternative title ideas: Invisible Walls]
[a]


1. Introduction




If you look at our world from the right perspective, it may appear that we are starting to live inside a computer. Our buildings and cities are already often organized according to specifically digital sets of information; buildings are identified via  socially-constructed address numbers[b], and their interiors are divided into rooms and floors associated with sometimes arbitrary names and numbers. Those are abstract operations in which analog information about the world – the exact number of floors in a building, the exact dimensions of a room, or the size of a building’s lot – is replaced with its simplified position on an abstract symbolic list of information, which no longer needs to directly correspond with the real-world layouts of the spaces involved. A forty- and fifty-foot-wide house lot located next to each other might be at equal positions on a list of addresses despite taking different amounts of time for a person to walk past, and in Western culture a list of elevator buttons will often portray the literal thirteenth floor of a building as being the fourteenth floor instead. Whenever we perceive our travel through physical space as occurring via such proxied measurements of space, then we are traveling through a tiny conceptual wormhole every time we use those abstractions.[c][d]
We see those spaces, in short, in slightly the same manner that a computer might. For a computer network they might as well be connected across vast distances by instant communication hyperlinks; on the internet where near-instant communication is possible between servers located in different parts of the world, physical distance does not directly matter. And today, as software eats more and more of our surrounding world (to paraphrase a 2011 prediction of the technology investor Marc Andreessen),[1] it is becoming possible to notice situations in which physical spaces are being arranged in response to the affordances of decentralized computer networks in addition to humans.[e][f] So-called “chaotic storage” organizational systems used in contemporary automated warehouses[2] decouple the physical locations of the objects in their inventory from their organizational categorization; in contrast to more traditional schemes where objects might be organized alongside others of a similar category, under that system they are placed in arbitrary locations in physical space. Their locations are retrieved by the system by searching in a database, and as a result the physical contexts of their location and their adjacencies with other objects no longer carry any inherent meaning. 
The assignment of such an abstract label for looking up an object also renders its distance from other objects to be irrelevant, since every physical location placed on a shared common informational network will instead become immediately retrievable. (At least in cases where the varying travel distances involved in retrieving items from different physical locations avoid imposing excessive latencies on the system.) As with the conditions of addressability found within a computer system where programs indirectly reference the physical locations of information stored in their memory, physical spaces which are subject to abstraction become randomly accessible. The design of physical spaces involves abstract systems and relationships in addition to physical matter, creating opportunities for protocolized thinking[g][h] [define] to be used to help understand its past and future.[i]




2. Digital Rooms
[j]


Digital methods of organizing physical space prominently emerged in the eighteenth and nineteenth centuries.[k] Around the same general time that the mathematical thinker Ada Lovelace was making her pioneering future prediction that an early computing system could be used to carry out a sequence of algorithmic operations,[3] increasingly bureaucratic governments organized within emerging jversions of the modern city and the modern centralized nation were developing methods of consolidating information about their physical territories. House numbers were developed for purposes of tax collection and military conscription, allowing, in the words of the historian Anton Tantner, for the interior of the house to become “transparent” to the state.[4] The adoption of corridors streamlining passage through the interiors of institutional and government buildings in that same general time period created conduits for faster information transmission within those organizations. The corridor placed doorways to offices alongside each other in a relatively neutral relationship, and the architectural theorist Mark Jarzombek suggested its spatial configuration might even have the capability to exert a potential democratizing influence. [5] Distances between rooms mattered less they had in the past.[l] [m]
The emergence of those new organizational layout schemes also coincided with the arrival of some basic features of residential interiors which we now take for granted within our homes; the idea of increasingly private rooms accessed via a single doorway rather than multiple openings onto other rooms, and of rooms which had doors on them to begin with in the first place.[6] The architectural theorist Robin Evans identified a progression towards increasingly private rooms occurring within Victorian households, in contrast to a prior Italian Renaissance-inspired paradigm in which rooms were boundlessly interconnected with each other by doorways “wherever there was an adjoining room.” Each individual room in a Renaissance interior would most often have “more than one door – some have two doors, many have three, others four" which opened onto its surroundings. Evans described the 1525 Villa Madama in Rome (commissioned by the Medicis and suspected by the author to have been designed by Raphael), as containing within its interior of interconnected rooms "ten different routes into the villa apartments, none with any particular predominance." Inside the villa it was thus “… necessary to pass from one room to the next, then to the next, to traverse the building.”[7] The connectedness of the villa’s rooms made them effectively function like “an open plan relatively permeable to the numerous members of the household, all of whom - men, women, children, servants and visitors - were obliged to pass through a matrix of connecting rooms where the day-to-day business of life was carried on.”[8]
        Within later British Victorian spatial paradigms governed by that time period’s notions of privacy, in comparison, rooms became more self-contained, entered via single doorways on one wall to leave the others isolated from their surroundings in a development associated with the emergence of modern concepts of privacy. (Technologies for locking doors meanwhile also improved, with the advent of industrial mass production.) Such a private and enclosed room could be perceived as a discrete entity unto itself, as a place within which one might not necessarily be aware of what spaces existed or what events occurred beyond the boundaries of the room’s walls.[n]
That set of circumstances helped facilitate the trend in the Victorian household in which rooms were associated with more specific functions than before, and with specific people or groups of people. A Victorian house plan included internal divisions between relatively public rooms where visitors were allowed, and more private ones where they were not. It had newly specific different delineated spaces for women and men, parents and children, and for servants and the household members whom they served.[9] Those rooms would thus all have more implicit associated informational labels than they had before; one room might be specifically known within a household as a library, another as a dedicated dining room. And as those rooms grew more enclosed, a visitor would be able to see less of the overall floor plan of a building while walking through it; seeing any part of the building would require more deliberate personal choices than before, and require more permission from other people. Access to such spaces was governed then, in a sense, by both physical and informational types of walls[o]. You were either allowed into a room or not, and a room either belonged to you or it did not; those were binary spatial conditions, and a binary condition can also be described as one which is digital[p].
Digital rooms were able to be randomly rather than sequentially accessed. Walking through one room to access another, in comparison with not needing to do so, was like the difference between retrieving information from a stored physical tape where previous information on the tape had to be recalled before a desired informational entry could be played back, and that of directly retrieving information from one specific physical location within a modern memory chip or via a read head swooping in to the right place on a hard drive platter. 


3. Elevator Buttons


A technology capable of taking advantage of the ability for adequately demarcated and legible physical spaces to be conceptualized as digital arrived with the development of the elevator, which eventually developed into an information-based interface offering literal direct access to physical locations. The technology introduced a new means of isolating and dividing interior spaces, operating at the scale of entire floors of buildings rather than rooms[q]. Stairwells used for circulation prior to the adoption of elevators might be interpreted as a kind of elongated corridor, where both circulation schemes provided independent access to relatively private spaces from a more public space, but the stairwell still presented different destinations throughout its height less neutrally in comparison with each other. Walking up to the fifth floor of a building was a noticeably different experience from walking to the second, differentiating the two vertical destinations more than walking a similar lateral distance to reach a different room along a corridor would. The elevator introduced a more neutral means of accessing any floor, making vertical circulation more like the process of accessing a room off a corridor, but its eventual automatic operation (in which occupants no longer needed to pay attention to its movement through space, or to any operators controlling its movement) also introduced something new into the experience of journeying through physical space: travel became non-linear.
In contrast to the experience of proceeding along a corridor within a typical public institutional building where you walk past and see the closed doors of other rooms (which might have names or numbers on them) on the way to your planned destination, or of ascending past the exits to other floors found inside a stairwell, within a typical modern elevator car you only see glowing floor numbers pass across a digital information display. As the elevator replaced the stairwell as a primary system of circulation in multi-story buildings, floors previously defined using qualitative terms like the piano nobile (the most desirable floor where the owner of a house resided, located two or three levels above the ground) were replaced by neutral (and thus more egalitarian) numbers,[r] at the same time that the individual doorways for each floor which one would walk past on a stairwell were replaced, inside the elevator, with just a number accessed via passage through the same automatic sliding doorway used to access any floor. That obfuscation in turn made floor numbers themselves vulnerable to potential distortion and manipulation.
When you look at the array of buttons in a typical elevator, you subconsciously assume that you already know how to use them. You will press a button, which is a solid plastic disc or square offering resistance to your fingers as you push it in. You generally expect as a result of your button-press that the elevator will take you to a destination in the building a certain number of floors above the ground (or in other words a certain number of stair flights which you would otherwise need to walk),[10] which is described with a corresponding floor number label next to the button which you pressed. If you press to access the second floor of a suburban American roadside motel you have just checked into for the night, it is reasonable to predict that when you draw open the curtains of your guest room window you will find yourself looking down from above on an overhead view of a parking lot, perhaps with the roof of your car in sight. Meanwhile if you’re traveling to a high floor in an especially tall building, the pressure in your ears might change during your elevator ride, giving you a concrete visceral indication that your physical body is rapidly traveling upwards. Those categories of physical experience are generally more mild than the sensations of exertion you would experience when ascending many flights of stairs, but they help to remind us that the elevator, despite archetypally being a windowless enclosed box which insulates us from perceiving the sights and sounds of the floors it travels past, may still not be a space entirely sealed off from its surroundings.
In certain situations, however, those lingering impressions of metaphysical coherence are so subtle they are capable of being bypassed or disrupted. One common example of how that disruption occurs in our everyday lives involves some fleeting moments of old-fashioned superstition. We expect and casually ignore how often the numbering on a Western elevator control panel will jump from indicating a button for the twelfth floor to one for the fourteenth floor, even though no additional physical floor necessarily exists in between them.[11] Buildings across China, Japan and Korea similarly omit the number four from their floor listings, since its linguistic pronunciation in those places resembles the word for “death.” On a larger spatial scale, the association in Chinese culture with “88” being a lucky number meanwhile leads developers to routinely over-count or under-count the entire overall floor counts of skyscrapers. [s]The city of Vancouver had to institute a ban on the practice of developers omitting floor numbers, after fire safety concerns were raised when some of them began to eliminate accurate labels for any floor or apartment unit number ending in “4”. In one case seven entire floors (including the 13th and 54th) were eliminated from the floor listing of the 53-story Burrard Place residential building in the city, allowing the developers to market it as being 60 stories tall instead.[12][t][u]
        Another significant quantity of floors within a building may be left invisible within its floor listing if they are designated to accommodate mechanical systems rather than people, and accordingly left out of the floor listings in a building’s public elevators. [Add examples of buildings with mechanical floors here, and a discussion of the space they take up.] Between all of those different varieties of floor space, an elevator passenger might be left to wonder what a floor in a building even means to begin with.[v][w]


Quantization


The elevator makes floors seem more floor-like than they might otherwise appear to be. The full expanse of each successive span of eight or ten or so feet of space within the overall height of a building which becomes known as a “floor” is reduced under the elevator’s labeling scheme into a single number associated with the top surface of its lower supporting slab at which an elevator lands. According to signal processing terminology, that reduction of information would be considered an act of quantization, or in other words a process of converting from analog to digital information by smoothing over its details and irregularities. 
[WIP: Counter-examples of elevators with more manual control, ie. older analog hand-cranked elevators run by elevator operators. How precisely an elevator docked at its floor added an additional richness of nuanced social information to the journey of the elevator ride.] 
        Prior to being smoothed over by digitization, the full-range analog reality of a high-rise floor level as experienced by its occupants could be considered more complex. In a more analog world, the elevation level of a stair landing might be easier to interpret as being a slightly different floor than those accessed by the steps above or below it; after all you could similarly access it by walking onto it with your body, and in the case of the American suburban split-level house typology it did form one of several partially-overlapping vertical floor levels within a building. 
One might attempt to think further past artificial generalization about vertical space by considering the perspective which a curious unsupervised housecat would have of the interior space of an apartment; from that animal’s point of view, [x]the top of a dining table or a bookshelf might form other additional inhabitable floors. Architects used to sometimes design in arguably similar ways; within the history of twentieth-century modern architecture alone one might consider the terrace-like staggering of multiple interior levels within homes designed according to Adolf Loos’s Raumplan concept, Le Corbusier’s floor-connecting ramps and his duplex apartments, and later in the century the adoption of the residential living room “conversation pit” as a trend. All of the different potential level heights produced by the many varieties of inhabitable surfaces found in the interiors of those architectural precedents is reduced, under the logic of the elevator, down to a single number.


Montage


In 1965 the architect Philip Johnson lamented the introduction of elevators into modern buildings by arguing that they disrupted the “whence and whither” of a deliberately-designed linear processional journey which visitors might otherwise be exposed to when walking through an architectural space. The elevator’s isolated windowless interior was anathema to the varied experiences offered by traversing a “shifting, rising, declining turning path” within a building or a landscape, or the sense of delight which a visitor might feel as a result.[13] In today’s world of ubiquitous elevators however, it is perhaps possible to also notice moments where elevators allow experiential narratives to be constructed in addition to being destroyed.[14] The random-access digitization of the spaces traversed by an elevator might allow it to operate in certain ways like artificially-constructed media experience. A narrative of some kind develops whenever an elevator bypasses inconvenient and unaesthetic mechanical floors within a modern skyscraper, or perhaps even unseen residential floors in a luxury apartment building designated for people of lower incomes and accessed via a separate so-called “poor door.” [Mention that later instead?] The elevator’s ability to eliminate bypassed spaces from the overall processional journey its passengers take through a building as they travel from its entrance lobby to a higher floors causes the remaining rooms and visual observations which they encounter to have different meanings than they otherwise would.[15][y]
        That situation might be comparable to the cinematic technique of montage, in which a movie cuts immediately between footage of two different locations. If a character in a movie travels from home to an office, viewers don’t see the entire experience of the car ride, the search for a parking space or even the elevator ride which they might have taken part in between those two destinations. In the context of a movie’s production, however, the camera which filmed two scenes in two disparate locations still might hypothetically been transported between them, even if it was turned off while being trucked between takes.
In the theories of the filmmaker-turned-architect Rem Koolhaas, the elevator within a building can be associated with accordingly cinematic categories of experience. The elevator's ability to grant independent access to any floor level encouraged the interpretation of individual building floors as being discrete enclosed zones, able to be built into distinct interior thematic worlds as a result of their inherent isolation from the continuity of their surroundings.[z] [Quotes describing those spaces here.] The structure of an ideal skyscraper [cite what it was] would “create at each elevator stop a different lifestyle and thus an implied ideology, all supported with complete neutrality by the rack.”[16] The result was a fractured environment resistant to the centralized control of an architect’s overall design intent. 
Koolhaas referenced a built historical example of that concept in the form of New York City’s 1930 Downtown Athletic Club, where such disparate interior spaces as a boxing gym, a swimming pool and a golf course co-existed on different floors within the same 35-story building,[17] generating distinctive but concealed experiences from each other. [Quote Koolhaas’s specific description of procession from elevator into a gym, and discuss it.] Such a spatial sequence might potentially suggest references to the architect’s own comment where he has said that he perceives “… almost no difference between architecture and film. Both work through montage. For a script, you have to connect episodes in such a way that a certain suspense is created. In architecture, we also connect episodes.”[18]
[More of a conclusion here. General overall outcome: digital encoding of floors combined with automation leads space to be capable of being rearranged.]


4. Time


The perceived continuity of the floors accessed by buttons within a 1930 elevator building may have been capable of being radically reshuffled in the case of Koolhaas’s analysis, but today high-tech modern elevators are often controlled by the even more abstract affordance of pixelated screens instead. [Discussion of how current elevators with screens and digital access dispatching are controlled by highly arbitrary forms of spatial access.] They are also controlled by automated computer systems which establish a variety of different hidden rules for the elevator’s movement in different situations.[19] Complex informational routing schemes might direct elevators to only land on odd or even floors within a building,[20] or passengers might be directed to either the upper or lower level of a double-deck elevator before they board for their ride.[21]
        In all those cases involving modern elevators, technological development makes the elevator a more effective system of conveyance by optimizing the temporal efficiency of how it travels through the building. Those types of efficiency improvements have been touted as offering an alternative substitute approach to the more conventional design strategy of increasing circulation throughput inside a building by including additional physical elevator shafts within its plans.[22] Such elaborate systems are still, however, operating by taking advantage of an older inherent attribute of the elevator: the non-linear temporal nature of its movement between different floors.
        Consider how time passes during a typical elevator ride. The comparative journeys to access high and low floors on an elevator are only distinguished from each other by relatively minor variations in waiting time. After the constant value of the time which elapses during an elevator's initial acceleration and later slow-down is subtracted, the different amounts of time the elevator spends traveling at full speed between floors have an even lower relative impact on the perceived time of how long a floor takes to reach. And other aspects of the overall elevator journey are often less predictable – different rides to the same floor might take longer depending on how long a passenger has to spend waiting for a car to arrive, or be interrupted if it stops on other floors – making it even less obvious how differences in floor height influence the overall duration of an elevator journey. 
[Counter-argument: longer elevator rides still might mean it’s more likely the ride will be disrupted in terms of more people stepping in/out of the elevator, but the presence of those people throughout the height of the building may not be evenly distributed.][aa] The difference between taking an elevator to either the fifth or the seventh floor of a building might feel like a less significant difference in travel time compared to the difference between riding the same elevator at lunch time versus 3 P.M. in the afternoon, or to walk up two extra floors on a stairway. Time and distance became interchangeable variables, with small amounts of time capable of being weighted to substitute for much longer expanses of distance.
[Conclusion: elevator floors ultimately become equivalently accessible entries in an arbitrary numeric index, their differences in access time reminiscent of the occasional slight delay which occurs when retrieving digital information from distant sectors of a mechanical hard drive.] [Rewrite this to avoid overlaps with my other hard drive comparisons?][ab][ac][ad][ae][af][ag][ah][ai]


Folding [section in development]


[Section introduction: if the passage of time during an elevator ride is non-linear, that means the physical path an elevator takes through a building may be designed to be non-linear as well.]
The two towers of the original World Trade Center featured a pioneering new type of operational system for their elevators, inspired by a previous similar organizational scheme devised by Frank Lloyd Wright for the nuclear-powered elevators within his unbuilt imagined mile-high skyscraper known as The Illinois.[23] [Description of what a sky lobby is.] In both cases, the scheme allowed buildings to be designed which were taller and of greater density than was previously viable, in either a designer’s fictional imagination or the real world. Those buildings specifically achieved greater height, where the maximum feasible height of a building had been limited before, because they were able to increase the throughput of their internal systems of circulation to enable people to more efficiently travel to their higher floors.[aj]
[Those towers achieved a dramatic spatial expansion in height [also in size, ie. 9 million square feet across two buildings] by expanding in the dimension of time. Logistics involving abstract digital forms of information allowed them to be built to a greater height in the same way that the advent of steel allowed earlier generations of masonry buildings to be built taller.[ak] (Earlier in the history of skyscrapers the advent of the telephone represented another case where the introduction of a technology involving digital information allowed taller buildings to be feasibly built than was possible before; Koolhaas made note of that cause and effect in his analysis of the skyscraper building type.)[24]]
[Possible metaphor: elevator buttons fold and re-arrange space into greater degrees of density.]
[Outcome: elevators, through increased automation, can be navigated in more digital ways than rooms along a corridor were, ie through indirect paths towards a destination. Takes the affordances and effects of corridor navigation an additional step further. That’s something concrete that the digitization of space can actually do.]
[Future section conclusion: it’s possible to construct new types of skyscrapers, conceptually speaking, which achieve greater densities of occupation terms of time rather than vertical height.]


Teleportation [section in development]


The sky lobby elevator’s topological folding of a journey through physical space is more inconvenient for human users than it is for automated systems capable of producing many of such rearranged journeys through physical space. From the imagined perspective of an automated system such as a robot[25] which is designed to perform tasks which would be tedious for humans, traveling to a destination one mile away is barely different from traveling one foot away. The fact that the longer trip would consume a greater amount of time is only of relevance to us when it is an event we are actively waiting to see completed, but those events are less relevant when they occur passively in the backgrounds of our schedules, and the passage of that time is irrelevant in some sense to the robot itself.
        [Further text introducing a transition from discussing elevators to discussing robots. Define my terms for discussing robots – maybe like a small trackless elevator which cannot transport people, and therefore similarly analyzed in terms of their building-scale spatial relationships rather than precise technical details. Labeled and numbered corridors and doors serve as de facto elevator tracks guiding a robot through a building.]
[Discuss here (?) if robots (and systems like GPS?) have the ability to make rooms accessed in lateral space more randomly-accessible like those on an elevator shaft.]
[Future content: comparison between a computer running a brute force calculation on a large number and a robot (or just a train?) traveling a long distance in physical space. That condition perhaps ultimately proves Johnson’s claim that the elevator negated the dimension of time in architectural procession.]
 [Conclusion: conceptually speaking, robots can teleport. (Also discuss how elevators relate to teleportation.)]


5. Duplication [sections in development][al]


However radically digital information and automation technologies might streamline and simplify the act of traveling between two physical locations in physical space, another property of those systems might also mean that the space from which a journey started and that of the journey’s eventual destination could be identical to begin with. [But not just because of the elevator – the implicit standardization of numbered rooms [?]  - after all, to a robot numbered rooms along a corridor were accessible through a similar conceptual process.]
A pioneering building in the development of standardized rooms, the Statler Hotel constructed in Buffalo, New York in 1923, incorporated repeated copies of identical guest rooms within its floor plans along with, in a certain sense, a relative degree of automation within the building’s operation.[am] Its arrangement would come to be taken for granted within future hotels with identical rooms or models of rooms across the United States and the world. In the description of the historian Lisa Pfueller Davidson, the hotel’s design overturned prior norms of high-rise construction in which room outlines were distorted to fit between the regularly-spaced columns in a building’s steel frame. Instead within the floorplan of the Statler, columns were moved out of their perfectly-spaced grids in order to enable the hotel rooms to have identical repeating dimensions instead. That change would allow the rooms to accommodate more standardized sequences of movement during their cleaning routines, in accordance with Taylorist methods of standardizing industrial work processes which were fashionable around that period in time. [26] 
what would have been insignificant irregularities in the building’s overall structural loads.


 [A robot of course is better at handling repetitive tasks than people, when and if the technology for them exists, and even though they can adapt to different situations, a standardized room still might reduce the range of situations they could encounter.]
[Discuss relationship between conceptual “teleportation” and duplication.]
[Note that the identical duplicate rooms still had individual address numbers.]




Searchable Space [in-development][an]




A space with a large enough amount of potentially retrievable digital information, like that of hotels which had hundreds or even over a thousand total rooms in them early in the twentieth century, may have its spaces comprehended more effectively via a computational process of searching. [Brief description of search algorithms.][ao]


[Random access space allows for searching, ie GPS.]
[Briefly my comparison between identical suburban chain businesses and object-oriented programming. Chain restaurant = instance of a class in computer programming terms.]
[Example: GPS searching for a chain business you already know the name of, before you know its physical location – more physically-disconnected way of navigating an unfamiliar place compared with having to learn the specific names and locations of local businesses.]
[Discussion of Starbucks business strategy of placing many locations of a chain story in a nearby area.]


[Discussion of fast food and duplication:
Fast food places – evolved from diners which were based on conversions of mass-produced rail cars
        Are designed according to standard store prototypes
]


[Need to distinguish this section from earlier brute force programming / robot travel comparison somehow?]


[Transitional tech – more about how duplication relates to affordances of computer networks.]


Distributed Appendages [in-development]


[Discussion of Issac Asimov’s scenario of multiple robots forming a single larger collective “robot” which was “twiddling its thumbs.” How would it perceive and interact with the world, including perceiving physical space? How would that collective entity operate from a spatial perspective if the individual robots were isolated from each other in different rooms?]
[Additional text]
[Also different affordances in terms of privacy: physical spaces adapted entirely to the needs of automated systems would no longer need to be designed to house bodies as a single continuous entity like buildings do when they are designed for humans, and that situation would mean that such architectural concepts as the interior wall and the enclosed room could have their necessity questioned in new ways.]
[Additional text]
[Comparison between computers and a clonal colony of aspen trees.]


Computer Vision [section in development]


Across all of the previous examples, it becomes clear that computer networks and automated systems are able to perceive and occupy physical space in significantly different ways from humans. In the terminology of the biologist Jakob Johann von Uexküll, those computer systems with their dramatically different affordances from ours might exist within a different Umwelt from our own, a totally different world framed around their specific sets of sensory affordances which would appear deeply alien to us from our own familiar point of our human perceptual capabilities.[27][ap]
We cannot directly understand how a computer capable of perceiving multiple different locations in physical space at the same time perceives our world any more than we can comprehend the perspective of a small insect dwelling on a blade of grass, and we cannot see all of the same world that it sees through its affordances[aq]. [Recap of the computer’s specific sensory affordances here as transitional text?]
 [Counter-argument / challenge: computers and robots cannot exist in a space of perfect physical/digital interconnectivity where they still need to serve the needs of humans.]
[Counter-argument: what happens when different appendages of a computer network are in different rooms, with a wall separating them and preventing them from acting in a truly interconnected way? Are they limited in some of the same ways as people? But at the same time still an unusual and different way of perceiving space; how can we really understand it? What things could such a system do?]
[There are boundaries to the edge of their network of perfectly digital connectivity through space.] 


Game Spaces [in-development]
[Proxied examples of what perfectly connected spaces look like might potentially occur within games, and could be analyzed to understand that condition. Are they closer to being the computer’s Umwelt?]


[Additional text]
[Technical discussion of game level construction.]
[Discussion of town interiors in Final Fantay VI]
[Interior space is surrounded on all sides by darkness – that’s the architectural drawing convention used to represent an underground space, even though the game’s spaces in that case are not necessarily underground.] But in a sense the game village interior spaces are similarly isolated from their surroundings, because they are connected to the outside world via programmed-in teleportation devices rather than any innate direct relationship. An architectural drawing of an interior floor plan of a building might also eliminate information depicting the outside world beyond a building’s walls, but it would show it in a lighter color from the walls. That means it would indicate where the thickness of the walls ended and the context of the outside world began. Similarly top-down portrayals of floor plans within the game levels do not show the presence of the walls around buildings; instead a dark (and thus architecturally speaking implicitly solid) void extends everywhere beyond an invisible line (or a visible wall place) indicating the inner surface of the exterior walls.]
[That condition occurs at the same time that conditions of “inside” and “outside” are also more arbitrary circumstances which are subject to greater degrees of re-mapping and of potential isolation from each other.]
[That makes them a more extreme version of Victorian rooms[ar]. They are also larger on the inside than outside.]
[Discussion of how the edge of a game level is represented here. They still have boundaries of their own. Brief overview of how the edge of a game level may be abruptly cut off by devices like an enclosing mountain range or an open ocean. Compare the ocean even to the “underground” building interior?]]
[Are game levels themselves are also self-contained rooms? Investigate the possibility of the enclosed Victorian room itself being a metaphor for being online vs. offline.]


7. Virtual Walls [in-development]


A virtual world, like a medieval map of the world, has hard edges to it. The edges mark the end of all information which has been constructed or otherwise perceived by the system perceiving it, at least in terms of what information that system is capable of perceiving. [Maybe another way of describing an Umwelt?] There could also by extension be edges found between one computer network and another, or an edge between a digital network and the physical world which it selectively interfaces with on the other side of that network.
The self-contained world of the inside a computer network cannot totally be ignored to the extent that it interacts with people and phenomena in our own world. [Make sure that’s not repetitive with previous writing.] It’s an abstracted ideal state that doesn’t truly exist, just as some people might imagine the physical world as existing in a former technology-free idealized state.


[The computer’s world has edges because of how it interacts with people. Its world can collide with ours, and when it does the edges of it manifested in our world can be used to divide people – just like physical “air gap” barriers can separate computer systems by disrupting their internal world with our own. This chapter discussions situations where virtually-generated walls disrupt people’s lives.]
 [Possible term: byte gap = the inverse of an air gap. A digital space a person cannot pass through because of digital information rather than physical barriers. Like floors you can’t get to on an elevator ride, even if a door was constructed at the right location to access them, because they are unlisted[as].]
[Discussion of relationships between both physical and informational barriers in space, with examples.]
[Continuum where labeling and enumeration of rooms can either be a social equalizer – with corridors and the early replacement of floor names with floor numbers in response to the introduction of the elevator – but also create social divisions. In Victorian homes room labeling divided social classes, and elevators in buildings partially accessed via “poor doors” skip floors of people associated with lower classes.]
[Additional text]
[In a hypothetical scenario, the skipped floors traveled passed by an elevator heading straight for a sky lobby could have doors in that elevator shaft which didn’t open, and which the elevator didn’t stop for. If so, the elevator would be capable of functioning in exactly the same way.]
[Additional text]
[Discuss the topic of privacy in relation to spatial enclosure somewhere in this discussion as well.]
[Discussion of potential social implications, ie revisiting the social problem of the “poor door” and other systems of segregation like it in the context of information gained from my analysis so far.]
[Potential reference point: Kafka’s story “Before the Law” - a space which is inaccessible because of informational rather than physical barriers. What is its social context? That was also a protocolized space. Maybe mention earlier or more throughout!]
[Discuss how virtual walls occur in relation to elevator floors, montage. Elevator floors not shown = infill demarcating the boundaries of traversable space just like those of a wall you can’t walk through.]
[Does quantization just equal smooth/striated in Deleuzian terms?]
 [Maybe define specifically that I’m looking at digital as something readable by computers – how does a computer read analog info better than digital info?]


Certain real estate listing practices, variously described as “ghost,” “whisper,” or “pocket” listings, involve properties which are less officially placed up for sale outside of the industry’s standard digital listing databases. Those practices occur in part due to []
 [Organize research on algorithmic apartment pricing.]




[Cloud kitchen - Pasqually’s Pizza example. Illegibility. Hides a something behind a wall which consumers would otherwise rather see]
[Example of servant call bells – a digital gate temporarily changing the privacy and thus the function of a Victorian room?]


Air Gaps


If a clonal colony consists of a single organism, then it has circulation path connections spanning throughout its network. If one were to hypothetically evaluate an artificially simplified model of a clonal colony from which any biological nuances which would prevent it from acting as a full metaphor for a computer network were removed, it would be able to move a water droplet between two trees at its very eastern and western edges which might be a thousand miles away. Beyond the edge of those tree roots and branches, though, it could move the droplet no further. Phenomena like elevators riding on tracks and electrical circuits inside wires and traces work in a similar way. Indeed, one of the safest (but most extreme) practices for protecting computer systems in information security is to use the concept of an “air gap” in their external connectivity to prevent them from interacting with a wider network at all.
[Further writing]
An example of a similar concept operating within a built environment embedded with significant numbers of ubiquitous sensors and information-collecting affordances might be found in George Orwell’s 1984, where the protagonist’s awareness of unofficial labels and informational boundaries in his physical environment enabled him to temporarily protect himself against a specifically malicious version of a widespread electronic network. An alcove in his room exists outside the view of the two-way television screen monitoring his activity; within which he can privately write in a diary without his normal fear of having his actions monitored. [Too culturally charged of an example to include?] [at][Include footnote with quote from academic describing the novel as not necessarily anti-tech.] 
[Further analysis/discussion]
[Conclusion idea: another parallel built environment exists in terms of informationally-constructed walls. We might see those walls as a kind of protocol, which you can learn to see through protocolized thinking.]








Flat Elevators
[Developments leading up to conclusion. Something about the inherent challenges of seeing “digital walls”?]
[How spaces pass through / intersect with each other when hidden digital forms of segregation are unseen. Recap about role of the dimension of time in modulation those unseen barriers and passages in space, if that transition works.] Where computer systems exist in a different relationship with the three physical dimensions of our built environment from our own because of how they utilize the dimension of time in different ways from how we do, then Edwin Abbott’s classic 1884 story Flatland might be a potential precedent for further understanding them. [Especially since the book was also originally developed as a work of Victorian social commentary.] 
[Comparison between a computer network’s ability to simultaneously perceive and inhabit different physical spaces, which is something we can’t fully perceive, and the example of the 3D sphere passing through the 2D world of Flatland. The protagonist could not directly see it, but could somewhat understand it in an indirect way by tracking the changing forms of its 2D sections as it passed through his world. We can track the presence of physical and digital walls in our world in a similar way.]


Opportunities for new protocols


[NOTE: List is incomplete. Currently considering rewriting this to focus more specifically around strategies people can use to notice the presence or absence of “digital walls,” and tools for maintaining or destroying them in socially beneficial ways.]


1. Protocols for using automation to further increase spatial density (such as of housing). This could be achieved by
* Using automated systems to enable frictionless methods for multiple tenants or owners to privately inhabit the same space at different points in time. 
   * Example: two apartments share the same kitchen at different times.
2. Protocols for enhanced and modernized digital labeling of physical spaces
* Opportunities for new types of ownership and renting based on increased granularities of addresses in physical space. 
   * Examples: addresses for rooms instead of just apartments, opportunities to rent physical space in shorter increments of time than before, evolving apartment floor plans which encompass different combinations of nearby rooms at different points in time, less centralized networks of mailboxes. 
* Labeling to better distinguish similar physical spaces from each other.
* “Reverse labeling” to physically differentiate spaces with similar digital labels [???]
3. Protocols for privacy and security in relation to automated systems[au]
* Opportunities for automated systems and computers to share physical spaces with humans in situations where multiple humans sharing the same space would be unfeasible.
* Systems of virtual doors and locks used to provide a robot with access to a private space, including maintaining appropriate practices of digital privacy when it is in that space.
4. Representational methods for depicting and analyzing physical/digital space
* Strategies for visually representing spatial/informational networks - potentially a set of symbols inspired by the precedent of those found on electrical diagrams?
* Strategies for detecting and identifying unethical implementations of spatial/informational networks. 
   * Example: strategies for detecting informationally-constructed barriers in physical space which are used to impose various forms of segregation[av].
* A design for an interface (such as an app) for identifying duplicate physical spaces and objects, and informing users about that duplication at points in time where that information is relevant and beneficial.
________________
[1] Andreessen, Marc. “Why Software Is Eating the World.” Andreessen Horowitz, August 20, 2011. https://a16z.com/2011/08/20/why-software-is-eating-the-world/.[aw][ax]
[2] [To do: include a note here, with citations, about how “chaotic storage” isn’t always perfectly implemented; automated warehouses may involve areas incorporating more and less categorization, or varying layers of categorization.]
[3] Füegi, John, and Jo Francis. “Lovelace & Babbage and the Creation of the 1843 ‘Notes.’” ACM Inroads 6, no. 3 (October 1, 2003): 78–86. https://doi.org/10.1145/2810201. [To do: replace with more direct source.]
[4]Tantner, Anton. House Numbers: Pictures of a Forgotten History. Translated by Anthony Mathews. Reaktion Books, 2015. https://press.uchicago.edu/ucp/books/book/distributed/H/bo22282433.html.
[5]Jarzombek, Mark. “Corridor Spaces.” Critical Inquiry 36, no. 4 (2010): 728–70. https://doi.org/10.1086/655210. p. 748-761.
[6] [Find misplaced source on the adoption of doors in Georgian home interiors.]
[7]
[8] Evans, Robin. Translations from Drawing to Building. MIT Press, 1997. 63-65.
[9] Flanders, Judith. Inside the Victorian Home. (New York: W. W. Norton & Company, 2005), 9. 
[10]The past historical reception of the push-button as a sometimes controversial cultural influence has also been discussed in the context of elevators. See Bernard, Andreas. Lifted: A Cultural History of the Elevator. (New York: NYU Press, 2014.) 163-172.


[11] In some buildings the thirteenth floor may be a hidden mechanical floor, rather than being eliminated entirely. [Source or example here]
[12] Lee, Jaehong. “No More Skipping 4, 13, 14, 24 in Vancouver Floor Numbers.” Vancouver Sun, November 4, 2015. https://vancouversun.com/news/local-news/no-more-skipping-4-13-14-24-in-vancouver-floor-numbers.
[13] [168]
[14] [Is this capable of being connected to Deleuze’s “lines of flight” concept?
[15] [Note here it the corridor has a similar function. The elevator makes it more explicit though, ie moving a “camera” around while the viewer physically stands still?]
[16] []
[17] Koolhaas, Rem. Delirious New York: A Retroactive Manifesto for Manhattan. New York: The Monacelli Press, 1997. p. 155.
[18] Stungo, Naomi. “Koolhaas and the Gang.” The Observer, July 17, 1999. https://www.theguardian.com/theobserver/1999/jul/18/featuresreview.review1.
[19] [Reference for elevator automation.]
[20] [future citation to add]
[21] [future citation to add]
[22] [future citation to add]
[23] Glanz, James, and Eric Lipton. City in the Sky: The Rise and Fall of the World Trade Center. 1st edition. New York: Times Books, 2003. 112-113.
[24] []
[25] 
[26] Davidson, Lisa Pfueller. “Early Twentieth-Century Hotel Architects and the Origins of Standardization.” The Journal of Decorative and Propaganda Arts 25 (2005): 72–103. p. 86
[27] Uexküll, Jakob Von. “A Stroll through the Worlds of Animals and Men: A Picture Book of Invisible Worlds” 89, no. 4 (January 1, 1992): 319–91. https://doi.org/10.1515/semi.1992.89.4.319. p. 5-8


[a]Comment test.
[b]domain name of ip address can be a counter part of this
[c]⭐️
[d]1 total reaction
Chenoe Hart reacted with 👍 at 2023-07-19 16:39 PM
[e]!! feels like it is the core argument of this essay
[f]I agree - this is a strong statement of the main idea
[g]I think it might be helpful for the readers to see some real-life situations here they can feel that humans are accustomed to this digital based design of physical spaces - as a kind of storytelling!
[h]maybe we can do some parallel storytelling of a person's journey to find something in physical space / digital one. e.g. a hybrid worker's WFH day which is only about digital data retrieval and their next day in a physical office. About how they request data from a server located in miles away and how they actually commute to a space within this addressable space frame!
[i]Might be distracting to introduce the "protocol" concept before you're ready to fully explain what it means (I'm trying to figure that out as well in my essay). Maybe you're planning for that here...
[j]This section of the paper is fascinating! You tell a really interesting story and now I can't wait to see what you do next. You've already made me see the world differently.
[k]wow started very early!
[l]Very eye-opening commentary on relationship between architectural configurations, information flows, social relations
[m]getting smaller and smaller and now even divided by time(e.g. coworking space / airbnb)
[n]⭐️
[o]interesting to think about the informational types of walls - in digital space, we sometimes even cannot realize there is a wall if there's no interface for the blocker provided.
[p]I struggled to understand what you meant by "digital" until here. Do you mean binary? Does it mean broken into parts? Might be helpful to define it very early on in this section.
[q]!
[r]One way we retained this in elevators is with the PH button :-)
[s]This is really interesting because it shows how irrationality can persist within a system that is becoming more rationalized - that could almost be its own essay.
[t]A bit random, but as someone with 0 architecture background, I did not know these "floor tricks" were used at the "professional" level like this: I thought they were just a way to "fool retail users", and that, for example, a 15 floor building which is labelled up to 16 in the elevator (and skips 13) would still be advertised as a 15 story building for real estate purposes. Seems wild that's not true - and potentially worth expanding on, as it might shock more readers about the extent to which the digital design we give things affects their real world perception.
[u]That's a useful perspective to hear, since I've been in the process of re-evaluating how much attention I give to that issue.
[v]there's a link here back to your "light physical sensations" part earlier: you can feel your ears buzz going from floors 1-20, but you don't notice whether you've gone up 19/20/21 floors, whereas if you take the stairs, you do!
[w]I took out a reference to that when I was trying to trim my count; this comment makes me think about putting it back in.
[x]I always wonder what my dog thinks when we take an elevator - to him it's probably a "magic wormhole" to different places you can't reach without going through the magic door again...!
[y]Also a characteristic of so many modern transportation modes, culminating with the freeway where the ability to bypass undesirable space was often an explicit goal (I know you've written about transportation and may also get into this below, just noting it here).
[z]⭐️
[aa]counter-counter-arguments: buildings where this is the case typically segment elevators in chunks (floors 1-10, 11-20, etc) so it's not perceptible
[ab]better example here might be a server - same "best case" response time, but when it gets overloaded, users' experience degrades
[ac]I like the idea of referencing an overloaded server, since I've been meaning to include more computer references throughout the document. I'd need to make sure I establish that the elevator situation is unique to an overloaded server vs. some other kind of overloaded non-digital system, but I think I could do that by referencing how the numerical floor requests are digital.
[ad]thinking about time complexity of algorithm from CS field, the tape recorder and elevator will be the case of 'linear time complexity O(n)' which means that the algorithms take proportionally longer to complete as the input grows
[ae]some related docs: https://en.wikipedia.org/wiki/Time_complexity
[af]https://www.mygreatlearning.com/blog/why-is-time-complexity-essential/#what-is-time-complexity
[ag]so it is not recommended to use such data types requiring these types(>= linear time complexity) of algo to iterate what's inside(e.g. list type in Elixir) from the beginning, but for the elevator's case, interestingly, it is unavoidable
[ah]meaning humans&elevator are traversing the building floors to get to the desired floor(data)
[ai]Interesting perspective; I'm working on a section about how that complexity is managed with developments like sky lobbies and destination dispatch systems, but this reference gives me another way of thinking about it.
[aj]I would expand this even more, to an almost ELI5 level
[ak]⭐️
[al]I wonder if making this section about houses/condos/apartments could make it resonate more with people? I suspect a similar phenomenon happened, and it feels more drastic, in a way, for people's main living space to be re-shaped than for "temporary spaces" like hotels. 


Also, if you are going down the hotel path, I wonder if it's worth ending with sleeping pods (which currently aren't elevator-enabled and still height-limited by ladders!)
[am]Back to the freeway example - series of highway rest stops within a given section/state are often fully identical, down to the layout of the offramps/parking lots/etc. For a long distance driver it creates a paradoxical sense of stasis as they traverse hundreds of miles.
[an]I think this might be a great section to include the Amazon warehouse example you mentioned in the past, which was really interesting!
[ao]I've thought about this searchability quality a lot with respect to media (music, TV/film, books) and how analog structures of transmission made the search/discovery process more intuitive, whereas Spotify/Netflix force you to submit to a more streamlined interface.
[ap]⭐️
[aq]Venkat wrote an interesting essay about AI and how computers' lack of embodiment affects their possible worldview https://studio.ribbonfarm.com/p/beyond-hyperanthropomorphism
[ar]⭐️
[as]⭐️
[at]I think so, yes. I feel like a less charged example can make the same point (but can't come up with an example off the top of my head...!)
[au]IoT devices were used as a massive botnet during the Dyn attacks (took down half the internet down in 2016). could be interesting to look at here
[av]How to discover an "unlabeled poor door"
[aw]the footnote here feels sufficient - attributing this to him in the main paragraph is more distracting than "value add" to a really interesting context-setting section!
[ax]Agreed

Artificial Memory and the Orientation of Infinity


Hundreds of years before the term artificial intelligence, debates about the development of “artificial memory” erupted. Distinguished from natural memory, artificial memory involves using aids designed to retain greater amounts of knowledge. Artificial memory meant both the development of internal memory aids, enriching personal memory through memorization techniques, and external memory aids like encyclopedias, enriching societal memory through the organization of knowledge and proliferation of media. However in these debates, not all saw the development of artificial memory as a positive thing. In 1600, the Renaissance physician Cornelius Agrippa cautioned against it:


“Artificial memory would not be able to last for the briefest second without natural memory… [Artificial memory], by overburdening the natural memory with innumerable images of words and things, can lead those who are not content with the limits imposed upon them by nature to the point of madness.”[1]


Worrying about madness from the experience of “innumerable images of words and things” might evoke a sardonic laugh from anyone alive today, but this disjunction between past and present perceptions of memory may be precisely why we should return in the twenty-first century to debates on the topic. A seemingly simple term used casually in daily conversation, memory reveals itself upon closer scrutiny as a surprisingly complex concept, especially when viewed simultaneously through historical, psychological, and neurological lenses. Yet both modern neuroscience and ancient literature do not consider memories static objects, but evolving and imperfect experiences that change through their mere recollection. Could we say the same of a public memorial statue? Where does a personal memory end and a public memory begin, and what is the role of new technology in all of this?


Ultimately, how we relate to memory over time produces an interesting history of humankind, and one that could shed light on changes coming from new technologies. Philosopher Bernard Stiegler considers humans homo technics, that is, the distinguishing feature of humankind as a species lies in the creation of technology, ranging from simple to complex tools. Such an innate drive to create technology, for Stiegler, deeply relates to memory. In his books on Technics and Time, Stiegler conceptualizes three types of memory, which could be described as genetic memory, epigenetic memory, and technical memory. Technical memory plays the role of a bridge between the two other types of memory, genetics and epigenetics, or more simply put, technical memory makes more than just the genetic memory of a human available to the next generation of humans. He gives an example using the archetypal object of the anthropologist, the flint arrowhead:


When a prehistoric man cuts flint […] obviously he doesn’t cut flint to preserve his memory. But the act of cutting the flint preserves in the stone the gesture of cutting, permitting the inscription of his gestures on the flint, and in fact, constitutes a new memory support.[2]


The construction of every tool also constructs a “memory support”, not only for those using it but for the generations to follow. In this sense, changes in the production of technology could be seen inherently as changes in the production of memory. Our technical archives shape the paths our descendants walk. Our language tools carve the way they think, the tools they will create. When all tools on some level convey artificial memory, Agrippa’s caution in the seventeenth century against pursuing artificial memory seems futile. However, examining what we traditionally consider memorization techniques can reveal more implicit models of how technology operates. Perhaps even desire for changes in how we relate to memory could be seen as an animating force behind the production of technology itself. 


Shy of such monumental claims yet, undoubtedly a resemblance exists between models of memory and models of technology. For example, computational models of the brain provide both empirical and metaphorical grounding for what “memory” means today. [Examples and metaphors here.] Looking at how different societies relate to memory throughout history shines light on a side door through which a different mode of analysis of technology can be entered.


  

[More on this diagram]
“Places are like matter and images are like form”: The protocolization of memory[3]




A traditional Western history of memory usually begins with a Hellenic lyric poet sometime around 500 B.C. As the story goes, Simonides recounted an epic poem at a dinner party and stepped outside, shortly after which the banquet hall collapsed. The guests, deceased and disfigured, could only be identified by Simonides’ memory. In his imagination, he walked through where each guest sat at the dinner table, one by one. This memorization technique would go on to be known as the method of loci: placing facts at specific locations one remembers. Such was the popularity of the method of loci that it became one of the five pillars in the rhetorical arts established by master orator Cicero in XX AD. Memory, alongside pillars such as X and X, could be practiced through a clear procedure. First, one should choose a “formal structure”, such as a church, school, or other familiar place, and then, within the formal structure, establish a number of “memory-places”.[p. 19] In each of these places, a memory “image” could be stored. For example, if one wanted to learn the list of the planets, one could recall their childhood church and use each alcove in the church as a memory “place” to store a memory “image” of a planet, including its name and related facts. Each time one wanted to recall the list of the planets, one could imaginally stroll through the church, facts coming to mind about a specific planet while passing each alcove. 


[Diagram explaining formal structure, memory place, and memory image relationship]


This surprisingly effective memorization technique would have hundreds of pages devoted to improving it throughout antiquity. Several canonized Western philosophers, from Aristotle to Aquinas, would articulate some variation of the practice. Advice varied but often suggested choosing a “medium-sized” formal structure without too many people passing through it, such as, for example, not using a public square.[18] Interestingly, it did not matter whether this formal structure actually existed, only that the practitioner could recall it vividly in imagination. The overall imaginative quality played a key role in this memorization technique, as a popular pamphlet wrote: “That which is extraordinary impresses the memory better than the ordinary. For this reason [...] the first philosophers composed poetry, because fables, which are composed of marvelous things, leave a greater impression on the memory.”[p. 10-11] The emphasis on using “extraordinary” and “marvelous” memory places and images would continue throughout the memorization technique’s popularity from 500 BC to 1500 AD among Hellenic societies. It is more than coincidental that the story passed down describing the memorization technique itself had macabre and extraordinary tones in Simonides’ extremely fortuitous survival leading to elucidation of the method. Would the story and technique have survived with a less somber tale? Thus it was recommended not only the memory places that should have highly emotionally charged content, but that the connections between the memory places themselves also be embedded within a compelling story.


Scholar Johannes Ravenna would carry Cicero’s torch that established the method of loci, and memory more broadly, as a pillar of rhetoric. Its light would burn steadily for the next 1400 years, as a practice passed down, changed, and revived over time, being taught in most formal academies as they multiplied. The method of loci’s durability lies in how it provided a developing system encompassing a universal procedure for the recollection of a particular experience. Its formulaic nature, following the simple steps of choosing a formal structure, memory places, and memory images that can be embellished over time, allowed its gradual adaptation and continued use. But also, its formulaic nature gave way to an incredible personal relation to the facts such a method sought to remember, “marvelous” and “extraordinary” for each practitioner in highly individualized ways. With this combination of the universal procedural and the affectively personal, the method of loci marks what could be the first successful protocolization of memory in the context of the Western philosophical tradition, evinced by its ability to withstand the inevitable evolutionary pressure of a rapidly changing culture while retaining the basic structure of the memorization technique.


  



In the classic Rhetorica ad Herennium, Cicero distinguished between natural memory and artificial memory as the latter being that which uses “places and images” . Thus the realm of artificial memory, as the term was constructed, primarily referred to the specific practices and techniques, sometimes referred to as “mnemotechnics”, which followed a universal procedure with a highly individualized and affective outcome. In this sense, we could understand artificial memory to refer to memory as protocol, a regime that produces serendipity rather than certainty, by employing formal aids to achieve informal, personal recollection. This protocolization would go on to deeply influence the traditional Western canon’s thinking on memory for the next millennia, convincing scholars that highly formulaic techniques, reproducible across media, could elicit highly personalized results that still articulated an image of the world.
A diagrammatic theory of mind
Another influential practice in this history of memory would come in the twelfth century from Raymond Lull. Lull was on a mission to convert others to Christianity. In his vision and intolerance, he developed his main work Ars Combinatoria, the combinatory arts, which he would sometimes refer to simply as Ars, the Art. He took an approach to try to unify several strands of disciplines toward explaining God. Sometimes framed as question and answers (Does good? A: )[x], sometimes framed as lists, Lull sought to develop a method beyond simply textual, narrative descriptions of his vision of faith. Such a list might include: X, Y, Z. Such categorical endeavors eventually led Lull to create “memory wheels”, an artifact that would become synonymous with Ars combinatoria. These memory wheels tried to explicate moral frameworks within diagrammatic form, creating spatial relationships between related values. For example, from the X:
[X image of wheel from video]
Each time the inner wheel turned, a new relational concept emerged. Such as [x[. While Lull’s work did not enjoy immense popularity in his lifetime, it would go onto influence other thinkers for the next several centuries, with his work sometimes even receiving credit for early theories of computation.x][ However, Lull’s zealotry is of less interest than what his successors did, in particular one successor named Giordano Bruno. A defrocked monk, Giordano Bruno gives the impression that he consistently considered himself the smartest person in the room. Sometimes he might even have been, at least until his death from being burned at the stake in YEAR after a five year trial in Florence for challenging the church. Bruno’s heretical ideas included considering the universe infinite and the earth not at the center of it. Throughout his life, he wrote a series of arcane texts on whatever subjects he liked, often presenting them to capricious heads of state. One such book, the joyfully enigmatically titled De umbrias ideareum, or On the Shadows of Ideas, dealt in memory. Its opening sentence promised to reveal secrets to the reader. Dedicated to King Henri III of France, such opportunities would grant Bruno the opportunity to develop this work in the following years including several books on memory seals, statues, and images respectively from the years 1583 to 1591. In his eclectic memory work, Bruno sought to combine traditions. He explored thThe compulsive desire for a universal languagee work of St. Thomas Aquinas, whom Bruno admired as an ex-Dominican monk himself, through Lullian wheels. Aquinas worked firmly within the Aristotelian and rhetorical approach to memory practices, which espoused a certain instrumental utilitarianism in their application, even if they quested for greater knowledge of God. Lull also had theological aims, and his work perhaps only came to sit staunchly within the tradition of memorization techniques retrospectively, largely through the later work of Bruno. Bruno is as Bruno does, however, and in combining the work of Aquinas and Lull, he transformed them both.


  
`


Narrated by the scholar Frances Yates in the absolutely seminal The Art of Memory published in 1966, Bruno’s work demonstrates the practice of not only techniques for changing memory but techniques for changing consciousness. It could be said Bruno recognized memory as one of the dominant constituents of consciousness itself, echoing earlier thinkers like the eleventh century physician and philosopher Avicenna who encouraged the therapeutic potential of examining memory.[X] In On the Shadows of Ideas, Bruno adapts the Lullian wheel to include thirty segments with letters on its outermost circle, sometimes with several concentric circles inside, all also with thirty segments. His book also contains several lists of memory images, each with 150 images given in five sets of thirty. In perhaps her greatest discovery or most unscholarly failing, though I tend to side with the former, Yates takes the liberty to place these memory images on a Brunian wheel, with the incredibly detailed result:


[X]


Why Yates pursues this bold move lies in its perfect exposition of Bruno’s theology. Taking the phrase “As above, so below”, Bruno continually explored the connection of humankind and the cosmos, the potential reflection of both in each. Through creating this diagram, Yates places human images on the outermost circle with astronomical images on the innermost circle, and as one turns the wheels, one reflects on, in fact recalls, new combinatorial possibilities of the cosmological divinity of humankind. This example of reflecting on the divine exists in Bruno’s later memory works, which less overtly reference Lullian wheels but, of course, take them further. In Bruno’s book Thirty Seals, he also lists memory places and images, beginning with the “Field” and the “Heaven”, both of which contain advice for memorization techniques, and moves onto the “Tree” and the “Wood”, tropes representing all knowledge in the work of Lull. Diagrams accompany each of these thirty seals listed in the book.


[X][X]


With each of these diagrams, one memorizes the concepts associated with each, as in the Field, where one recalls how it’s best to use memory images with a “striking and unusual” character. But it’s important to note that more than memorizing concepts, one also deeply internalizes the diagrams themselves and a larger relation of the concepts to one another. As Bruno’s work suggests:


        Black diamond doors of psyche quote


Just as in David Lynch’s Dune when Paul Atreides realizes during a fantastical montage scene that worms are, in fact, spice, in Bruno’s approach to memorization techniques, diagrams are the concepts. What ever could this mean? As Frances Yates writes, “These pages of [Bruno’s] are a kind of manifesto of the primacy of the imagination in the cognitive process which he refuses to see as divided among many faculties but as all one.”[251] It means, in one sense, that contemplating a concept cannot be superficially divided from the formal qualities that convey that concept, such as in this case, diagrams. What seems as needless and anachronistic esotericism in Bruno’s work actually echoes premier media scholar (and it’s worth noting, theologian) Marshall McLuhan’s popularization of “The medium is the message”. Put in even more popular terms, ripping off classic dietary advice, you are what you think. For Bruno, artificial memory does not mean solely following a universal procedure to recall particular experience, but instead, following a universal procedure to recall particular experience that is in itself a procedure to change consciousness. The diagrams do not simply point to the cosmological divinity of humankind, but they help to produce it through their contemplation. Insistence on this point could actually be considered radical amidst the intellectual environment of the preceding millenia, in which many scholars occupied themselves reproducing manuscripts about Aristotle’s five senses or their increasingly encroaching compulsion to exhaustibly catalog the inexhaustible nature of life in the universe through tables and taxonomies, a tendency which would gain its full momentum by the next century. For Bruno, artificial memory is not necessarily about cataloging concepts, but becoming them. This intuition about memory becomes more clearly captured by the arts, as Prousts writes:
We find a little of everything in our memory; it is a sort of pharmacy, a sort of chemical laboratory, in which our groping hand may come to rest, now on a sedative drug, now on a dangerous poison.[X]
The memory resembles a pharmacy, like Philip K. Dick’s Dial-a-Mood machines in which you enter a number into a jukebox-like contraption and an emotion comes out. However, amidst everyday memory we usually have far less agency. As Proust writes “our groping hand may come to rest” on different memories, accidentally stumbling upon them and their narcosis. While on one hand, Bruno’s approach to memorization techniques closely resembles wishful thinking on the road to madness, as though belief produces truth, on the other hand, Bruno’s approach resembles a form of acknowledgement, a form of agency, that memory, while often involuntary, when cultivated can produce different consciousness. Modern therapeutic regimes fall within this approach, seeking to reorient relationships to emotional memories, and even in some cases like EMDR therapy, to overwrite emotional memories.[x] And of course, here surfaces the metaphor of computational models of the brain, that memories could be overwritten like zeroes and ones. In the glow of the LED screens so prevalent today, the traditions of artificial memory as diagram, consciousness, and pharmacy contain a key insight relevant for us now. One that waited in relative obscurity during the five centuries that followed Bruno as the more official Western canon of artificial memory took its course.


Keys, patterns, and the encyclopedic impulse 
Clavis et normae, keys and patterns, created the foundations for the praxis logica formulated by a relatively unknown artificial memory scholar Johann Heinrich Bisterfield.[Logic 143] These keys and patterns consisted of definitions of concepts. Not just any concepts, these were primary concepts, transcendental, perhaps at the root of all concepts themselves. In order to know these concepts, one needed a dictionary of them. In order to parse this dictionary, one needed an encyclopedia through which such dictionaries could be organized. Artificial memory would be transformed by this compulsive desire to organize knowledge, and its compulsions, often helpful, pervasive, and extremely normative, guide us today in the form of the encyclopedia. However, it’s worth tracing how artificial memory, protocols consisting of universal procedures for recollection of particular experience, became a public practice. That same Johann Heinrich Bisterfield wrote about the aim of his encyclopedic project to create the “pictorial amphitheater of the world”.[Ibid/143] This metaphor of memorization techniques as theaters has a longer history.


As the method of loci instructed, memory could be achieved through choosing a formal structure, memory places within that structure, and memory images within those memory places, and then walking through them imaginally for recollection. These formal structures sometimes took the form of a church, sometimes the form of a childhood home. Their popularization as the form of a theater would begin in Renaissance Italy. Though never completed, ultimately remaining more of a legend than a realized location, the memory theater of Giulio Camillo in the early sixteenth century would capture a certain zeitgeist. Said to contain “many images”, “full of little boxes”[136 AoM], the memory theater initiated by Camillo with ascending rows corresponding to the seven visible planets would “locate and administer all human concepts, everything which exists in the whole world”, such as facts about natural sciences and the aspirations of art.[X MoM] Through memory theaters, inspired by Camillo and others, Renaissance era philosophers constructed what could be considered virtual architecture, explicitly combining the notion of public space, a cosmological orientation, and a knowledge system. While the suggestion that memory theaters may administer knowledge of “the whole world” smells of hubris, not unlike the scenario modeling of RAND Corporation today, still their claim that such virtual architecture gives us an image of a world can be recognized as a moment in which the Western canon of artificial memory took a collective turn: the particular recollection becomes a public space with purportedly universal knowledge for others to traverse, internalize, and recollect.


Camillo’s memory theater predated Bruno, and it would find influence within Bruno’s approach. Where the Western canon of artificial memory follows this collective turn, however, could be traced to the encyclopedic impulse, under the influence of scholar Petrus Ramus and evident in the work of people like Bisterfield. In the X century, Ramus would become known as his work as an early logician, suggesting the bifurcation of rhetoric into two distinct fields: rhetoric and dialectic. More familar today, dialectic approached knowledge [x]. Like each approach to artificial memory, dialectic cultivated mediums key to its expression, such as the table:


[Image] 


As a medium, the table clearly delineates hierachical, categorical relationships between concepts. No longer the revolving memory wheels of Bruno’s art, tables expressed an empiricism present through their static quality: the relationship between concepts became fixed in time, only ameliorated by thesis, antithesis, and synthesis in forward progression. The dialectic approach enlivened what could become the great encyclopedic impulse of the eighteeneth century. Remembered through figures like [full name] Alstead and exponentiated through typesetting, suddenly the quest to unite the knowledge of the world on the printed page replaced a generational search for the Holy Grail. 


* Alstead encyclopedias


While some historians such as Pierre Gasendi insist Ramus placed artifical memory neither within rhetoric or dialectic but apart within psychology[102], in Logic and the Art of Memory Paolo Rossi suggests how situating artifical memory within the the practice of logic might be the most lasting influence of Ramus. As Rossi writes, for Ramus memory took on a “ordering function”, defined as a method of “systematic and ordered disposition of notions”, which in Rossi’s opinion “abosorbs many of the ‘rules’ of mnemotechnics”.[101] This perhaps even over- “idenfitication of memory and method” placed artifical memory squarely within the practice of logic as Ramus defined it, using memory sometimes interchangeably with the terms judgement and disposition, both core to the practice of logic.[100-101] Such positioning, Rossi argues, would make artifical memory a fundamental element in the forge of thought leaders to come.


The compulsive desire for a universal language
Rene Descartes, that lingering thought leader of the X century, could not escape having an opinion on artifical memory. Openly critical, Decartes in Discourse on Method described an older man versed in artificial memory who “spoke without judgment” to “explain to others what they already knew”, having memorized topics more than he had learned them.[104 LoM] Francis Bacon and others mirorred such dissapointment in their public texts, but not without picking up the discplinary shift cued by Ramus, that beyond its supposedly superficial practice within rhetoric, wheels, or palaces, artificial memory could be redeemed and in a way lay at the core of a practice of logic. Instead, the logicians preferred, for instance, stakes and ladders. As Decrates muses, the “true art of memory” consists of “reduction of things to their causes”, as “one should not just consider the nearest [memory] image when copposing a new image, but also the others in the series, so that the fifth is connected to the first by [...] a stake [...], the image in the middle by [...] a ladder” [113]. Rehabiliating artificial memory, Descartes preferred more complex signications between memory images, rather than the linear stroll though the memory palace. However, such complex relationships do clearly exist in Lullian wheels and the Brunoian art. For Descartes, the important difference seems to lie in what the memory images must represent, which he stresses as primary “causes” with deductive logic between them rather than mere snippets of trivia for superificial oratory. With this intention in mind, both Descartes and Bacon rehabilitate the idea of “memory aids”, Bacon wrote…


* Bacon and Descartes “memory aids”
* Leibniz’s universal language


* Bacon and Descartes “memory aids”
* Leibniz’s universal language
Memory as orientation
* Computer memory and random access memory history
* Chaotic storage and memory through automation and “just-in-time” logistics
   * Procedural logic that produces results not predictable to humans
* Warburg and Mnemosyne Atlas, how Warburg’s abstract, associative connections prefigured the modern shift to memory as orientation
   * Pathosformel
   * Diagramaka
* Bruno’s great return!
* Lukasa board
* Songlines


  

* These are all evident of a more understandable nature of memory, despite being led astray in the 17th century, of memory as orientation: an affective sense-making tool that takes the place of deductive logic in the face of contemporary technology
The interruption of infinity
* Memory is not the infinite replication of digitized data but incisions in space, relations between mediums
   * Interruption of infinity vs infinite data-fication
* Traditionally memory perceived as recording, whereas knowledge is perceived as synthesis and comprehension, with new tech these functions are reversed because strict recording no longer required or even feasible to expect of memory – it becomes a process of orientation in infinity of “innumerable images and words” of our media and experience 
* AI resembles memory more than traditional memory aids because of its ability to make connections and produce relative novelty from them
* From here we can understand digitization, database-ifcation, and on-chain maximilaism to be in response to a misunderstanding of memory
* Memory as protocol, not object familiar refrain


When Tobias a￼sked the angel if he knew
The way to the city of Media, he responded
By saying he often walked there, through its streets,
Among the plenitude of things. The angel
Was not in a place as we are, but the place
was in him - intuitus, whole - as he was not
passing through time - beginning to end -
but held all of it everywhere-happening
And though he sat to eat with men
And took food into the mirror of his body
he had no need for memory.
“The Art of Memory”, Jemma Borg


Footnotes


[1] Agrippa, De vanitae scientiarum, cap. X (‘De arte memortativa’), Opera (Lyons, 1600), II, p. 32. Also page 67 in logic and art of memory[2] Bernard Stiegler, The Ister, 2004
[3]: Logic and the Art of Memory, page 10
________________




OLD DRAFT 


—-
Artificial Memory and the Interruption of Infinity[a]


Hundreds of years before the term artificial intelligence, debates about the development of “artificial memory” erupted. Distinguished from natural memory, artificial memory involves using aids designed to retain greater amounts of knowledge. Artificial memory meant both the development of internal memory aids, enriching personal memory through memorization techniques, and external memory aids like encyclopedias, enriching societal memory through the organization of knowledge and proliferation of media. However in these debates, not all saw the development of artificial memory as a positive thing. In 1600, the Renaissance physician Cornelius Agrippa cautioned against it:


“Artificial memory would not be able to last for the briefest second without natural memory… [Artificial memory], by overburdening the natural memory with innumerable images of words and things, can lead those who are not content with the limits imposed upon them by nature to the point of madness.”[1]


Worrying about madness from the experience of “innumerable images of words and things” might evoke a sardonic laugh from anyone alive today, but this disjunction between past and present perceptions of memory may be precisely why we should return in the twenty-first century to debates on the topic. A seemingly simple term used casually in daily conversation, memory reveals itself upon closer scrutiny as a surprisingly complex concept, especially when viewed simultaneously through historical, psychological, and neurological lenses. Yet both modern neuroscience and ancient literature do not consider memories static objects, but evolving and imperfect experiences that change through their mere recollection. Could we say the same of a public memorial statue? Where does a personal memory end and a public memory begin, and what is the role of new technology in all of this?


Ultimately, how we relate to memory over time produces an interesting history of humankind, and one that could shed light on changes coming from new technologies. Philosopher Bernard Stiegler considers humans homo technics, that is, the distinguishing feature of humankind as a species lies in the creation of technology, ranging from simple to complex tools. Such an innate drive to create technology, for Stiegler, deeply relates to memory. In his books on Technics and Time, Stiegler conceptualizes three types of memory, which could be described as genetic memory, epigenetic memory, and technical memory. Technical memory plays the role of a bridge between the two other types of memory, genetics and epigenetics, or more simply put, technical memory makes more than just the genetic memory of a human available to the next generation of humans. He gives an example using the archetypal object of the anthropologist, the flint arrowhead:


When a prehistoric man cuts flint […] obviously he doesn’t cut flint to preserve his memory. But the act of cutting the flint preserves[b] in the stone the gesture of cutting, permitting the inscription of his gestures on the flint, and in fact, constitutes a new memory support.[2]


The construction of every tool also constructs a “memory support”. In this sense, changes in the production of technology could be seen inherently as changes in the production of memory.[c] Our technical archives shape the paths our descendants walk. Our language tools carve the way they think, the tools they will create. When all tools on some level convey artificial memory, Agrippa’s caution in the seventeenth century against pursuing artificial memory seems futile. However, by looking at what we traditionally consider memory techniques can reveal more implicit models of how technology operates. Perhaps even desire for changes in how we relate to memory could be seen as an animating force behind the production of technology itself. 


Shy of such monumental claims yet, undoubtedly a resemblance exists between models of memory and models of technology. For example, computational models of the brain provide both empirical and metaphorical grounding for what “memory” means today. [Examples and metaphors here.] Looking at how different societies relate to memory throughout history shines light on a side door through which a different mode of analysis of technology can be entered.


  

[More on this diagram]
“Places[d] are like matter and images are like form”: The protocolization of memory[3]
A traditional Western history of memory usually begins with a Hellenic lyric poet sometime around 500 B.C. As the story goes, Simonides recounted an epic poem at a dinner party and stepped outside, shortly after which the banquet hall collapsed. The guests, deceased and disfigured, could only be identified by Simonides’ memory. In his imagination, he walked through where each guest sat at the dinner table, one by one. This memorization technique would go on to be known as the method of loci: placing facts at specific locations one remembers. Such was the popularity of the method of loci that it became one of the five pillars in the rhetorical arts established by master orator Cicero in [X] AD. Memory, alongside pillars such as [X] and [X], could be practiced through a clear procedure. First, one should choose a “formal structure”, such as a church, school, or other familiar place, and then, within the formal structure, establish a number of “memory-places”.[19] In each of these places, a memory “image” could be stored. For example, if one wanted to learn the list of the planets, one could recall their childhood church and use each alcove in the church as a memory “place” to store a memory “image” of a planet, including its name and related facts. Each time one wanted to recall the list of the planets, one could imaginally stroll through the church, facts coming to mind about a specific planet while passing each alcove. This surprisingly effective memorization technique would have hundreds of pages devoted to improving it throughout antiquity. Advice varied but often suggested choosing a “medium-sized” formal structure without too many people passing through it.[p. 18] Interestingly, it did not matter whether this formal structure actually existed, only that the practitioner could recall it vividly in imagination. The overall imaginative quality played a key role in this memorization technique, as a popular pamphlet wrote: “That which is extraordinary impresses the memory better than the ordinary. For this reason [...] the first philosophers composed poetry, because fables, which are composed of marvelous things, leave a greater impression on the memory.”[10-11] The emphasis on using “extraordinary” and “marvelous” memory places and images would continue throughout the memorization technique’s popularity from 500 BC to 1500 AD among Hellenic societies. It is more than coincidental that the story passed down describing the memorization technique itself had macabre and extraordinary tones in Simonides’ extremely fortuitous survival leading to elucidation of the method. Would the story and technique have survived with a less somber tale? Thus it was recommended not only the memory places that should have highly emotionally charged content, but that the connections between the memory places themselves also be embedded within a compelling story.


Scholar Johannes Ravenna would carry Cicero’s torch that established the method of loci, and memory more broadly, as a pillar of rhetoric. Its light would burn steadily for the next 1400 years, as a practice passed down, changed, and revived over time, being taught in most formal academies as they multiplied. The method of loci’s durability lies in how it provided a developing system encompassing a universal procedure for the recollection of a particular experience. Its formulaic nature, following the simple steps of choosing a formal structure, memory places, and memory images that can be embellished over time, allowed its gradual adaptation and continued use. But also, its formulaic nature gave way to an incredible personal relation to the facts such a method sought to remember, “marvelous” and “extraordinary” for each practitioner in highly individualized ways. With this combination of the universal procedural and the affectively personal, the method of loci marks what could be the first successful protocolization of memory in the context of the Western philosophical tradition, evinced by its ability to withstand the inevitable evolutionary pressure of a rapidly changing culture while retaining the basic structure of the memorization technique.


In the classical text Rhetorica ad Herennium, Cicero distinguished between natural memory and artificial memory as the latter being that which uses “places and images”. Thus the realm of artificial memory, as the term was constructed, primarily referred to the specific practices and techniques, sometimes referred to as “mnemotechnics”, which followed a universal procedure with a highly individualized and affective outcome. In this sense, we could understand artificial memory to refer to memory as protocol, a regime that produces serendipity rather than certainty, by employing formal aids to achieve informal, personal recollection. This protocolization would go on to deeply influence the traditional Western canon’s thinking on memory for the next millennia, convincing scholars that highly formulaic techniques, reproducible across media, could elicit highly personalized results that still created an articulate version of the world, a cosmos, a world image.
A diagrammatic theory of mind


* More on Aquinas and the connection between memory places
* Continuation of Cicero’s work through the work of Ravenna
* The method of loci and its embeddedness in rhetorical arts in the first millenium AD marks what could be considered the first (remembered) explicit “protocolization” of memory in the West, i.e. developing a universal procedure for the recollection of a particular experience
* Method of loci, as a protocol, was simultaneously highly formalized as well as highly adaptable to suit any given practitioner
* Cicero Rhetorica ad Herennium distinguished between natural and artificial memory
* Artificial memory was highly protocolized as an explicit tool
* This protocolization of memory would go on to influence all thought surrounding the topic in its wake
A diagrammatic theory of mind
* Raymond Lull and memory wheels
* Bruno as early consciousness studies
* A diagrammatic theory of mind, in which the structure or phenomenology of memory retained becomes one's consciousness itself
* Proust and memory as pharmacy
   * We find a little of everything in our memory; it is a sort of pharmacy, a sort of chemical laboratory, in which our groping hand may come to rest, now on a sedative drug, now on a dangerous poison. Marcel Proust, In Search of Lost Time
* Contemporary pharmaka and 11th century Avicenna on the therapeutics of memory
* However, this intuition lost out to a more rational compulsion…
Keys, patterns, and the compulsive desire for a universal language
* The lineage chosen by the West
* Ramus dialectics
* Alstead encyclopedias
* Ramus positioning memory as logic
* Bacon and Descartes “memory aids”
* Leibniz’s universal language
Memory as orientation        
* Computer memory and random access memory history
* Chaotic storage and memory through automation and “just-in-time” logistics
* Warburg and Mnemosyne Atlas, how Warburg’s abstract, associative connections prefigured the modern shift to memory as orientation
   * Pathosformel
   * Diagramaka
* Bruno’s great return!
* Lukasa board
* Songlines


  

* These are all evident of a more understandable nature of memory, despite being led astray in the 17th century, of memory as orientation: an affective sense-making tool that takes the place of deductive logic in the face of contemporary technology
The Orientation of Infinity
* Memory is not the infinite replication of digitized data but incisions in space[e][f], relations between mediums
   * Interruption of infinity vs infinite data-fication
* Traditionally memory perceived as recording, whereas knowledge is perceived as synthesis and comprehension, with new tech these functions are reversed because strict recording no longer required or even feasible to expect of memory – it becomes a process of orientation in infinity of “innumerable images and words” of our media and experience [g]
* AI resembles memory more than traditional memory aids because of its ability to make connections and produce relative novelty from them
* From here we can understand digitization, database-ifcation, and on-chain maximilaism to be in response to a misunderstanding of memory
* Memory as protocol, not object[h][i]
________________
Footnotes


[1] Agrippa, De vanitae scientiarum, cap. X (‘De arte memortativa’), Opera (Lyons, 1600), II, p. 32. Also page 67 in logic and art of memory
[2] Bernard Stiegler, The Ister, 2004
[3]: Logic and the Art of Memory, page 10


















________________


Outtakes 


How we relate to memory could form a history of humankind. In ebbs and flows, its definition sometimes coincided with knowledge and sometimes strayed. 


Both memory and knowledge often imply initiation, transmission, and revelation. Such as more precise knowledge of when the Nile may flood, revealed by inscriptions only visible when its water lowered, and then only to priests who could access these inscriptions.[x] 


Memory implies something lived, stored, or transmitted through time.


Always adjacent but slightly different from knowledge, memory often implies something that requires revelation.


   * The Western history of memory often begins with the Hellenic lyric poet Simonides, who used places and images in the imagination for recall. This practice could take the form of walking through one’s first home in one’s imagination and remembering, at each familiar alcove, a line from an epic poem. Known as the “method of locii”, this practice acted as a universal protocol an individual could use for the recollection of particular memory, and it continued to be taught as a memory protocol within the field of rhetoric for the first millennium C.E. The next millennium, however, saw greater divergence in protocols for memory. In one direction, building on the work of Raymond Lull, Renaissance polymath Giordano Bruno took the method of locii further, emphasizing the use of places and images, with deepened affect and abstraction, as a memory protocol that would reorient one’s perception of the world. In another, perhaps more familiar direction, scholars like Petrus Ramus took the emergent encyclopedic impulse toward knowledge further, integrating protocols for memory into the field of logic. Influential thinkers like Frances Bacon and Rene Descartes followed this latter direction, and as a result, protocols for memory became subsumed by protocols for organization, in which remembered knowledge, when externalized and categorized, supposedly promised to take on a unifying and universal character. From this tradition, one sees its natural continuation in contemporary filing cabinets, databases, and taxonomies. However, a curious shift in protocols for memory may be subtly afoot today.
   * [a]Nice
[b]Makes me think about what happens with actions and gestures which are not inscribed, the are forgotten.
[c]Makes me wonder about the feasibility of increasingly complex technology as memory supports... A Macbook, for example, contains the inscriptions of millions of gestures. Do you have thoughts on that?
[d]I think this is what Sherlock Holmes used to do as a character. At least in the show:


“You’re most certainly going to die, so you need to focus,” Hooper says. “It’s all well and clever having a mind palace, but you only have three seconds of consciousness left to use it.” Holmes discovers the answers to staying alive are indeed in his brain.
[e]Makes me think about inscriptions on the blockchain, which has a sense of physicality to it. When Folklore "acts onchain" there seems to be more memory to it, than when it's just "saved" to a computer archive. Unsure if there's a difference tho. Might just be semantics.

Emoji Legend
📝 = section that needs further prose development.
🧩 = piece of the argument. current length is a result of ordering the argument, not necessarily how long the section needs to be.
🐍 = needs evidence, do not trust! prioritize getting evidence for these points above other tasks.
Brown text = tentative points that are unfinished, includes commentary.
________________


📝 Introduction
* The internet has yet to produce communities of scientific and intellectual inquiry that rival legacy knowledge institutions
* But because new information behaviors on new media layers do not interface well with older institutions, there is increasing conflict and pressure for reconciliation between these two knowledge environments
* In this paper we’ll look at some of the features that make existing knowledge institutions successful, and compare them to the new institutions emerging on the internet
* Through this inquiry, we identify what we call a “credit culture”: a set of protocols that define a community of knowledge over space and time
* What can we observe about the nascent “credit cultures” that are forming on the internet? How is power accruing to different knowledge communities in similar or different ways? What could make internet knowledge communities more successful?
Part 1
* The institutional repository of the world that internet migrants were born into: law, academia, science, medicine, business, trades.
* 🧩 The importance of institutions
   * institutions form the bridge between past, present, and future
   * institutions constrain and supply certain forms of human behavior [North]
   * communities of practice know how to get stuff done and train people to do it [Wenger & Lave]
   * Give some examples of both civilization-critical and life-enriching legacy institutions throughout
* 🧩 Defining scope: which institutions do we care about? Knowledge communities
   * knowledge communities generate questions and answers that embed wisdom and drive progress
   * These communities are responsible for shaping the world of thought and intellectual inquiry via philosophy, and share extensions into the world of practice
   * Discuss the relationship of knowledge communities to communities of practice. E.g. theoretical psychology and clinical practice
   * We aren’t interested in communities of pure practice which persist through the reproduction of tradition. In short we care about communities which persist over space and time through using written or recorded media.
   * Discuss formal properties of the types of knowledge communities we’re interested in
   * 🐍 Maybe something about how written / print media has shaped knowledge communities over time, and or shaped today’s institutions. This could start with Plato’s Phaedrus, or could be the link to some of the “bureaucracy” references [von Mises, Weber, Castells, Graeber]. This part sets up the introduction of “credit culture.”
* 🧩 The idea of “credit cultures”
   * We are proposing a name for the protocols that define a community of knowledge over space and time and allow for its continuity: a “credit culture.”
      * 📝 Unpacking and justifying use of the term “credit.”
      * Philologically, the “cred–” root is at the heart of many of the salient features of knowledge communities that persist: credentials, credibility, crediting, credence
   * Explanation of the components of credit cultures. We will dig into each in turn in a later section.
      * Certifying and legitimating knowledge
      * Assessing progress
      * Preserving and maintaining knowledge practices
      * Assigning reputation, codifying provenance and propriety
Pivot
* 🧩 Why focus on credit cultures?
   * successful institutions are definitionally those which obtain resources and achieve entrenched privileges. the most successful are able to rewrite themselves - legally and behaviorally - into society’s definition. Eg: In the way that it is hard for us to imagine a world without the banking and credit money system.
   * Just as credit cultures form a part of sustained institutions, institutions that entrench themselves entrench credit cultures as valid ways of producing knowledge. An entrenched institutional ecology = an entrenched epistemology
   * 🐍 shifts that involve new institutions coming into power often ((result from, produce, or coincide with?)) significant epistemological crisis
   * describing the current moment
      * new technological media layers are causing new information behaviors that do not correspond to the credit cultures of existing institutions. this itself is causing epistemological crisis and the rise of uncorrelated institutions
      * At the same time, we see a big shift in the material conditions for intellectual work. This point requires further development. Historically this has produced dynamic periods of intellectual creativity, at least in philosophical fields [collins]
      * However, this change is also challenging some of the core components that made prior credit cultures viable.
   * Therefore, we think it’s important to take a deep look at the composition of a credit culture. What makes specific knowledge communities work over decades? And what kind of institutional ecology does that lead to? What can we observe about knowledge communities that are forming on the internet?
   * So we will look at new types of institutions that are evolving in tandem with recent technologies, what can we observe about the “credit cultures” that are forming? How is power accruing to different “knowledge communities” in similar or different ways?
Part 2 - Comparing old & new
* Let’s look at some already-institutionalized (legacy) knowledge communities

* 🧩 LKIs as communities of practice [wenger & lave]. We start with the CofP framework because it introduces the contextual matter of knowledge institutions as essential to the social practice and the reproduction of the institution

* 🧩 What does this context consist of? An institutional description of university knowledge work with examples woven in. The entire right column of the research questions chart goes here. In order of most local to most abstract:

   * the local specificity of the collaborative “work site” [garfinkel]
   * shared types of questions, shared types of tools, shared language worlds [kuhn, Williams]
   * regular in-person meetings and scientific cabals [collins, price]
   * shared social practice and constant learning process toward identity of mastery [wenger & lave]
   * Discrete boundaries between individuals and organizations [ong, stanley]
   * funding mechanisms and shared objectives which strengthen these “lanes” [stanley]
   * 🧩 Epistemology and the construction of (knowledge) fields

      * 🐍 How does—or did—an epistemology arise from the legacy institutional ecology?
      * 🐍 Something about the premier status of “scientific” knowledge
      * rise of the royal society and then what replaced the royal society?
      * science and industry?
      * 📝 Return to the notion of “credit cultures” and show how the evidence supports our definition

      * 🧩 What institutions are “native” to the internet?

         * New, different types of media—social media, forums, algorithms, and networked publics [boyd, kelty]
         * New, different types of organizing logic—peer production, fandoms, and swarms. [benkler, jenkins etc, rafa]
         * Here we can introduce a few of the IKCs as examples that we’ll go through in more detail later
         * EA as an example of an IKC that effectively maintains knowledge and a shared canon to its benefit
         * Briefly cite a few fandom related examples
         * [reference] “Points of Contact Between Activism, Populism, and Fandom on Social Media” for #freebrittney
         * [reference] “Exploring The Dynamic Between Online Social Infrastructure and Online Community of Practice in Social Media Fandom” for general discussion of how fans develop communities across online social infrastructure
         * Internet-native institutions assume their shape by embracing the most obvious formal properties of the web: speedy distribution, replication, scalability, user-generated content
         * brainstorming
         * there wasn’t tooo much that was useful from the “how do intellectual communities progress” paper but I thought it was interesting they discussed discrepancy between theories of progress that prioritize novelty vs theories of progress that prioritize convergence [passage below]

            * “Next, one difference in emphasis between debates about scientific and philosophical progress is the importance accorded to novelty and convergence.
            * Philosophy of science shows a preoccupation with discovery. Conversely, regarding philosophy, the focus is on consensus on the correct views. A comprehensive theory of progress will do justice to the importance of both novelty and of convergence. And finally, progress seems to be a gradable notion—intellectual communities can make partial progress (and regress) relative to some intellectual end. More precisely, progress is gradable both with respect to the scope of novel contributions and to convergence. Some novel discoveries contribute more than others, and an intellectual community can converge on the truth to greater or lesser extents. Each of these thoughts should be accounted for in the framework we use to think about progress.”

            * Made me wonder about internet-specific determinations of what information is important (emphasis on spreadable and repetition over lineage progression and novelty) and if that leads to emphasis on convergence as part of what “progress” looks like

               * 📝 Brief comparison again to the “legacy” institutions.
               * these differences can be only partially accounted for by technological “structuring” or determinism [winner, turner, i forget who my favorite references on this are]

               * 🧩 Brief discussion of technological vs social determinism. what is our perspective on this?

               * 🧩 Information behavior of knowledge communities

                  * LKIs deal with knowledge in a qualitatively different way than internet knowledge communities (IKCs)
                  * LKIs: enshrined and certified knowledge, synthesized and published.
                  * IKCs: diffuse knowledge that people attach themselves to in brief affiliational ways, ongoing and in-process. Work and ideas are published and given a false sense of finality, creating a sense that things are “always-in-progress”
                  * “Languaging” into existence and creative misprision (Striphas, boyd, warner, Williams)
                  * Infrastructure: what kind of media support the work in distributed contexts?
                  * Collapsed knowledge contexts: the opposite of a functioning community of practice [boyd, meyrowitz]
                  * Identity: no milestones, no identities of mastery, issues of “blocked access” (wenger & lave)
                  * 🧩 What sort of institutions can be built? Other types of knowledge communities and why they only comprise some ingredients

                     * 📝 Swarms [rafa], Open source [kelty], Fandoms, Wikis, Algorithmic similarity clusters, “Big accounts”.
                     * Alt: what is the design space for a new credit culture?
                     * What are the ingredients of a credit culture within this new space (especially ones that differ)
                     * The role of lifestyle philosophies [collins], and what else is close to evolving?
                     * Open areas for new research to go on
                     * Some examples of organizations that are trying to build proper institutional KCs today

                        * DeSci, Progress Studies
                        * Despite successes in peer production institutions, IKCs have not succeeded in creating robust credit cultures, with one or two notable exceptions. We will explore why and how that might be corrected

                           * when we say robust do we mean their outputs can compete for legitimacy w those of legacy institutions
Case Studies
                           * Rationalism and the rise of effective altruism
                           * Counter case study: accelerationism
                           * I currently think we should do the case studies first so we have better examples to work through when we discuss IKCs properly. But not entirely convinced
                           * might be cool to talk to more web 1.0 communities about communities that didn’t survive
                           * there’s probably some chartmaking we can do depending how many we collect
📝 Conclusion
                           * Toward an internet-native credit culture, and random prognostication and commentary

                           * We want new credit cultures to emerge. It would be good if this were to happen. However, the new credit culture is undergoing a break with previous knowledge cultures, because print media publications and archiving are being superseded by different forms of media

                           * The current historical moment is an impasse because the epistemology of the older credit culture is being actively challenged, but the new infrastructure has yet to develop ways of certifying knowledge. In the absence of this, only the instrumental reasoning of tech companies seems ‘valid’

                           * There are attempts to take over older credit cultures and their attached institutions - but not yet formal knowledge validating systems of their own

                              * they can also attempt backwards-compatibility with legacy institutions to leverage legacy influence
                              * Building this may at first seem like endeavoring to build “useless” knowledge - in the way that “science” was originally seen as a gentlemanly activity performed by civilized men, not a profession to itself. It may seem useless to begin with. But it’s necessary for the other benefits it may later provide

                              * AI is not a solution* this is a footnote

                                 * LLMs have the power to consolidate … they basically produce things that feel “legitimate” through breadth of inputs but is really no better a mishmash than a human could produce
                                 * only statistically meaningful not semantically meaningful ***
                                 * chatgpt can’t use epistemic status tag that rationalists came up w
                                 * the way the gpt UIs are designed make it seem like the knowledge is static or reproducible
                                 * mcluhan argument we like - there are faster forms of media

                                 * epistemological crisis being the imperative for change

                                 * baconian science - bacon invented current scientific method. has to be based on in the world evidences vs other types of scientists wanting to base it on logical inference

                                    * this was a shift that had nothing to do with the shift in media technology
                                    * Evan… protocol labs efforts, they’re an interesting player because they’re implicitly interested in building a new credit culture on new media rails (desci culture)

                                       * eth foundation same level as protocol labs
                                       * desci question….. we’re talking about “science” but science is different from other types of intellectual inquiry. it operates pretty differently. social sciences bears more resemblance to philosophy than to science. We shouldn’t necessarily exclude science as a theme from this work, even though the requirements for validating scientific knowledge are more strenuous than other types of intellectual production
                                       * (also code)
                                       * the desci question could be an interesting case study. its the most direct attempt to do what we’re talkimbout
                                       * see researchhub drama
                                       * noodling on… ethereum foundation, the funders of this program - what they’re hoping to get out of it. we’re not really answering their questions of “how do we steward the protocol and understand what it’s for” we’re most suited to comment on the project and the ethereum foundation’s role

                                          * what do we want to communicate to them? what is their strategic position as funders of an intellectual culture through ppl getting rich on crypto
                                          * they’re foundation level stakeholders who we have more leverage with than we do with a traditional foundation.
                                          * how should they be thinking about fostering intellectual work that contributes to their overall goals


Dangerous Protocols
WIP. Still very much in draft mode!
Introduction
* Protocols are frequently touted as the “liberated” alternative to walled technology gardens. But by definition, protocols always ask us to relinquish[a] control[b]
* Sometimes this is good! Giving up control can enable us to spend precious brain cycles on other, more pressing decisions
* Protocols have the potential to liberate us, and the potential to tyrannically control us. They’re better understood as neither harmful nor liberating, but – as Alexander Galloway once put it – “dangerous”[c][d][e][f][1]
* We tend to focus on the benefits of protocols b/c they are often weaponized for political reasons (as an alternative to platforms)[g]
* But protocols control us just as much as platforms do, if not moreso, and are even harder to escape[h][i] b/c they lack central authority[j]
* We simply don’t notice how much protocols control us, because their control is subtle. Good protocols slip by undetected[k]


From strongly to weakly expressed protocols
* It’s particularly important to understand protocols as dangerous, as we are currently transitioning out of a long history of “strongly expressed” protocols, and into the beginning of what might be another long era of “weakly expressed” protocols
* Discussion of protocols is typically confined to the technical layer, but technical protocols are only one type of protocol
* Protocols are innately social in nature. In order to understand how protocols establish and maintain control, we need to zoom out from the technical layer and start with a broader definition of protocols
Protocols are primarily social, not technological, in nature
* Protocological thinking only appeared with the onset of industrialization, or what James Beniger called “The Control Revolution”: a need to regain control after a sudden explosion of information
   * Ex. the use of standardized forms, time zones


I define protocols as systems of control that dictate the procedural steps to simplify decision making between autonomous actors.


* Protocols are procedural, rather than descriptive or normative: “In order to do X, we follow Y steps”, rather than “Y is the preferred method.”
* Protocols simplify decision making: Protocols gently dissuade their participants from using reason or first principles thinking. The promised benefit is that decisions can be safely outsourced to the protocol
* Protocols establish a frame of control: Protocols can only exist within an ecosystem or network, which participants must first join, before they can be controlled by a protocol
* No one is responsible[l] for the protocol: Protocols may have an original “creator,” or claim to be managed by an organization or committee, but they are more powerful than any one entity. Protocols resist confinement. As their legitimacy grows, they want to become more powerful than their containers. Over time, protocols become difficult to change or influence. (For some reason, this is sometimes touted as a good thing by those who dislike centralized control, but it is precisely this quality that makes protocols dangerous.)
* Protocols are unopinionated[m][n][o][p][q]:[r][s] Because no one is responsible for the protocol, there is no clear agenda driving its persistence. Protocols persist because they have persisted; the more we use them, the stronger they get. They are a reflection of participants’ existing desires, rather than imposing unwanted, extrinsic desires.[t][u][v]


From a historical perspective, protocols clearly predate the internet era. The protocol definition does not specify a form factor; it only refers to an abstract pattern. Protocological behavior[w][x] has tightened its grip over time, manifesting across many different corporeal forms, or material layers:


  
[y][z]


Here are several examples of trends or movements where protocological thinking emerged as a way to exert social control. While these examples live on different material layers, the way in which they exert control is consistent.


* Personality frameworks:[aa] Derived from early psychoanalysis (Freud/Jung) -> 1910s to screen for “maladaptive” personality traits in soldiers -> 1940s-1950s rise of MBTI, 16PF, MMPI, and later, DISC, Big Five. Goal is to understand people’s psychology in order to “sort” them into buckets, whether for hiring, military, etc
* Company towns: (late 1800s through the early 1900s) Employers built entire communities for industrial workers to live. These turnkey towns were nicer than living in cities, but they were also a way of “civilizing” workers through their built environment, which made them more reliable employees. Ex. alcohol and gambling were prohibited, church attendance was encouraged, some towns only used “scrip” (like in-game currency) instead of money.
* Disneyland: Originally conceived of as a more “civilized” alternative to the bawdy carnival style of amusement parks at the time. Designed as a place where differences were minimized or discouraged; anything besides happiness is not an emotion that has a place at Disneyland[ab][ac]
[ad]
* Internet protocols derive legitimacy not from their technology, but from the compliance of their participants. They represent the summary of prior consensus, which others can now follow without having to replicate those same debates.


Why do we tend to describe protocols as pure technology and ignore their social primacy, or pretend that the social is merely a subset of the technological?
* Some blockchain governance enthusiasts, for example, like[ae] to point out the ways in which the technical aspects of protocols can simplify social coordination problems
* Technology makes us[af] feel safe and secure, as if the protocol is merely a tool that we (the fully agentic human) is wielding, but I think it is the other way around. We are never in charge of protocols


Rather than influencing human behavior through explicit rules and restrictions, protocols exert social control by firstly creating an ecosystem, or frame of control; then, asking its participants to enter into the social contract; and finally, introducing protocols[ag] that effectively control those participants’ behavior. Compliance becomes a joy that participants willingly opt into.[ah]
Protocols are evolving once again
* Since the birth of the social web (“Web 2.0”), protocols have moved on to subsume the cultural layer, raising a new set of ethical questions as they begin to control our values and beliefs
* As Beniger identified, the onset of industrialization created a massive influx of information that needed to be managed, which gave way to protocols
* But the social web created yet another, even bigger, tidal wave of information
* Industrial Revolution -> crisis of control -> anomie -> re-exertion of social control via strongly expressed protocols[ai][aj]
* Social web[ak] -> crisis of control -> context collapse  -> re-exertion of social control via weakly expressed protocols
* With the advent of the Social Age came a deluge of opinions. We now have easy, unlimited, infinite access to off-the-shelf takes on everything from reusable shopping bags to geopolitics
* The prevalence of such information, for most people, cheapens the value of original thinking. Instead of having to reason about ideas on our own, we can simply borrow others’ ideas and replicate them[al]
* This is perfectly rational behavior: the cost of developing an original idea is now prohibitively expensive, versus having free and reasonably vetted ideas at one’s fingertips. Just as most people don’t own and raise their own cows for meat and dairy, most people cannot be expected to do the hard work of developing and cultivating opinions of their own[am][an][ao]
* Whereas industrialization caused us to protocolize our social institutions (the “social” material layer), the social web caused us to protocolize our values, thoughts, and opinions (the “cultural” material layer)


Strongly vs. weakly expressed protocols
* Strongly expressed protocols: All participants know what the protocol is and willingly enter into its governance. For example, when a high school student takes a college entrance exam, they understand that this is the protocol to attend an institution of higher education, regardless of their personal feelings about the value of these exams[ap]
   * With strongly expressed protocols, such as the institutions of Foucault’s “discipline” society – school, family, work, factories – individuals participate in the collective as individuals. The benefit of the traditional family unit, for example, was that it provided a stable structure that enabled men to flourish as individuals and have successful careers. School is a protocol we follow in order to attain the education needed to get a job. The benefits of protocol participation ultimately accrue to the individual
* Weakly expressed protocols: Many participants are not aware that they are operating by protocol, but allow themselves to be controlled by it nonetheless. For example, when someone piles onto a controversial issue on social media, they think they are acting as well-reasoned individuals – in fact, sharing one’s opinions or beliefs on hot topics of the day is often considered to be a form of self-expression. But they are not actually expressing themselves as individuals; they are acting according to protocol[aq][ar]
* Weakly expressed protocols are a product of memetic culture that crystallized in the 2010s -> rapid replication protocolized original thought
   * The willingness to opt in is a hallmark of protocols, which never coerce its participants, but instead make it a delight to comply. We were not forced to give up our agency; we chose[as] to give it up because it was easier and more convenient
* Protocols are not unlike any other control-seeking organism[at]. “Power tends to corrupt and absolute power corrupts absolutely;” – this is true regardless of whether the entity in question is a sentient human capable of reason, a headless organization, or a protocol. Protocols moved into the cultural layer because they are not satisfied with controlling an aggregation of individuals. Protocols want to consume us on the most cellular level; they desire life itself[au][av][aw][ax]
* Strongly expressed protocols also became easier to identify, and can be manipulated or taken down for personal gain. Just as antibacterial products can lead to the evolution of super-resistant bacteria, so are protocols learning to avoid our collective immune system, emerging in a more powerful, more resistant form that is harder to discern. If we don’t even know the protocols are there – as is usually the case with weakly expressed protocols – we won’t try to defect
The growing influence of weakly expressed protocols[ay][az]
Examples:
* Cancel culture[ba] and mob mentality crop up on every new social platform or protocol, no matter how new or different it promises to be. We keep producing the same results, regardless of the form factor. The social media platform itself is a strongly expressed protocol; how we punish and sanction others on those platforms is a weakly expressed protocol
* Doomer industries as another byproduct of the social web: climate change, AI safety, misinformation, and population decline being a few examples of ecosystems that formed around the threat of an apocalyptic event
   * Accepting the inevitability of the event establishes a frame of control, enabling protocols to arise within each ecosystem (recycle; reduce energy; break up Big Tech; make more babies)[bb]
   * These protocols are considered “weakly expressed” because they are, again, mistaken as a form of self-expression or personal agency (i.e., working on “one of the most important issues of our day,” doing meaningful work, etc)
   * These protocols maintain totalizing control over participants via the interminable, unfalsifiable fear of “bad things will happen to the world if you don’t do this”
* General purpose AI can be thought of as the productization of “solved conversations,” [bc][bd]promising us less work in exchange for giving up our reason. While we can understand general purpose AI as a set of distinct tools, nobody knows exactly what goes into it. It’s the aggregation of millions of ideas, packaged in the form of a recommendation. It’s self-learning, beyond the control of direct human influence. “Nobody is responsible for the protocol.”
Subverting protocols
* Subverting protocols is difficult, and even moreso when the protocol is weakly expressed. Because nobody is in charge, there is no singular authority to take down[be][bf][bg][bh]
* Explicit resistance tends to be unsuccessful; protocols are more powerful than you
* The first step to subverting a protocol is to recognize that one is being governed by it in the first place
* We may decide that we like being governed by certain protocols; again, protocols are not good or bad, but merely dangerous. But if we want to subvert a protocol it’s difficult to opt out of a protocol without exiting the network it belongs to. Consider this the “hard fork” option, only when no other solutions exist[bi][bj][bk]
* Successful example of subverting protocols by exiting the network is climate action
   * For the last 20+ years, the climate crisis response has been based on an assumption of energy scarcity; that is, reducing our energy usage, or making it more efficient, will bring us to “sustainable” levels. In recent years, some people have asked, “What if we don’t assume energy scarcity, but instead seek energy abundance?” Reframing the climate narrative this way created a new network that enabled a new set of protocols to develop, such as advocating for the revival of the nuclear movement[bl][bm][bn]
   * However, I think these types of subversions are rare, in part because exiting the network is very expensive: both the social cost of operating under a different narrative from one’s peers, as well as the cost of building a new one
   * Instead, the art of protocol subversion seems to take a tai chi approach. Subversion requires working with the protocol, not against it
* Vibe culture[bo] as a response to memetic cancel culture is an example of protocol tai chi[bp][bq]
   * Exiting the protocol would mean leaving social media entirely, which some have done, but even more have tried and failed to do. For many people, participating in some social platform is a requisite part of life[br]
   * Instead, the initial calls to “delete Facebook [etc]” and “I quit social media and my life is so much better”, which were popular nearly a decade ago, gave way to quieter forms of protocol subversion[bs]
   * With protocol tai chi, we participate in the social media ecosystem, but we make ourselves undetectable[bt][bu]. The spiciest conversations have been pushed down a few layers, relegated to DMs and group chats. We saw the birth of the “cozy web.” And, more recently, we’ve seen memetic culture give way to “vibe culture,” as Peter Limberg eloquently expressed in his “Meme to Vibe: A Philosophical Report” (emphasis mine):[bv]
      * “Memes are cultural information the mind perceives, coupled with an urge to replicate them. Vibes are cultural “exformation” the body receives, coupled with a choice to experience them.”
   * What is vibe culture? It’s hard to describe. That’s the point[bw][bx]. We know it’s there, and that it governs our behavior, but it escapes the stubborn heavy-handedness of protocological control. It still takes place on social platforms, but it’s difficult to identify or track. In contrast to meme culture’s doomscrolling and pornification (or “the phenomena of everything being commodified and shamelessly put on display”), vibe culture is about “experiencing, not replicating,” a way of “protecting one's mind while online.[by]”


The old battle of strongly expressed protocols – which involve very literal interpretations of protocols – is between “social media platforms vs. [technical] social protocols.” The new battle of weakly expressed protocols – whose influence is so subtle that they frequently go undetected – is between “cancel culture vs. vibe culture.”


How do we know which protocols are good for us?
* Protocols are good when they help us streamline behavior, reduce complexity and unnecessary decision making, reduce transaction and coordination costs, codify “common wisdom” (ex. healthcare protocols)[bz], or bring everyone into the same context quickly and easily (ex. emergency or diplomatic protocols). At their best, they liberate us from decisions we don’t need to make, or that we lack the authority to make confidently, capturing the best of collective human wisdom.
* Protocols become “dangerous” when they are taken too literally. In the industrialized era, they became prescriptive and rigid, and sometimes more harmful than helpful – for example, the use of personality tests in hiring, or the use of standardized testing to evaluate students’ aptitude. In the social era[ca], protocols are dangerous when they become difficult to detect, quietly rewriting our internal values and beliefs, causing us to surrender our agency to control mechanisms that we don’t even realize are there.
* Protocols are outright bad when they remove individual agency from places that require creativity (ex. knowledge work, reason and debate). While this might occasionally be a desirable tradeoff when it’s made consciously, the rapid takeover of protocols at the cultural layer threatens to slow human progress to a crawl.[cb]
* There is no stopping the eventual protocolizing of every aspect of our lives[cc]. Protocols are difficult, if not impossible, to break up or escape from once they take over. Subverting protocols, then, starts by recognizing which protocols govern our lives, and which we want to govern our lives. When should we relinquish control to protocols, and when do we need to subvert them?


The difference between protocols that help or harm is easy to describe in the abstract. It’s actually applying it to real-life situations that makes it clear how hard it is to evaluate the dangerousness[cd][ce] of protocols against some heuristic or list of criteria. Furthermore, which protocols are dangerous depends on every person’s own calculus as to how much agency they’re willing to give up. We cannot stop the spread of protocols into the most personal layers of our lives, but living peaceably with protocols, especially in this new era, starts by noticing how much they control our lives in the first place.[cf][cg]
________________
[1] "...it’s not that protocol is bad but that protocol is dangerous.” https://www.amazon.com/Protocol-Control-Exists-Decentralization-Leonardo/dp/0262572338 
[a]thought: Is it closer to consensual delegation? Or participatory surrender?
[b]1 total reaction
dorian taylor reacted with ➕ at 2023-07-22 17:38 PM
[c]What’s the full quote/context/idea? I have no idea who he is so appeal to his authority doesn’t help me
[d]_Marked as resolved_
[e]_Re-opened_
[f]oh damn I had a footnote here that got lost, will re-add. (I'd flesh it out more in a final draft, this is still bullet point mode)
[g]I like this point, that protocols are currently being framed as a cure to a well-known problem, and are therefore benevolent by default.
[h]Accountability argument is also useful here... protocols are not accountable themselves. They cannot "hold obligations",,, only "be a codified ruleset." They are not answerable and the reward/punishment cannot be enforceable.


A Joanna writes in this magazine article (https://zine.zora.co/joanna7459-accountability)


“A is accountable to B when A is obliged to inform B about A’s (past or future) actions and decisions, to justify them, and to suffer punishment in the case of eventual misconduct.”


Things, as they usually do, become murkier on closer investigation. In this definition, and in Schedler’s frequently cited work on the topic, accountability can be broken down into two dimensions: answerability and enforcement. Answerability is the requirement to both inform and explain your actions to another party. Enforcement is what it sounds like: rewarding good behavior and punishing bad behavior with real, material consequences. Answerability without enforcement is not accountability. As Schedler writes: “If a police officer kills someone in custody without due cause and still walks free, it does not satisfy the principle of accountability if a journalist documents this abuse of authority.”
[i]Rights to exit document that might be useful: https://cuusgfnwhwlflkfirssf.supabase.co/storage/v1/object/public/mendable_user_content/Folklore/708907634_Rights%20of%20Exit.pdf
[j]Not quite following the logic of this assertion but presumably you explain it downstream somewhere
[k]This ties back to Venkat's ribbonfarm article on "deep protocolization" --- protocols "make water" we swim in haha
[l]See earlier note on accountability definitions, might be useful
[m]I'm not sure I agree here. Protocols tend to embody the values of their creators -- in some ways, they are extensions of their creator's intentions. But like a wish to a genie, often have deviating secondary order consequences that the creators didn't realize would happen at scale.


Might just be semantics tho. Would be curious how this comment jives with Angela's unconscious protocols thread of thought. Do protocols really reflect children's desires when they are adopted through training? Or, could it be that we embody the desires of protocols we learn, until we grow up and test the freedoms we have in breaking protocol?
[n]Yeah, I think "unopinionated" is the wrong word here. I'll try to find a better one. I don't mean that they don't reflect a creator's values or interests, but more that protocols don't care what you think about them. Participants can complain about protocols all they want, but if they continue to take part in them (vs. exiting), it's still clearly giving you something you want. Your decision to participate in a protocol says more about you and what you want, vs. the protocol actively trying to tell you what to do. (There is something sort of hauntological about this that I find intriguing: trying to argue with a protocol is like screaming into the void.)
[o]"Selfish" could be a better word here (per Timber's comments further down about "selfish genes"), though it doesn't fully convey the cruel, horror-ish aspect I'm now mentally stuck on haha (love me some Lovecraft)
[p]1 total reaction
Nadia Asparouhova reacted with 🌟 at 2023-07-11 12:15 PM
[q]Maybe another word would be better here? Maybe self-reinforcing? potentiating? 


I feel like all protocols are created with some stance or opinion in mind, whether we realize it or not, it's going to shape the world according someone/set of peoples worldview, but like you explained in the previous point, it's hard to attach accountability to them.
[r]I would argue protocols are strongly opinionated: HTTP isn't stateful, Bitcoin vs. Ethereum clearly have strong values/opinions expressed which lead them in different direction, health protocols, esp. the whole COVID saga, are also strongly opinionated.


Also disagree re: "no one is responsible" - I think it feels less like it because it's more diffuse, but again you have organizations which set web standards, Bitcoin Core Devs, the WHO, etc.
[s]inclined to distinguish between the design of a protocol (opinionated) vs the invocation or enactment of the protocol ("opinion" not really meaningful save for the choice to use the protocol)
[t]Do you think it's possible that a protocol could be reflective of past desires that have since changed? (ie the zombie protocol remains in place for some reason but the purpose it originally served is no longer wanted - this is a phenomenon I'm looking at in the built environment)
[u]What about people who consciously and repeatedly "act out" a protocol? Say they're just trying to avoid being reprimanded
[v]I think I'd ask: why do they want to avoid being reprimanded? Who are they trying to please? Participating in the protocol, even if they don't like it, because they want to avoid being reprimanded suggests that compliance gives them something they want (the approval of someone else). Maybe that logic is a little too circular, though. Have to think about it
[w]I like this wording
[x]+1: it helps close the gap between what a protocol vs. norm/standard/ritual/etc. is!
[y]is there a logic to the order of the layers?
[z]haha there was when I started, but after thinking about it this past week, I think I need to rewrite it
[aa]This caught my attention, can't wait to learn more in a later draft 🙌
[ab]I liked Sarah Perry's take that Disneyland is actually a much more narratively rich correlate to an amusement park and instead provided a shared myth for what America's frontier is (https://www.ribbonfarm.com/2015/08/06/frontierland/).
[ac]amusement parks, not disneyland, seem to better fit the conclusion you're drawing
[ad]Not sure if this fits in well, but I believe the Menlo Park FB Campus was designed by the same architect as disneyland (citation needed!) 


If it holds, nice link between Company Towns "civilizing" and "anything beside happiness is not an emotion that has a place"
[ae]This may be overly charitable: there's an incentive to point things out as being "tech only", because it limits accountability or the degree to which you have to respond to outside pressures. I'm sure Angela has strong opinions about the best way to frame this!
[af]It might be worth unpacking the "us" here: the designers of the protocols get to "never be in charge", and the users get to assume it just works as is.
[ag]This is a bit confusing to me - perhaps "norms" may be better here?
[ah]Banger
[ai]do you clearly define what strongly expressed vs weakly expressed protocols are earlier? it seems central to your piece but I am not fully getting it yet
[aj]oh oops — you do so later, I see. might be helpful to have it defined sooner
[ak]I think there's probably something to be said here about recommender systems that automate curation, which is a direct link to the bullet point about not having to develop/cultivate ideas. To put it another way, I think it's not just about the deluge (there were more blog posts than I could read on the web in the mid 2000s), it's also about not really having to use your brain to sift through the deluge (advent of social media)
[al]god... so true & well articulated. I've been thinking that "critical thinking is the most scarce resource" for a while
[am]This is a compelling analogy. From physical decoupling... to concept or abstract decoupling.
[an]what exactly makes it expensive? I sort of get it but I think making it explicit will help
[ao]I guess it's _comparatively_ expensive: you can either develop an original PoV on 1 thing or get N other PoVs for the same time effort.
[ap]I think there's a weakly expressed meta protocol dictating that these are the ways to even get into an institute, or that you have to go to the institute in order to accomplish what you're looking to do
[aq]fire — and what exactly is the protocol?
[ar]+1 for these examples feels like it would be helpful to explicitly map out the protocol, also makes a cool exercise to the reader in identifying these protocols that are getting harder and harder to detect
[as]you could distinguish here, choice is probably more true for some protocols than others and some participants have more choice than others. i think you describe in the point above a process of internalization, being unable to tell what is "authentically us" and socially imposed. that point further complicates "free choice"
[at]Protocols are viral, but ones conditioned on the consent of the carrier are more likely to be symbiotic, whereas those that creep in subversively are more likely to be parasitic.
[au]1 total reaction
Nadia Asparouhova reacted with ♥️ at 2023-07-10 13:23 PM
[av]does this mean protocols are also memes?
[aw]1 total reaction
Nadia Asparouhova reacted with 👍 at 2023-07-10 13:23 PM
[ax]you can say protocols work by eliminating degrees of freedom which creates a structure, but i don't know the extent to which that is inherently self-perpetuating
[ay]This seems like the buried lede… the theme is protocols and control but this is the lede?
[az]++
[ba]I once did a thread on how the opposite of cancel culture is “know your place” culture… wonder what determines which one rules https://twitter.com/vgr/status/1360037779766267906?s=61&t=Z7ogL5oXQlE-zjAQ_mFVBw
[bb]This is a really interesting point I'd never considered in this way before.
[bc]this is an awesome take
[bd]agreed, but stumbling over terminology. i know "commodification" would not really work, but then  is a chatbot essentially a product? tricky :) would "closed debate" work?
[be]The exit vs voice framing feels relevant here - usually easier to exit/avoid a protocol than change it.
[bf](I see that you get to this below)
[bg]This is really interesting. "Dangerous (Weak) Protocols)"
[bh]would still caveat for "scoped protocols" that definitely have an owner
[bi]I like the application of this hard technical concept in a social context
[bj]+1
[bk]+
[bl]did these advocates exit a network though? not sure i would see it as a paradigm shift (yet). a lot of climate discourse though is an attempt at subversion of the protocols of capitalist modernity.
[bm]I think so! One thing I noticed about this group is they explicitly use the term "energy," not "climate," to describe their interests. They don't want to be part of the climate tribe (i.e. exited the network), they made a new tribe
[bn]That's a neat language/social distinction that may be worth adding to the essay!
[bo]Reminds me of "Memes" as subverting the back-link SEO optimization protocol. Distribution through vibe instead of technical connections.
[bp]1 total reaction
Nadia Asparouhova reacted with ♥️ at 2023-07-10 13:27 PM
[bq]procedural analogue to standards hacking, lol
[br]going back to the arguemnt of no "real choice" above
[bs]it is interesting how more people are realizing that doing this is actually a liability
[bt]2 total reactions
Nadia Asparouhova reacted with 🔥 at 2023-07-10 13:29 PM
Shreeda Segan reacted with 🔥 at 2023-07-14 13:59 PM
[bu]great list! hadn't quite made this connection, but it's very helpful. thank you
[bv]I'm referencing this same report in my work haha
[bw]I know it's hard to describe (and illegibility helps with cozyweb stuff, obviously)... but a bit more description would indeed help. I don't know what "exformation" means. A twitter interaction that encodes vibe culture would be enlightening to look at as an example.
[bx]1 total reaction
dorian taylor reacted with 👀 at 2023-07-24 08:39 AM
[by]This seems quite similar (or at a minimum adjacent) to "extitutional theory", which tries to describe a core of human experience/sentiment/emotion around which people coordinate without the rule-based protocolization that suffuses much human social activity. I did a workshop at Harvard at this and know the two folks most central to the development of the theory (Primavera de Filippi and Jessy Kate Schingler).
[bz]very charged post-covid example 😅
[ca]what's that? :)
[cb]adolf hitler's art theory - boris groys http://www.c-cyte.com/OccuLibrary/Texts-Online/Art_Power_Boris_Groys.pdf
[cc]This feels a little more nuanced -- as you point out, protocols are evolving once again, so they don't ossify irreversibly (although you do say it's difficult if not impossible to break them up). And it feels like there are aspects to my life that are protocolization-resistant, although I'd have to think more about why I hold this belief... I think humans crave novelty and excitement, and we like distinguishing our tastes from that of others, and would find it difficult to protocolize literally everything. (Maybe you mean something more mix-and-match though, different people subscribe to overlapping protocols for different aspects of their lives?)


My working thoughts here, hopefully interesting/helpful: even if it's difficult to actively push against protocols as individuals/as a small % of the participants, on the collective level protocols do react to external pressures -- technological, social, environmental etc changes. They generally have shelf lives, perhaps some combination of individual action / collective action / external factors.
[cd]1 total reaction
Nadia Asparouhova reacted with ♥️ at 2023-07-10 13:33 PM
[ce]1 total reaction
Shreeda Segan reacted with ➕ at 2023-07-14 14:00 PM
[cf]You really open up your horizon for most of the essay talking about all kinds of apsects of social life with its protocols having existed and developed through history. So it might be helpful to point out when you talk more about recent, technology-driven aspects specifically. You can still make the argument about dangerousness in both. Pointing again to the term "social fact" that Emile Durkheim coined in 189something to describe what "enables and restrains" action in societies (booktitle: the rules of sociological method). It fits your argument perfectly :) Really appreciate this project!
[cg]Really like this. Succinctly illustrates just how difficult and subjective the problem is. Feels adjacent to the issue of public safety, which I'd like to hear more from you on... but it's also a big rabbit hole

In defense of death[a] 


working title/draft. underscores and brackets indicate areas that need to be looked up and filled in


Part 1: The Nature of Death
(Some kind of intro including a summary of common views of death[b])
But humans are not the only things that die. Indeed, it seems even non-living things can die. In a reddit thread, When is an MMO considered "dead" to you?[c][d] reddit user u/UnoriginalAnomalies asks, [e]"I'm curious at what point people start actually considering a game "dead". What's your threshold?" (cite). Various users list criteria: when they downsize their staff, when they go free to play, when there are no more updates from the developer, when there is no one at the spawning area. There are related threads, What constitutes a "dead MMO"?, and The Signs: How do you know if an MMO is dying? (cite, cite). Across all three threads, there are ___ replies, many of which offer other criteria, different combinations[f], or counterexamples.
(quick quantitative analysis of the consensus from reddit)
What other things die? Malls, certainly. Myspace is dead. So is LiveJournal and GeoCities. Web2 platforms and social media sites can die. We might also call a party dead - or a music scene[g][h]. An open source code base can die. So can an ecosystem. If we can call these things dead, does it follow that they were alive? Are things made up of the actions of living things, themselves alive?[i] It's not obvious. “What distinguishes an ecosystem from an organism?” and “Are ecosystems ‘alive’ or do they just contain life?” were two of the questions listed at the 2008 International Society for the Systems Sciences Annual Meeting, a research community dedicated to transdisciplinary inquiry into the nature of complex systems – the subject is worthy of serious thought. (cite) There are many famous and controversial examples of things even more questionably on the boundary between living and non-living than ecosystems: At what point is a fetus alive? A virus? etc. But these dead things - MMOs, malls, social media sites – are not biological quandaries. When we say they are dead, we are using the word dead as a metaphor.
Can a protocol die[j][k]? If a protocol lives at all, it lives the way a mall lives – through the activity of participants – and dies by metaphor. What is a general term we can use for these metaphorically-dead things? I propose worlds, [l][m]following the popular concept from Ian Cheng, artist and author of Emissary's Guide to Worlding:
We could say a World is something like a gated garden. A World has borders. A World has laws. A World has values. A World has dysfunction. A World can grow up. A World has members who live in it. A World gives its members permission to act differently than outside of it. A World incentivizes its members to keep it alive, often with the pleasures of its dysfunction. A World counts certain actions inside it as relevant and meaningful. A World undergoes reformations and disruptions. A World has mythic figures. A World is a container for all the possible stories of itself.
A world is what grows on top of a protocol when the protocol lives. (a world has boundaries … cell membrane … coherent behaviour … lifelike, different from life)
What do we mean when [n]refer to something that arguably never lived as dead? [o]Most of these dead worlds in fact still exist. Dead MMOs that are not shut down by the developers in a clear apocalypse event can linger on indefinitely. The mall we call dead has not actually been demolished, and it may still have one or two retailers holding on. I can still log into my livejournal. But nonetheless, they have meaningfully changed state. When we use death as a metaphor, as is often the case with other metaphors, we reveal a truth that is difficult to articulate otherwise: Something is legitimately gone. And much like death in more familiar contexts, it cannot be recovered.[p]
Diagramming the lifecycle of an MMO, we find something like this:

(games[q] which are still active) =>
(games which are referred to as “dead” but have not been shut down) =>
(games which were officially shut down) =>
(games[r] which were officially shut down but there are unofficial ways to play)

Many MMOs which have lost their rich player community, and match the reddit descriptions (______) are more accurately described as in a liminal state between alive and dead - something like protracted dying. Its nature is debatable: one can argue about relative "deadness", or debate different criteria. A game might linger in this liminal state for years, or perhaps be shut down quickly. A similar transition unfolds for many other digital deaths. At some point in the lifecycle, the liminal state of dying ends [s]– in the case of an MMO, this usually looks like the developer going out of business or making the decision to no longer maintain the servers.[t] In general, the end of the dying process takes the shape of a decision, usually undertaken by an authority or marked by ritual.
Following this observation, we will discuss two alternate understandings of death and how they apply in a protocol context. First[u], death-as-a-process: death is not a simple binary transition, instead it is a process (dying) that may result in a protracted liminal state between alive and dead. The nature of this liminal state is debatable – it is sometimes understood as transitional, and characterized by possible disagreement or ontological uncertainty about relative liveness or deadness. The death-process ends with the second key hallmark of death as it will be discussed in this paper: death-as-a-decision. Death is not a given, nor is it a clear natural fact – death is instead a decision, similar to a diagnosis, with domain-specific criteria determined by technology, authority, and culture. We can further separate death-as-a-decision into cases where there is a clear decision-maker, such as an MMO, and cases where there is not, such as a p2p protocol.
Neither death-as-a-process nor death-as-a-decision are unique affordances of digital death. Even in the non-digital context, or cases where death appears to be inarguable or clearly delineated, such as the death of humans, we find many parallels. Not only in the medical profession, but also spanning spiritual understandings, funerary rituals, and the legal system, we find decision-making protocols surrounding death that in many cases allow or even require a lengthy liminal space or process between the world of the living and whatever follows.
In Twice Dead, ____ recounts the changing criteria of human death over the 19th and 20th centuries. In the late _____s, before the invention of ___ and ____ diagnosing death was considered medically difficult. Until ___, some doctors insisted the only inarguable marker of death was putrefaction. This was reflected in public anxiety about being buried alive, such as ____ and _____. (finish example)
Importantly, the nature of these processes as they manifest in the west – including clear consensus around brain death as legitimate death[v], leading to the phenomenon of normalized organ donation from brain dead bodies – are not global universals. In Japan, a country where access to medical technology is certainly as widespread and advanced as the United States, situating the brain ______. Though the brain dead body is an artefact of technology, and it can exist only because of technology, the location of death in the body (or subsequently the location of life) is culturally determined.[w] With current capability,[x][y] a body can continue in brain dead stasis for years. Arguably, and certainly in places where brain death is not considered equivalent to the death of the person, this can be thought of as a protracted death process, or perhaps a liminal state.
Further, in the legal and statutory context, there are many cases where death is understood to exist separately from the death of the body, leading to further and even more protracted liminal states. (Legal death, presumed dead).
In Celebrations of Death, Anthropology of the Mortuary Ritual, anthropologists Peter Metcalf and Richard Huntington use the framework of "rites of passage" to examine death rituals. The term rite of passage was coined by ethnographer Arnold van Gennep, in his 1909 book of the same name, where it is applied to pregnancy, childbirth, initiation, betrothal, marriage, and funerals. He characterizes a rite of passage as being a transition from one state to another, and as having three phases: separation (from the previous state), liminality (being between states) and incorporation (into the new state). Metcalf and Huntington follow this with an extended analysis of cultures where there is secondary processing of the corpse, namely, an initial ritual, followed by a lengthy intermediary period consisting of perhaps years, whereafter the corpse is exhumed and a further ritual concludes the funeral process. Secondary processing is also common in _____. (Toraja, finish example) Though these examples may seem isolated, they also represent edge cases that may have more in common with rites familiar to the western reader, for example, christian purgatory, the _____. or ______. 
This is not to mention the most clear case where death is a decision: suicide and relatedly euthanasia[z]. (________) But the most salient point on the matter of death-as-a-decision is that death is always already a decision.
Another example where death is poignantly a personal decision, but simultaneously used as a metaphor, is the dead name[aa]. (________) A world may not be a living thing, but a living thing can be a world. Or to borrow language again from Ian Cheng, "These days it’s hard to see a person as a person in the classical sense. Better to think of a person as a world grown on the substrate of a biological body. A World of You."
Once we name death-as-a-decision, like a magic word the curtain draws back and the decision-maker appears. Who has been given authority to decide that something is dead, and what rituals or protocols are undertaken to cement its deadness? In the human context, it is doctors, lawyers, and sometimes religious authorities. In the context of worlds, it is ______.
There are of course also dead blockchains and dead smart contract protocols. The particular nature of p2p protocols means the end state that gives the most clarity - all servers shut down by the developers, unpingable, no nodes alive - is very rarely reached. There is no clear decision maker who could turn off the server, and indeed no single server. This decentralization is the source of some desirable properties, such as censorship resistance and ability to persist even in the face of takedown attempts by well-resourced actors. (Vitalik warlock story). Nonetheless, we could define a series of death criteria and make a diagnosis. In a report on the subject, Coinmarketcap suggests projects with negligible trading volume, joke projects, and projects with little to no funding are dead (cite). Coinopsy.com maintains a list of dead coins, even offering a 0.1 ETH reward for new death reports (cite). It has four criteria[ab][ac] for being instantly added to the list: Ranked below 1000* for over 3 months (unspecified where, possibly on its other ranking pages), Volume under $1000 USD for 3 months, Website dead and no trace of updates, and No nodes or other similar problems. As of June 2023, there are 2439 coins on its list, going back as far as 2011 (cite). 99Bitcoins' page for dead coins, formerly deadcoins.com, maintains a list of more than 1700 blockchains that have died and for each one gives a series of "death indicators" including: Inactive Twitter, Low Volume, Not indexed, Not Listed on exchanges, and Website Down (cite). I would propose additionally: transaction volume, difficulty of attacking the consensus system, and (______?????)
In cases where there is a clear decision-making authority on the matter of death, such as a company that administers an MMO, we can find that a world has a legitimate funeral moment. In 2008, Electronic Arts decided to shut down the servers of The Sims Online, a multiplayer version of the classic single player simulator—essentially sending the entire world and all of its community members to a scheduled apocalypse. Released in 2002, the game had never seen the success EA had hoped for, and after a rebrand in 2007 as EA-Land the shutdown was announced for 2008. (cite) In the final minutes of the game, players gathered in the town hall to attend a See You Soon Party, where one of the community members gave an emotional final speech:
I just wanna thank you all, it’s been an amazing experience, it really has, and I told myself I wouldn’t make myself cry, but I can’t stress enough how much you guys have meant to me over the past, oh however many years it’s been. It really has been awesome and uh, some people don’t get attached to things but when you make friends the way people have in this game, it’s actually really hard. (cite)
Another example is Asheron’s Call, which ran from 1999 to 2017. The game had a strong and dedicated community of players, but it had shrunk over the 17 years and was no longer profitable. (cite) In a video documenting the final moments, as player avatars disappear in waves, each death marked by a purple spiral, one streamer says through tears:
Don’t let Dereth go to waste. And all these people and all their items. And everything that we’ve worked for, the friendships, and the relationships, and the marriages and the kids and the friends and everything that people have had. Don’t let this go to waste. Thank you everyone. (cite)
Moreover, in these moments of grief we can see clearly[ad]—perhaps more than at any other time—what characterizes the life of these worlds. While we casually say “irl” when we mean “in real life” and not on the computer, these digital worlds have always been real in the Philip K. Dick sense: “Reality is that which, when you stop believing in it, doesn't go away.” (cite) Like Margery Williams’ classic children’s story, The Velveteen Rabbit, about a toy that becomes real, they have been loved into a kind of transubstantiation:
‘Real isn't how you are made,' said the Skin Horse. 'It's a thing that happens to you. When a child loves you for a long, long time, not just to play with, but REALLY loves you, then you become Real.’
'Does it hurt?' asked the Rabbit.
'Sometimes,' said the Skin Horse, for he was always truthful. (cite)
The emotional investment and the participation of living beings that inhabit a world is not obviously quantified or measured, but it too has reality – and this is not unique to games. The first marriage vows were enshrined on a blockchain in 2014 by David and Joyce Mondrous. (cite) One of the contenders for “first NFT” is OLGA, minted on the counterparty blockchain by JP Janssen, also in 2014. The image is a drawing of Janssen and his now wife and is titled “Моя Вечная One & Only” which translates to “My Eternal One & Only.” (cite) In 2015, Terra Nullius, an early project on the Ethereum network that allowed people to record a short message, hosted a memorial message to Nóirín Plunkett, an open source contributor and VP of the Apache Software Foundation. (cite, cite) More recently, a smart contract was deployed to serve as a memorial for coronavirus whistleblower Dr. Wenliang Li. (cite)
It may be that there is an inverse rite of passage and liminal state preceding the life that in turn precedes death of these worlds.
The participation that is key to bringing a protocol (ethereum) into a world is why I suggest transaction volume and difficulty of attacking the consensus layer as key metrics - both gauge participation in the protocol outside of economic metrics.
These moments are possible because the death-decision has been made. The companies that ran these games clearly possess the authority to decide. This is one of the strengths and weaknesses of a centralized system. The exact strength that makes a decentralized system or protocol resilient to attack is [ae]the same one that makes[af] it resilient to a death decision[ag], or lacking a death decider. Grief is useful – not only for emotional processing, but for archiving.[ah]


Part 2: Death and the Archive
To think about death in a digital context is inevitably to consider the archive. [ai]Most of the worlds we have looked at closely exist online – and it may be that the online context is uniquely suited to worlding. Certainly, the starting-cost-to-world ratio is lower there than in physical space. The online context also uniquely lends itself to the question of archiving[aj].
What kind of archive, cemetery, or memorial, is appropriate for these dead worlds? The first impulse might be to store all of the data that encompasses the world. In the game context, that would mean the game save file, the data of the map and all of its buildings. One could also store the game code, the game engine, even the operating system the game runs on, and the drivers for common hardware. We could store every version of the world that exists, for every tick in game time, so that we could replay deterministically every move that every character made with every game item—in fact, in our attempt at constructing an archive for a digital world, we could reinvent a blockchain from first principles. But what would we have actually captured what matters about the world?[ak]
In the Preserving Virtual Worlds Report, part of a collaborative research endeavor between Lindon Labs, the creators of Second Life, and several North American universities, the authors write:
An important part of the historical context for virtual world history is the fact that human beings (through their avatars) fill these digital environments with meaning that emerges from their activities in social spaces, regardless of whether the spaces are synthetic (digital) or physical. (cite)
Furthermore, “If we save every bit of a virtual world, its software and the data associated with it and stored on its servers, it may still be the case that we have completely lost the history.” (cite) They call this the fantasy of perfect capture: While we can back up the data that encompasses a game map, all the buildings that inhabit it, even the engine that runs it, none of these acts in fact captures what it really is. A world, much like a protocol, cannot exist in a static instant. A protocol must be enacted, and a world must be lived in.[al]
The Arctic Code Vault epitomizes this fantasy of perfect capture.[am][an] It is a ______. It contains _____. Unfortunately, the premise of storing this code in this way is completely unvalidated – even farcical. Who wrote this code and why? What did they use it for? The mistake being made is to equate the "meaning" of this code, with the code itself. Much like the game save state in an MMO, there is something fundamental about the life of an open source software project (both in terms of contributors, and users) that is absent.

(museums[ao] and permanence/immortality)

If all technology is, as has been often claimed, an extension of the body – is the endgame of all technology, ultimately, immortality?


A blockchain in particular makes a certain kind of claim about time and permanence: that it is ongoing, that it is unidirectional, that the past is immutable, that it is singular.


(the internet archive)

It's useful to contrast an archive with a memorial[ap]. With an archive, the fantasy of recreating the thing being archived potentially exists – if not literally the thing itself or the original document. No such pretense persists with the memorial. Instead, perhaps it exists as a testament to what cannot be archived. A memorial is a kind of compression technology[aq]: We make memorials when there is something uncapturable, too big, or that should be made to stand in for a larger symbolic meaning.


(some example memorials[ar][as], maybe the memorial against fascism)
The thing that dies is the thing that cannot be archived[at][au]. Only by understanding this will we be able to construct useful archives[av][aw] – and for what cannot be archived, to grieve
What digital processes aid grief? (second death, grief tech[ax])
Does the archive only grow? In both a personal and cultural context, we are all amassing archives accidentally and constantly. By _____ Facebook's databases will contain more dead people than living ones. Even in ___, Vannevar Bush, theorizer of the memex, complained of the __________. (cancers, life/systems analogy?)
In a paper called The Art of Forgetting[ay], Tomas Zahora discusses Agrippa and the medieval technology of memory. (explain this). Alongside this art of memory, was a less-discussed art of forgetting – undertaken when memory palaces had overgrown their usefulness or needed to be renovated. In other words how "a culture dependent on complex mnemonic systems handled an excess of remembered material" (cite). Techniques can be as literal as visualizing a wing of the palace being destroyed, or ___. This is aligned with some contemporary findings about memory. For example … (If you negate, you may forget). Zahora considers that the art of forgetting may have become particularly urgent in the era of Agrippa's writing, year ____, because it was a time of profound cultural change.[az] He writes,
Does it mean that in times of paradigmatic change our minds are stuck with outdated mnemonic frameworks amassed during years of study and subsequent practice? What happens when a whole mode of learning shows signs of stress? (cite)
(death rituals focusing on goal of repairing social fabric aka forgetting)
(revisit[ba] archives vs memorials. Conclude: some notes on how to kill[bb])


[a]This title is quite (and delightfully!) provocative. At a minimum, it seems to be in opposition to the life extension suddenly rich tech entrepreneurs have been fixated upon. I, for one, cherish the knowledge that I am impermanent and will eventually be altogether forgotten, so maybe I'm reading too much into this, but if this is an intended theme, it could be made more explicit in your analysis throughout.
[b]I was reading your essay and it reminded me of "this place is dead anyway" from the movie Swingers I think


https://www.youtube.com/watch?v=AuInkEF_dQg
[c]Vitality is in the eye of the beholder
[d]Could add here that later versions of this draft probably need to briefly note what an MMO world is, if say an older reader came across this who wasn't immediately aware.
[e]Love the cold opening in a Reddit thread 😀
[f]In writing about identity, especially online, there's a concept of "relativity" -- and maybe, death online also inherits that relativity. Yes, we have the same IRL but I think it's far more consistent than in digital spaces and abstract concepts.
[g]It's interesting to me that all of the most vivid examples of something that has died involve a residual part of the thing that continues to exist. Maybe if something completely vanishes it is more likely to be forgotten altogether rather than considered dead.
[h]Ok you get to this below but I'll leave the comment up lol
[i]In Saving Time, Jenny Odell argues that aliveness is much more about negotiating/adapting one's existence with time (entropy) — this may be a good reference for defining aliveness for your article, too.
[j]Related concepts: hibernation, frozen, dormant
[k]deactivation
[l]This is excellent. I love this
[m]+1
[n](maybe missing the word "we" here)
[o]makes me think about liminal spaces and hauntology
[p]2 total reactions
Nadia Asparouhova reacted with 👍 at 2023-07-06 08:45 AM
Alice Noujaim reacted with 👍 at 2023-07-12 11:07 AM
[q]in the diagram, you might want to use the word "world" oooorrrrr translate the diagram from a "Game" POV to a "World" POV
[r]Could be cool to include "zombie mode" type of transition, when it is revived by a fanbase later on in a new form. Maybe reincarnated is a better name.
[s]This called to mind the sentiment of "you die twice--bodily first, and then again the last time someone remembers you" 


Do digital worlds experience death when they lack attention, do they experience a rebirth when they are stumbled upon again (by...a human OR a bot??)
[t]Why is game death a supply-side process? 


Do users/players just not have sufficient ability to make decisions?
[u]Reminded of “meditation is the art of conscious dying”
[v]The Yale research on reviving portions of pig brains after death might be a further development slightly challenging this consensus, which some readers might be aware of: https://news.yale.edu/2019/04/17/scientists-restore-some-functions-pigs-brain-hours-after-death Though the outcome of the research was still limited, and I may have come across some criticisms of it which I forget the details of offhand.
[w]Fascinating
[x]I read somewhere (IIRC) about certain approaches for restarting the heart etc. being a relatively recent 20th century invention which changed the older meanings of death. It may have been in the context of reading about near-death experiences - I think I read something (this should be double-checked) that they're a more common experience in modern times now that we have more ways to revive people from certain physiological conditions.
[y](Here's one article I came across in a quick search which seems to talk about their history as a relatively recent scientific concept, from what I saw quickly skimming it: https://www.scientificamerican.com/article/what-near-death-experiences-reveal-about-the-brain/)
[z]Canada MAID program? https://www.justice.gc.ca/eng/cj-jp/ad-am/bk-di.html
[aa]Makes me think about various interpretations of queerness in the metaverse -- how do digital identities enable multiple lives/deaths?
[ab]Do you know if these criteria have changed over time? Similar to how human death criteria have
[ac]Good question - I don't know if they have (especially on this site) but it would be interesting to try and find out
[ad]Something worth exploring might be the distinct ways in which different cultures grieve or celebrate death. The relationship between death and beauty has long fascinated me. At a minimum, many of life's most cherished moments are finite, such that I see an inherent link between our own mortality and many of the experiences I most cherish. Freshly cooked food, a perfectly pulled cappuccino, and a fine bottle of wine all fade rapidly due to their best aspects relying on fundamentally impermanent chemical compounds. A keurig is not an exquisite coffee experience, and never will be. Even the euphemism "le petit mort" evokes the concept of death to refer to the exquisite. 


That being said, there's an odd tension here, for having witnessed death, I have never found it beautiful, whether violent and sudden or peaceful and elderly.
[ae]This idea of decentralized systems lacking a death decider makes me think of religions as kind of a cross between a centralized and a decentralized protocol system. People can keep practicing their religion even if the central authority (e.g., the Pope) says it's dead, but I guess they would be doing it in an 'unofficial' 'less legitimate' sense.
[af]I'm not sure if this is the intent but this almost makes me feel sad. What would happen to the couple who put their marriage vows on the blockchain, etc., if these protocols die without room for any grief? 


I find myself wondering: what does good protocol death look like?
[ag]but they are not immortal. Which is cool to think about.
[ah]🔥
[ai]I think I mentioned this before, but I strongly feel like this paper would benefit from mentioning the concept of the death of authorship, since I found myself repeatedly compelled to look for it to be mentioned throughout my time reading. It would be especially interesting to consider in the context of game worlds, since they might challenge the typical “death of the author argument” of an audience outliving and re-interprets an author’s work by involving situations where both an author and an audience would “die.”
[aj]Isn't memory (per Kei's project) an intrinsic form of archiving in the analog realm?
[ak]🔥🔥
[al]so good — a protocol is alive
[am]This is going to stick with me - such a succinct expression of the problem
[an]so many digital preservation projects in archaeology suffer from this delusion
[ao]This line makes me also think about the concept of a "living museum," ie. when something like a ship or a farm is preserved in an operational state instead of being a static archive. Or a "living memorial," though I feel like those necessarily become less precise in implementation.
[ap]Funerals are declining (at least in US society recently). I tried to find similar data on monuments, but given the recent focus on confederate statues, couldn't find anything apart from number of those monuments still standing v. those taken down. If I had to guess, I would say we're also erecting less monuments over time. Are memorials therefore becoming less common? Does this time trend tell us anything interesting about society currently versus one that produced more memorials?
[aq]🔥
[ar]rachel whiteread's holocaust memorial in vienna is a beautiful example -- it's a sort of inverted library where the books spines' are not front-facing, evoking the impossibility of accessing that (those lives) which has been lost forever 
good photos here: https://www.re-thinkingthefuture.com/designing-for-typologies/a2623-judenplatz-holocaust-memorial-by-rachel-whiteread-a-plethora-of-signs/
[as]Day of the Dead seems like an angle on memorializing as an interactive process which would be relevant to include; I feel like it's something many American readers have already heard of a little bit while still being foreign and unfamiliar.
[at]I'm curious to understand this better. just to poke it a bit further: museums regularly archive and collect human remains for example. isn't a mummy is something that died and has been archived?
[au]I would say instead that the "person" or human being is what has died, and the thing that can be archived (the mummy, the person's belongings, associated artifacts, etc) is not really equivalent
[av]i want to hear more about this! and what makes a useful archive with this context of things that should be left in their graves
[aw]very good question - I don't think it's articulated clearly anywhere in here, but I think it's related to the section on the arctic code vault (the meaning of the code is not in the code) ... as in, a more useful archive of something like an MMO or blockchain would try to capture the things not present directly in the game/blockchain itself that were nonetheless central to it's meaning, and this would probably be involve something more like history-writing or storytelling than a game save file or copy of the blockchain itself
[ax]Example IRL grief tech https://www2.obitus.com/
[ay]Right to be forgotten?  https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/individual-rights/individual-rights/right-to-erasure/
[az]This is a great thought. The Art of Grieving / Moving On / Letting Go... the urgency of these things is a function of how much things are changing.


On the one hand, it is an important art in cyberspace, as often things you used to know get lost in the flood of information. So there are more times when being able to forget is useful. On the other, maybe it's less and less often that we lose things online. This makes the art of forgetting a rarely practiced skill, which results in digital loss being surprisingly traumatic.


On a related note, the concept of Search/View/Watch Histories on various apps is something we don't have IRL, unless perhaps one keeps a daily journal.
[ba]This whole essay has an appropriately ethereal, dreamy vibe like it was written by a ghost almost who is not aware there is a reader in the room :) It almost feels intrusive to read it. I like it though. You can either lean into this vibe or make it more self-conscious of the reader and speak to them. The flow feels natural where I get the references but a little obscure where I don’t. I’m not a gamer, so some assistance over those parts would help. But overall a really original wander through an unusual topic with unexpected blockchain insights. Speaking explicitly to ossification might be interesting. Do the mortuary cultures of Ethereum and Bitcoin differ? How? One thing I’d suggest you *dont* change is the slight sense of wildness in this essay compared to the others. That’s valuable. I’d try to retain that feel as you polish and finish.
[bb]i really love the meandering of this essay and the provocations it makes throughout. I think I'm left wondering how do i attend to the worlds i care about (whether that means active maintenance of their life or making the death decision and hosting an appropriate rite of passage). this question of how to determine whether to maintain (and the work involved in that, the preservation of life) or to kill when it can stem from the same care of the world lingers



    Motivation & Rationale
















    false 2023 05 03T05 19 07Z





              Summer of Protocols  Retrofitting the Web




              Technical Note 1








      The problem that I'm trying to solve or at least contribute in some
      small way to a solution has been notoriously hard to articulate
      I've narrowed it down to not one  but three points of departure

        Repairing a medium Hypermedia has been an object of both theory
        and practice for decades preceding the Web and even before
        computers  The Web now ubiquitous traded off a lot of really
        powerful ideas in return for not only easy implementation and
        deployment but also easy  instantaneous  global  publishing
        plus one key capability the others categorically lacked  links
        that cross both system and organizational boundaries  But  there
        is nothing in principle preventing those powerful ideas from
        earlier hypermedia systems real and imagined from being grafted
        back onto the Web  It just needs a clear vision and a little
        elbow grease Kicking off a Jevons Paradox

          If that proposition isn't valuable enough on its face  consider
          the problems early  and proto   hypermedia pioneers were trying
          to solve

            1895  Paul Otlet wanted to organize the world's knowledge 1945
            Vannevar Bush wanted to supercharge individual intellectual
            pursuit 1960  Ted Nelson wanted and still wants  among many
            other things to open up an entire new dimension of creative
            expressivity  and get authors paid for their work 1962
            Douglas Engelbart literally wanted to augment human intellect

          All of these goals are contingent on making it super cheap to
          marshal lots and lots of tiny little pieces of information
          connected to one another by an even bigger number of links
          and this is important links of all different kinds  Compared
          to its predecessors as a specimen of hypermedia  the Web  at
          least out of the box  ranks mediocre to poor  Without help
          the Web is limited in its range of expressivity as a medium
          because the cost of managing tiny pieces densely linked is
          prohibitive  and so it reverts back to the page being the basic
          unit of discourse  Again  nothing in principle prevents this
          from changing  so the question is  what could we make if the
          econophysics of the Web were dramatically altered

        Empowering people  platforms can fend for themselves

          And if that appeal isn't  well  appealing  consider that
          there is an entire class of tools that exist or would exist
          just beyond the reach of a spreadsheet  That is  they do very
          little besides afford data entry into some intricate structure
          or other  do some trivial manipulations  and then represent
          that information back to you or perhaps somebody else  Many of
          the needs for these tools come from professionals in various
          niches often very close the places such tools would be produced
          The overhead  however  is still so high  that they need to wait
          for a vendor to come along and make an app for that Personal
          knowledge management tools are beginning to fill this gap  but
          in my opinion they fail irretrievably in one important way or
          another  One such failure mode is platforms  the integrity of
          your content depends on paying regular tribute to some company
          or other  Notion  Roam  in perpetuity  In systems where you
          own your data  Obsidian  Logseq   its disposition is ad hoc
          In either case  you lose a piece of what makes the Web what it
          is  a set of open standards and protocols that provide a single
          common interface  that doesn't have to be tailored to a single
          vendor  no matter how progressive they turn out to be The idea
          behind this initiative is to set a new floor  where the little
          guy has some bargaining power  not only because they own their
          data  but that data doesn't favour any particular vendor  This
          is about addressing a bottleneck to getting important work
          done as much as it is about addressing a power disparity





        Repairing a Medium The Web left a set of powerful capabilities
        on the table  Let's put them back

      The Web  out of the box  is a system for creating what I
      characterize as sparse hypermedia  long documents pages with few
      links connecting them  If you were to take a typical website and
      strip off the navigation bar and footer  the result may as well
      be a collection of Word documents or PDFs in other words  print
      era documents  In my opinion  one of the main value propositions
      of hypermedia is being able to spare people the need to read  or
      watch  or listen  any more than they have to  You accomplish that
      by breaking up the content into little pieces and making them
      subject to activation by the reader  I call this  by contrast
      dense hypermedia  Dense hypermedia is what we had before the Web
      from Hypercard and StorySpace  all the way back to the experimental
      systems and the original visions of hypermedia pioneers  What these
      systems had in common  though  is that they were all monoliths
      hermetically sealed environments  What the Web did was make
      hypermedia permeable  able to cross administrative boundaries with
      no more effort than it takes to access a local resource  It is also
      characteristically easy to deploy  with the heavy lifting hived
      off to the poles of browser and server  the part in the middle
      instantaneous global publication can be had practically for free
      The Web did these unique and powerful things at the expense of
      capabilities present in its predecessors  as well as introducing
      some problems of its own

        Backlinks On the Web  links go forward only  You can't tell what
        links to a particular resource  That's extra work   This is one
        of Ted Nelson's original beefs with the Web Transclusion and
        the general problem of content reuse On the Web  you can embed
        images  audiovisual content  and even entire other documents
        into a document  and you can reuse scripts and presentation
        information  but you can't seamlessly integrate an arbitrarily
        narrow excerpt from some other source without resorting to one
        of a zillion clunky ad hoc solutions  This leads to a dynamic
        where it's easier to copy information than it is to reference
        it  which in turn leads to additional overhead of keeping said
        content up to date  and the inevitable failures to do so Other
        linking mechanisms left unexplored On the Web  you have arcs
        whether ordinary safe links or state modifying ones  such as
        form submissions   and you have what I characterize as naïve
        embeds basically just a rectangle of remote content plunked down
        wherever you put it  In addition to proper  seamless  transclusion
        earlier hypermedia systems had stretchtext  telescopically
        expanding detail   conditional display  e g   Choose Your Own
        Adventure   and view control  different representations of the
        same content   among other interesting and powerful specimens
        of rhetorical  didactic  and literary value left on the table

      Again  the Web can do all these things in principle  and ultimately
      does accomplish many of them in practice   but the solutions are
      ad hoc  one off  and mutually incompatible  We're looking for a
      principled approach to restoring these lost capabilities  so they
      span organizations and implementations  as the Web does with its
      off the shelf repertoire

        It's The URLs  Stupid After decades of experience with this medium
        I conclude that much of what ails the Web can be traced back
        to the inherent tensions around URLs  HTTP URLs were initially
        derived from file paths  File systems themselves come from an era
        when storage was scarce and computers were typically not networked
        As such the naming and locating of computer files a necessary
        precursor to saving anything at all wasn't something one did very
        frequently  and in any case  what you chose to name a given file
        and where you located it  was typically nobody else's business The
        cardinal sin of the Web was to attach public consequences to what
        had until then been a largely private affair  The state of the art
        is that URLs are shamefully unreliable  because there's nothing
        making them be  Link rot  and its subtler relative  content drift
        is so bad  the median survival time of a URL is something like
        three months  Decades into this technology and there's still
        thin  if any meaningful  systematic support for the continuity
        of Web resources and the URLs that identify and locate them

          The Web's inventor is aware of the situation  and tried to
          absolve himself of the issue 25 years ago   Maybe we can
          forgive him  it seemed like a good enough idea at the time

        The tensions I alluded to have to do with diverging interests
        around the content of the URL string beyond its bare technical
        function  For one  URLs are important user interface elements
        they lend themselves to being made intelligible  memorable  even
        guessable and or inferrable  They telegraph information about the
        topological structure of the website  and they provide an entry
        point to the Web from any other  typo graphical medium  URLs are
        therefore potentially valuable symbolic real estate  which gets
        even more valuable as they diffuse out into the environment  The
        tension  precisely  is that choosing a good  intelligible  future
        proof URL takes considerable editorial and curatorial attention
        At the same time  there is demand to pick one quickly  since
        you can't refer to  or even save   the document until it has a URL

          It has been common for some time for URL slugs to be derived
          from document titles  but if you change the title  you change
          the slug or maybe you don't  So then you change the title but
          the URL doesn't change along with it  Again  from a technical
          perspective this doesn't matter  but if you want slugs to track
          titles  something  somewhere has to store that naming history

        The elephant in the room for URL continuity is other people's
        websites  The solution in principle is obvious  you can't trust
        other people's websites so you will have to police every outgoing
        link your website makes to ensure it continues to represent the
        same thing it did when you linked it  This is an easy  if labour
        intensive  engineering problem in theory but a hard design problem
        in practice  so hard I am officially declaring it out of scope
        for this project  I do  however  endeavour for this project to
        demonstrate a model citizen on the Web that is  one you can depend
        on its constituent URLs pointing to the same thing in perpetuity

            That is  as long as the domain and Web server  cloud
           lambda  whatever  bills get paid  I have considered what
           happens after the sunset of DNS and single point of hosting
           but the best I am willing to do at the moment is design a
           system that contemplates these possibilities and leaves the
           door open to them  without trying to tackle them directly





        Kicking Off a Jevons Paradox What could we make on top of a
        substrate of dense hypermedia

      The introduction of the spreadsheet contributed to the M&A boom of
      the 1980s  and eliminated an entire class of journeyman accountant
      It achieved this by shrinking the problem of financial scenario
      planning to a point  45 years later  spreadsheets are used for
      everything  they are a veritable Swiss army knife of computing In
      fact  to date  the spreadsheet remains the only viable fully self
      serve programming environment available to the general public  The
      problem is that there is a gulf between what can be accomplished
      with a spreadsheet  and what merits an entire team to be spun up
      to deliver an entire product  The gap is sparsely flecked with
      solutions like R and Jupyter Notebook  but they still require
      learning how to code  Merely knowing how to code isn't enough though
      because of the colossal step change in effort once you exit the
      spreadsheet capability envelope  Many such targets aren't viable
      unless you divert your energy into creating a product  This of
      course is another entire universe of effort on top of what it would
      take to make a tool just for yourself  What this means  in the first
      place  is that a lot of problems simply aren't going to get solved

        I may as well preemptively state my position that I am less
        sanguine about the potential of statistical methods of computing
        than many  Arguments have been floated about the benefits of so
        called generative AI for code  but if you don't understand it
        well enough to write it yourself  how are you going to understand
        it if the machine generates it for you  What makes spreadsheets
        successful is that they are like Duplo blocks  pure functional
        language  batteries included  no messy variables to worry about
        In other words  they are intelligible General purpose programming
        languages have so many degrees of freedom  to say nothing of
        copious housekeeping minutiae  that being able to tell when
        a large language model generates a subtle but consequential
        error will still require a considerable amount of programming
        expertise  You can't trust an LLM for anything that has to be
        genuinely load bearing  and I doubt more parameters will make a
        difference  I do nevertheless believe that there are many roles
        for machine learning in software development  but facilitating
        programming neophytes by magicking up entire production ready
        apps is not on my list

      The most significant advancement in spreadsheets in decades was
      putting them on the Web  That was actually a genuinely good idea
      Same with the implicit version control and collaborative editing
      capability that came along for the ride  That said  I suspect
      people are finally outgrowing the fundamental constraints that
      make a spreadsheet what it is

        The data types are hot garbage  insert joke about Excel trying
        to turn everything into a date there are no composite data
        structures beyond row or column vectors  or otherwise entire
        rectangular regions there are no cycles  nesting  or recursion
        there are limited means of hiding how the sausage is made  like
        you can with the various notebook products and so on


        Consider the following pedestrian example of the limitations
        of a spreadsheet  a humble contact list  one person per row
        Now  suppose some people have more than one address  or even
        simpler  more than one phone number or e mail  To capture these
        values you'll need as many columns for each as the person with
        the most of either  This situation can be palliated somewhat by
        concepts like home phone or work email  but what if somebody has
        more than one job   Or home  for that matter   Contrast with a
        dedicated contacts app  which is record oriented  you can have
        as many phone numbers or e mail addresses  each tagged with its
        respective qualifier  per person as you like

      The existence proof that people want more can be observed in the
      proliferation of general purpose products from AirTable to Notion
      to Roam  to specialized products  bug trackers  CRMs  even tip
      calculators  too numerous to list  I submit that each one of these
      is nevertheless missing at least one piece of the  second  brain
      What makes these products viable is that unlike a spreadsheet
      shaped problem  individuals can't fashion their own solution or if
      they can  their solution is cumbersome enough that they prefer the
      niche product Imagine spreadsheets didn't exist and you needed a
      vendor to show up and make a separate app for every trivial little
      thing you use a spreadsheet for  What I'm arguing is that this
      is in fact the situation  just a little bit farther up a gradient
      of  not complexity per se  but let's call it perhaps intricacy of
      structure  There simply isn't enough software development capacity
      for there to be An App For That   for every that

        Even if there was  the apps would be walled off from one another
        as apps are wont to do  As primitive as spreadsheets are  they
        are at least permeable  the CSV file format is the de facto
        lingua franca of structured data  for some value of the term
        Which is depressing  but at least it's something  Try making
        adapters for every pair of APIs under the sun assuming their
        respective apps even expose them


         DIAGRAM TBD

          Once I figure out how I want to draw this spectrum lookin' thing

            not even worth the trouble of a spreadsheet use a post it or
            calculator or whatever well within spreadsheet rubric at the
            edge of spreadsheet capability envelope  no macros  if you
            are using macros you are Actually Programming can't be done
            with a spreadsheet but worth writing code for assuming you
            can code can't be done with a spreadsheet but categorically
            not worth writing code for worth writing code for  but only
            if you can productize it serious software product need a
            whole team company to support it

          The idea is to move the line of what can be achieved by an
          individual  and is furthermore worth doing  to the right  and
          even fill out the left a little bit


      I posit that if we had  at the core  a way of entering  representing
      and exchanging structured data not to mention its global
      instantaneous publication plus a handful of sundry computations
      we could dramatically extend the capabilities of an individual
      knowledge worker  before they had to touch anything that looked
      like conventional  imperative code  A number of the problems that
      aren't worth solving because they currently require a lot of time
      to program if not spinning up an entire team and company and capital
      to tackle will be within reach  This implies that the companies who
      currently occupy this zone will have to move up the effort gradient
      Some will go the way of those journeyman accountants  Oh well

        To set my argument apart from the current profusion of no code
        low code systems  what I still don't see a lot of again  having
        spent my entire adult life and then some in this industry are
        ways for ordinary people to enter  by hand or programmatically
        arrange  and represent structured data  That is  notwithstanding
        the 45 year old spreadsheet  and all its limitations

      Two decades ago  the media theorist Lev Manovich posited that
      the database by which he meant navigable hypermedia environments
      would be the transformative medium of the 21st century  just
      as film transformed the 20th  I submit that we feel the effects
      of databases on us  like victims ultimately of governments and
      corporations but we still can't afford our own  After 25 years
      immersed in this industry and I showed up late in the game it's
      still a serious project to do anything on a computer that is more
      sophisticated than a spreadsheet  and waiting around for a vendor to
      serve my niche  esoteric needs especially without onerous strings
      attached is a laughable proposition  So what would happen if the
      unit economics of the database or more appropriately  the knowledge
      graph were to get dramatically cheaper




        Empowering People


        Who owns your attention

      there isn't enough software development capacity to do all the
      things  and the capacity that is present extracts a shitton of
      rent i'm basically arguing that the hole between spreadsheet and
      software that genuinely merits a company to be built up around it
      is bad for society gotta talk about the intersection of

        climate change increasing wealth inequality misinformation
        disinformation eroding democracy


        Climate Change While the germ of this project goes all the way
        back to 2006  I got a boost in 2015 when I read Bret Victor's
        sprawling essay on climate change  I was particularly moved by
        the chapter called Media for Understanding Situations  in which he
        argued for the use of models and data in interactive simulations
        to inform public policy discourse  It was enough of a lift for
        Victor to do all he did to make the argument  we shouldn't also
        expect him to figure out how one would pull it off at scale
        although he did put a dent in it  What got me interested was
        that the technical problem of marshalling data and computational
        models and making them available is one I actually know something
        about The complicating factor here is that Bret Victor wrote his
        essay in 2015  when it was still possible to believe that if you
        just gave people good information  they would make good decisions
        That is  before the bullshit renaissance that was in full swing a
        year later  I nevertheless believe the technical problem is still
        worth solving  just perhaps not for the exact original reason


        Increasing Wealth Inequality Climate change is obviously Bad®
        and We Should Do Something  about it  The We in this case is
        typically shorthand for citizens of democracies  Except our
        governments only do what rich people want  and the things rich
        people want are typically good for them and bad for everybody
        else   Or  if it is any good for us  it's better for them somehow

          You don't have to even posit some grand conspiracy for this
          mechanism  enough of it can be accounted for with boring old
          availability bias  rich people and politicians have a lot of
          access to each other  even if those interactions were completely
          benign In democracies you can think of this as part of the game
          rich people and politicians collude in the gaps  on everything
          that isn't acutely invidious enough to get the latter voted
          out  Sometimes even that isn't enough  in Canada right now we
          have a government in its third term a minority at that pushing
          through bill after bill that nobody  well  somebody  wants  and
          they keep getting re elected because the voters understand the
          other guys would do the same  but worse Presumably how you keep
          this behaviour in check is an engaged citizenry who has a set
          of baseline expectations across the board  and a list of things
          that nobody  regardless of who they vote for  will tolerate

        As wealth inequality sharpens  the dynamics produce a decreasing
        headcount of individuals  each with an increasing quantity of
        resources at their disposal  Everybody else is distracted with
        their own affairs  many of which are economic in nature  Too
        busy putting out fires to do things like participate in civil
        society  Now  when people write on inequality they usually mean
        income inequality and remark about how unfair it is  I'm not
        particularly interested in questions of fairness  rather I view
        extreme wealth inequality  when there are eight billion people
        on the planet and counting  as hazardous to human civilization

           LORENZ CURVE SIMULATION GOES HERE

            I don't love Lorenz curves because they represent perfect
            equality as not only normative but somehow achievable  Perfect
            equality in an economic system is an unstable equilibrium
            that disappears the instant agents start interacting  And
            even if it was somehow stable  perfect equality would amount
            to political gridlock  What you're going to see instead is
            something power law ish  and the question to ask is what
            is the value of the parameter  Represent that as a Gini
            coefficient and the higher number corresponds to fewer people
            in charge of more resources each  which you can interpret
            roughly as how much of other people's attention can one of
            these individuals divert



        My thesis is essentially that people lose resolution as the
        pool of resources under their control increases  as the pile
        gets bigger  they just round off at bigger orders of magnitude
        When individuals are in control of vast quantities of resources
        even the smartest  most well meaning person will make what are
        effectively targeting errors  What we're more likely to get
        though  are people who are not well meaning  nor even especially
        intelligent  This happens irrespective of whether those resources
        are yours  The difference with private wealth is it's your money
        so you're only accountable to yourself

          When you don't have a lot of money  you have to think
          scrupulously about how you allocate every penny  When you have
          more of it  there comes a point and it doesn't take much where
          you can afford to be sloppier  Not only that  you kind of have
          to be sloppier  because if you acted the way you did when you
          didn't have any money  you'd have all this money left over
          and no time to enjoy it  So you allocate bigger and bigger
          chunks at once  Eventually  you hire someone to do that for
          you  and they hire someone  and so on  and that's basically
          the economy in a nutshell

        The reason why I focus on wealth and not income is because wealth
        represents how much of other people's attention you can divert
        at any instant  Income is at best a derivative of that   If you
        want you can think of net  of course income as in the limit what
        you get for your attention and wealth as how much attention you
        can command


        Strategies for getting stupendously wealthy  feel free to mix
        and match

          find some activity that reliably earns you a dollar in profit
          repeat a billion times play a sequence of super convex
          compounding parlays externalize costs and risk  heads I win
          tails you lose  etc own something that enables you to extract
          economic  rent

        Software systems contain these ingredients in unprecedented
        abundance

        When you're poor you can't speculate cause you can't afford to
        lose anything  ever  When you're rich  you can make tons of risky
        bets with huge antes and way way bigger upsides relative to any
        conceivable downside  lose most of them  and still come out on
        top  because and this is true for everybody but it takes being
        rich to really leverage it your adventures only have to pay off
        on average  There's also a huge chunk you can just burn without
        any expectation of return  That means copious vanity projects
        and white elephants  lavish consumption  wild speculative bets
        and unending rivalries with peers  Rocket to Mars  indoor ski
        hill  solid gold toilet  giant limestone pyramid  ultra super
        megayacht  whatever  Do you really want the attitude of the
        people setting the global agenda to be I'll be fine regardless
        of the outcome  so YOLO

        the final remark i'll make before i close this section out is
        wealthy people get a lot of leverage  that only becomes more
        pronounced under conditions of extreme inequality

        human civilization has laboured under much more extreme wealth
        inequality than we see today but that was before the anthropocene
        pick your favourite emperor  they could command entire nations
        but couldn't put a scratch in the planet's ecosystem no matter
        how hard they tried

        climate change is a problem of the modern era basically a side
        effect of us being really successful as a species  of business as
        usual but also a story of people who knew and sold that existence
        to us anyway

        if we want to survive as a civilization we have to evolve new
        ways of organizing and people who do well with the current way
        of organizing are always going to get in the way of that


        The sharpness of the inequality curve is going to track with
        the topology of the economic network  sharper reflects one that
        is more concentrated and centralized  blunter is more peer
        to peer and distributed  A gentler wealth distribution curve
        means the people at the top can't divert as much of everybody
        else's attention for their dumb bullshit  To blunt the curve
        you have to be able to cut these entities out of your economic
        interactions  You have to be able to deny them your attention
        In Bruce Sterling's words  keep more of the money yourself


        Misinformation & Disinformation One evolution in my own thinking
        over the last several years was a Copernican inversion in
        how I conceptualize information  belief  rhetoric  and reason
        okay what exactly do i want to say about this something like
        more information means more misinformation most instances of
        misinformation are barely consequential  people absorb a loss
        that amounts to a minor inconvenience and carry on

           at least the consumers of that information  misinformation
           about a person can be seriously harmful to that person
          so you have a cipolla stupidity area effect there

        corollary  you can believe a lot of wrong shit and it doesn't
        matter ooda loop is not natural  it's something you have to train
        into people aren't looking for facts to integrate into a model
        to make decisions  they're looking for stories that justify what
        they're already motivated to do
         or what they've already done
        okay but what if you actually wanted accurate information
        how would you get it trusted sources  including identifiable
        authors   receipts  full bibliography  tamper resistant  expiry
        date also wait to respond  unless the outcome is worse if you
        don't may need to pay for good information if misinformation is
        sufficiently consequential will need some kind of pricing model
        for that  ie how much is appropriate big thing though is people
        who want good information should be able to get it hypothesis
        is they will outperform the ones who insist on bad information
        albeit subject to certain conditions



        The Erosion of Democracy the whole thing about voters getting
        the governments they deserve

          At this point I should address the well maybe we need to get
          rid of democracy crowd  I submit that democracy is a handbrake
          on the really  really bad outcomes  at least when it functions
          Also note I'm not American  so my experience is different
          from them The point is that there are limitations on what
          individual human beings can address and coordinate around
          Also  if you're holding out for a world order presided over
          by a benevolent AI dictator à la Culture series  the planet
          will probably be a cinder by then



        How Anything I'm Doing Could Possibly Help i'm not pretending i'm
        saving the world here radically more powerful ways of authoring
        consuming  verifying information radically more powerful ways of
        communicating facts radically more powerful ways of storytelling
         sorry  the information density of substack blows

        XXX THIS IS RIPPED FROM ABOVE SO REWORK IT LOL The problem with
        these niche products is that they are sold as subscriptions  and
        so the intricate memory palaces we create with them are contingent
        on plugging some meter in perpetuity  It would be one thing if
        it was just a matter of money  but there are mounting qualitative
        concerns with SaaS platforms that are becoming apparent as we gain
        more experience with them  To hedge against platforms  we need
        to be able to possess our own data but the data is worthless if
        it can't be used outside the particular platform that disgorged it

          Platforms leaving aside the cornucopia of self dealing
          they engage in with your data have outages and data loss
          incidents  they get hacked  and they go out of business  Or
          they get acquired  and the new owner shuts down the product or
          changes its policy  Or they just change their policy anyway
          to whatever they like  whenever they feel like it  Consider
          how certain platform companies responded to the war in Ukraine
          by denying service to otherwise uninvolved Russians who were
          literally minding their own business  One could argue that's
          an appropriate response  but if some company you depend on
          decides without warning that for whatever geopolitical reason
          they're no longer going to serve you  you're screwed if you
          can't take your business elsewhere





      The problem that I'm trying to solve or at least contribute in some
      small way to a solution has been notoriously hard to articulate
      Some prior art I can draw on for part of  an explanation is the
      independent researcher Bret Victor's 2015 climate change essay
      because climate change is one possible point of departure

        The essay itself is on the order of 12 000 words  and richly
        illustrated with animations and interactive demonstrations
        Victor said he took over two months to write it

      The essay is an exhortation to people trained in various technical
      disciplines  about specific things they can do to address climate
      change  In it is a chapter with the somewhat obscure title
      Media for Understanding Situations  Victor uses as his example
      the situation of public policy discourse where quantities are
      involved and what public policy doesn't involve  at least monetary
      quantities  He remarks that it's possible to use the same data to
      argue either for or against a given policy prescription  simply
      by tweaking some of the variables  The conventional op ed format
      Victor argues  simply doesn't have the bandwidth to communicate this
      nuance  because it has to set the dials somewhere  His solution
      hand said dials over to the audience  so they can determine where
      if at all  is the sweet spot of parameters that intersects plausible
      acceptable  and effective

        Major media outlets like the New York Times and Washington Post
        produce these kinds of interactive features from time to time
        but they tend to be big budget  one off endeavours  Victor is
        calling for this capability to get cheap enough to do everywhere

      This essay was written a year before what can charitably be
      described to use a technical term as a bullshit renaissance  Its
      rise highlights a closely related problem  it is manifestly no
      longer sufficient assuming it ever was to simply give people
      information  though accurate information has scarcely never been
      more necessary  To animate the facts  there needs to be a narrative
      and not every audience either understands or is motivated by
      the same story  A superposition of stories that hopefully all
      draw the same conclusion has to exist all at once  The medium
      for understanding situations therefore has to understand not
      only the situation that needs to be communicated  but also the
      various overlapping constituencies to whom a given situation must
      be communicated  and what their often diverging  if not outright
      conflicting expectations are Climate change  moreover  is a prime
      example of a global  existential  wicked problem

        there is no correct solution  only better and worse ones there
        are numerous stakeholders involved  with diverging  if not
        outright conflicting interests most  if not all solutions will
        have identifiable winners and losers most solutions involve most
        constituencies giving something up there is often disagreement
        about what the problem even is and in the extreme case  whether
        it even exists

      So it is with climate change  there are people who have demonstrated
      that they are perfectly content with the prospect of living on a
      cinder  as long as they're in charge of it

      support communicators to communicate accurate information
      persuasively you don't need accurate information to communicate
      persuasively  but if you're going to claim accuracy you'll need
      to cite your sources and those sources will have to be legitimate

        Arguably a structural basis for the bullshit asymmetry principle
        that bullshit costs an order of magnitude more effort to refute
        than to create stems from the fact that non bullshit has to cite
        and just as if not more importantly  vet its sources Worth noting
        as well that bullshit just got at least another order of magnitude
        cheaper to produce with generative artificial intelligence


      the state of the art of scientific communication sucks basic unit is
      the paper  only just starting to make data available no publication
      conventions for data replication is in the toilet Scientists don't
      need publishers as much as they need a publishing function science
      journalists often misinterpret results

      we can posit a certain category of tool it isn't really a tool as
      much as a thin wrapper around data really just a way to create
      a constellation of small  structured  highly connected chunks
      of information  and navigate around in it plus a handful of ways
      to manipulate how said information is represented plus a handful
      of other operations besides the pulverized bits are addressable
      meaning they can be referenced and reused pkm  tools for thought
      second brain  are in this category add collaboration support and
      message queue task scheduling and you net most groupware  which
      also means  modulo scaling  you net most social networks a big
      subcategory is niche tools for professionals


The Participant in a Thousand Protocol Systems - Angela Walch
My Take on Protocols and Protocol Systems
Protocols are sets of rules (explicit or implicit) that shape the behavior of groups of people. You can put all the other terms for rules in that bucket, too: norms, laws, standards, customs, traditions, what have you. Protocols provide signals to humans that they **should** act in a particular way – that one path of action or inaction is normatively better than another in a given situation, and there will be consequences of following or not following the signaled path.
Rules aren’t that interesting on their own, though, as this former lawyer and any reader of the Internal Revenue Code will tell you. It’s when you mix rules with people doing things in response to them that things get interesting.
Let’s put it in equation form:
Protocol System = Protocol + Group of People + Group Acting in Response to Protocol
It is the people systems that protocols enable - people interaction systems — that make them interesting to me. Systems like nations, religions, schools, professions, families, blockchains, and countless others. Systems that, through the interactions between the participants in them, can have interesting outputs and outcomes. Outcomes that would not emerge from a single party following rules. Or a mass of individuals behaving in ways not shaped by a common set of rules.
### ***The Protocol Participation Cycle (PPC)***
The meat of my project is mapping out the journey an individual takes through a protocol system — from entry through participation to exit (or death, I suppose). I see humans as “bundles of protocols” in that we each participate in countless protocol systems simultaneously and enter and exit them throughout our lives. I think there are key moments in our individual interactions with protocol systems that apply whether the protocol system is a religion, a nation, a blockchain, a career, or a marriage. 
The title of my project, ***The Participant in a Thousand Protocol Systems***, winks at Joseph Campbell’s famous book, *The Hero with a Thousand Faces*, which describes how an individual (in his case, a mythic hero) goes on “the hero’s adventure”:
<aside>
<img src="/icons/walk_green.svg" alt="/icons/walk_green.svg" width="40px" /> A  hero ventures forth from the world of common day into a region of supernatural wonder: fabulous forces are there encountered and a decisive victory is won: the hero comes back from this mysterious adventure with the power to bestow boons on his fellow man. (3d Edition, New World Library, 2008, p. 23.)
</aside>
In Campbell’s vision, the hero figure could have a different face (Hercules, Buddha, Osiris, or countless others), but the model of the journey is the same. 
In my vision, the participant travels through a thousand (arbitrary high number) different protocol systems in their life (their religion, family, education, profession, gender, etc), but the path through every protocol has similar important inflection points (entry / thriving or dysphoric participation  / deconstruction / revision of identity or protocol or exit from protocol / building new protocol system or entering another). 
I think there are important questions raised by the way people enter protocol systems (was it with with agency, unconsciously, via coercion, etc?) and how that affects the legitimacy of the protocol system itself, as well as participants’ consciousness of the protocol system they inhabit. 
Similarly, there are important questions raised by the identity individuals assume within a protocol system, and whether that identity is a good “fit.” I introduce the concept of “protocol identity dysphoria” to describe the angst of figuring out whether a protocol identity is right for you and what to do about it. 
Finally, the exit of a person from a protocol system is another key moment in the journey. The stakes of leaving a protocol system may be extremely high, particularly if a person has built a strong identity within the protocol system and has many ties (social, financial, reputational) to it. An exit may mean that a person loses their protocol identity and everything tied to it, and has to begin again by entering another protocol system or building a new one. Further, a protocol system may leave an overhang even after a person’s exit, in the ways it has shaped their identity and psyche (e.g., trauma). 
I think that the different phases of the cycle interact in interesting ways (e.g., your mode of entry may impact whether you suffer from protocol identity dysphoria), and that an identity you have built in one protocol system (System 1) may affect your choice of whether to join a different protocol system (System 2) (what I call “protocol determinism”). Protocol determinism can be highly problematic if your entry into System 1 was involuntary, and thus taints the agency of your entry into System 2. 
At any rate, I haven’t made the Protocol Participation Cycle pretty or refined yet, but here is a very rough draft of it to give you the idea.
![Protocol Participation Cycle - June 14.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/0eb5b1a7-de81-43f2-be2f-0265e3f7f489/Protocol_Participation_Cycle_-_June_14.png)
Artifact Ideas
I have two possibilities for artifacts currently.
1) An aesthetically pleasing version (map? cycle?) of the Protocol Participation Cycle.
2) A framework to help people “see” (become consciously aware) of the protocol systems they are participating in. This would likely take the form of a series of reflection questions and exercises. 
### ***Ideas of Ways to Engage***
-Help refine the PPC. What makes sense? What doesn’t? How would you change it? Is it a useful tool?
-Make an artistic or software-based form of the PPC.
-Create a mascot for the Participant in a Thousand Protocol Systems, or a representation of them in their myriad protocol systems.
-Interrogate the Protocol Participation Cycle (PPC) or the various concepts that appear in it (e.g., Bundle of Protocols, Protocol Identity, Protocol Identity Dysphoria, Involuntary Protocol Participation, Protocol Determinism, Protocol Overhang). 
-Explore how the different parts of the PPC interact with each other.
-Explore legitimacy, ethical, or stability questions raised by the PPC.
-What types of protocol systems could the PPC apply do? What types does it not work for and why? 
-Tell one of your protocol stories, or collect the protocol stories of others (podcast? videos? other?). Does the PPC help with telling the story?
-Apply the PPC to a protocol system (e.g., education, citizenship, fandom).
-How does the PPC relate to theories or frameworks in a particular discipline (e.g., psychology, systems theory, anthropology, sociology, law, religious studies)?
-Build a framework to help people determine where they are in the cycle for a given protocol they are in.
-What other frameworks does the PPC resemble? How can the PPC be improved by these? Does the PPC add anything new? (e.g., Joseph Campbell’s Hero’s Journey? John Boyd’s OODA-Loop?)
-How could the PPC help people in the real world? How could it be publicized or polished to make it useful? How could it go viral?
-Whatever ideas you have! The sky’s the limit!

RAFT NOTE: This is an early draft. It’s missing citations, footnotes, and references which will all be added shortly. Please let me know what parts work, what don’t, and whatever else I’m missing. 
-DL


Standards Make the World


“Now there is a popular fallacy about this business of setting standards. It is the belief that it is inherently a dull business. One of the reasons that I am glad to see the present history appear is that I believe it will help to dissipate this misunderstanding. Properly conceived the setting of standards can be, not only a challenging task, but an exciting one.” 

- Vannevar Bush, 1966
MEASURES FOR PROGRESS: A HISTORY OF THE NATIONAL BUREAU OF STANDARDS


Everything in the built environment is touched and shaped by standards. These nearly invisible rules establish trust between engineers and give rise to commerce, industry, and possibilities. Even now, just by reading these words, you are relying on dozens, if not hundreds, of guiding technical standards. Some of them might be familiar, like the World Wide Web (WWW) or the Internet Protocol (IP) that deliver packets of information to your device. What about the standards that went into manufacturing it, like the allowable Radio Frequency Interference (RFI) and Electromagnetic Interference (EMI) limits for that device? What about the shipping and transportation standards that brought it across oceans? Would you know where to find the spec? What about the group that created them? Or who maintains them?


The rabbit hole of questioning extends to most every object in our lives. Technical standards form the foundation of our modern world. They're often mistaken as limits or boundaries to creativity, which can happen when they're poorly implemented. But if they're well designed by engineers on the front lines, standards can become enabling technologies: the internet, shipping containers, time.


Techno-competition—startups, companies, and new products—get all the attention, but techno-cooperation—standards and protocols—drives an equal measure of civilizational progress.  


Despite their importance, standards get little attention or appreciation. Most people, if they're aware of them at all, think they’re boring and overly bureaucratic. This is partially due to the word itself—standard. It sounds basic and is broad enough to cause constant confusion. A standard could refer to anything from the Unicode system that approves new emojis to the bacteria levels allowed in pasteurized milk. The name is just a small part of the problem. Another issue is first impressions.


For many, the first and only exposure they have to standards is as a cursory hurdle. They encounter them as a constraint on the way to some other goal. For example, a product designer runs into several safety and interoperability standards through the course of making a new product. An architect is bound by building codes in designing a new home. Even managers are guided by standards—ISO 9000—when they try to add quality assurance measures to company processes. Then, when they peek behind the curtain—when they start asking why that standard is the way it is—they find a committee or a consortium or some other process that seems impenetrable. 

Understandably, this is where most people stop thinking about standards. They adhere to their basic legal obligations and they move on.


This is unfortunate. A deeper understanding of standards-making—and how that process has evolved over time—creates a healthy respect for the scale of influence. Standards are some of the most powerful tools we have to affect our world. And here’s the kicker: you can make them. 


I tried it. And the experience changed what I thought possible with technology. I played a small role on a team that made a new ocean connectivity standard.[1] The process was surprising in many ways, perhaps mostly in how it provided us with an entirely new canvas for creative invention.

Standards are not divine rights. They are made and remade by (usually small) groups of people and projected into the world through various means and with varying effectiveness. And that process is dynamic. Standards are something that anyone can engage in, even though almost no one thinks to do it. But they should. You should. Too often, better societal outcomes —overcoming technological bottlenecks or ensuring new tools are safely deployed—are held back through poorly-designed or missing standards. 

Shedding light on the gritty process of standards-making is a promising first step, but more could be done. There are new philosophies and approaches emerging about how better to support and sustain this important work: tending and mending the technological commons. 


Down the Rabbit Hole


Kids don’t grow up with dreams of becoming standards makers. I certainly didn’t. Standards were an unplanned pursuit. 

My journey[a] started more than a decade ago when my friend Eric Stackpole and I were building an underwater remotely operated vehicle (ROV) in his garage. With little money and big dreams of a low-cost device, we shared the project designs online in the hopes we’d get help from others. We called it OpenROV–an open source underwater robot design. Our design goals were a 100m depth capable robot for less than $1,000 (the average market price when we started was closer to $20,000). We were modeling our effort on other open hardware projects that we had seen, like the RepRap 3D printer or the Arduino microcontroller, who themselves were inspired by open source software projects like Linux. We thought sharing the design would allow us to crowdsource ideas for an ever-improving design. 


The project struggled and succeeded in ways we didn’t intend. The openness helped us meet new people and gather a few contributions, but not nearly as many as we’d hoped. And compared to the speed of open source software development, the iteration cycles were painfully slow. Moving physical atoms compared to digital bits proved logistically more complex.[b]
 
Despite the challenges, OpenROV was effective in changing the economics of micro-ROVs. Thousands of people bought and built our initial OpenROV kits. Dozens of spinout projects and companies started from those designs, and you can now find several capable ROVs that meet our initial specs. Small ROVs got good and cheap, just as we’d hoped and not at all how we’d planned. It seemed almost an accidental outcome. 


The experience left me with more questions than answers. Open source hardware had been a well-intentioned mirage, but there were lessons worth salvaging. As OpenROV wound down as a project and evolved as a company, I began trying to piece together what worked. I found answers in a missing history lesson. 


A Short History of Standards


In 2019, researchers JoAnne Yates and Craig Murphy published Engineering Rules: Global Standard Setting since 1880 where they tell the long story of standards-making. They write alm[c]ost spiritually about the role and mission of standards entrepreneurs (or "standardizers" as they call them), the unsung heroes who convene and build consensus amongst engineers and organizations. They suggest viewing the process as “an entirely different realm with a very different logic from either commerce or politics, something that developed in response to the greater social complexity that accompanied the pressure toward the greater economic integration of industrial capitalism.[d]”


Neither state nor market, but essential to both. [e]


According to Yates and Murphy, standards are good for three main purposes: safety, interoperability, and performance. They use the history of the early standardization efforts of steam boilers (safety), screw threads (interoperability), and steel rails (performance) as examples of each. Exploding steam boilers on riverboats caused engineers and policy makers to rally around standards for design, construction and maintenance. Screw threads were a matter of obvious convenience; standard designs would make lost screws easily replaceable by local shops. Steel manufacturers needed a way to quantify their durability and justify higher prices over iron rails, but they didn’t have an easy way to convince buyers. Performance standards—industry-wide assurances of quality—helped facilitate commerce. [f]
[g]


While the purposes are different enough to justify different names, they’re still all lumped into “standards”. The commingling of terms hides their utility. 


The diffuse definition begs the first question in standards-making: What's the goal? Safety, interoperability, or performance? 
[h][i]


The categorization of standards is relevant for many major challenges facing society. Take the debate and discussion around AI regulation and alignment, which is fundmentally a question about missing safety standards. Holden Karnofsky and Open Philanthropy recently put up a bounty to source more case studies of effective safety standards in the hopes of gathering insight. Or take the growing demand for carbon offsets, which has shown the need for effective performance standards. The carbon market simply cannot operate without a trusted mechanism for verification. The recent exposure of fraudulent carbon credit reporting has rattled confidence and left the entire industry grasping for how to move forward.

Gleaning the right lessons is predicated on a good taxonomy and a new vocabulary.


We should have started here with OpenROV. We could have saved ourselves years of work. It wasn’t until much later that we came to the right conclusion: ocean technology was being hindered by the lack of interoperability standards. It was Eric’s insight: “Everybody wants custom shit.”[j]


No matter what we built, our customers and partners always needed some other sensor that we hadn’t accounted for. This was a costly proposition, as every new configuration would require software, electrical and mechanical engineering reviews to ensure compatibility. There was no plug-and-play connectivity between sensors (dissolved oxygen, conductivity, pressure, etc) and platforms (buoy, robots, ships). A custom need translates to custom (and expensive) budgets[k]. Modular components and a simple, universal interface would be a better situation, but the industry hadn’t settled on any one design. The Universal Serial Bus (USB) and the other terrestrial analogs stand no chance against the corrosive ocean environments and the underwater connectors that did exist were too valuable for any of the private manufacturers to consider sharing. [l]


Standards problems often manifest in this form, not as a tragedy of the commons, but as a failure of the commons to materialize in the first place.[m][n] The economic forces placed on individual actors create just enough friction to leave the problem unsolved, even though addressing the common challenge would benefit everyone. Technology standardization, in the words of Stephen Walli, is the art of “commercial diplomacy” and very few companies have trained or appointed diplomats. 

Identifying the type of standards problem is only the first step. Then what? Again, a history lesson helps.
The How of Standards-Making


Standards end up as outcomes—agreed-upon specs and rules—but standards-making always involves a process, too. Yates and Murphy separate the major standards movements into three distinct waves, each with unique operating modes.


The first wave occurred between the 1880s and 1920s[o]. The standards-making process was mostly straightforward: groups of interested parties made the case for a shared design and convinced others to join. Working groups of engineers, interested business leaders, and relevant government officials formed to oversee their adoption. Organization followed function.


The gradual adoption of Greenwich Mean Time (GMT) is a good example, one that impacted the railroads and beyond.[2] It started as a maritime tool. British mariners kept one chronometer at Greenwich time in order to keep track of their longitudinal position. The British railways adopted GMT in the 1840s to keep the trains running on time, and other uses in Britain quickly followed. Following the British lead, engineers and scientists representing American and Canadian railways met in 1883 to create a standard railway time across the continent along with established time zones based on GMT. The adoption and implementation by the railways set the stage for the International Meridian Conference in 1884, organized at the behest of President Chester Arthur, which formally established Greenwich as the prime meridian and set the standard for global, universal time. Pragmatism ruled the day. 


After the successful standardization work of the late 1800s, the idea coalesced into a recognizable social movement by the 1920s. Standards entrepreneurs like Charles Le Maestre, head of the British Engineering Standards Association, gained status and influence through their commitment to the common project. However, this was not usually a profession or job in itself. Rather, it was “largely made up of men who volunteered to work on technical committees because that was something they believed professional engineers should do to serve the public good.”


They had adopted an ethic and a commitment to advancing progress through cooperative design, and were busy proselytizing the benefits to the engineering masses.


The second wave of standards entrepreneurs went much bigger. The world wars had slowed momentum. As countries focused inward, they became wary of cooperation. But the standards pioneers had left the blueprint: network effects ruled. Le Maestre and others had shown the value of strong national standards bodies, and provided a glimpse of how international standards could go even further. The second wave started in the 1950s and thrived from the 1960s-1980s. Globalization was the story, and associations like the International Organization for Standardization (ISO) were formed to facilitate trans-national adoption.


The momentum of ISO was buoyed by new, global industries like air travel and freight shipping, as well as a convergence of political interests. The defining standard of the era—intermodal containers—literally connected the world. Developing countries saw standards adoption as the easiest path to international markets, whereas more powerful developed countries, like the U.S, recognized the importance of standards-setting as a way to sustain trade advantages. Most countries, from Japan to Sweden, fell in between, adopting the standards that made sense from a cost saving or political perspective. Standards had gone global, with all the associated network effects and layers of bureaucracy.


The third wave of standards entrepreneurs grew alongside the development of computers and computer networking. As engineers and companies quickly innovated, the need for standards outpaced the capacity of the international, multi-stakeholder consensus process, and a new model emerged in response. Yates and Murphy called this a “consortia-based” model, where the parties most closely involved in development (companies, engineers, etc.) self-organized to create standards that worked for their purposes. The process of formalizing these standards happened later. I call this process “disruptive standards-making” because of the close resemblance to Clayton Christianson’s model of disruptive innovation–unassuming outsiders breaking through with ever-improving technology. 


The Internet Protocol was the pioneering third wave standard. DARPA didn’t set out to create a new type of standards-making process. The ARPANET program was created to solve a practical problem in the cheapest way possible. Vint Cerf, a key member of the ARPANET program and Network Working Group team, understood the basic motivations of the agency. From an interview with the Charles Babbage Institute in 1990:

The INTERNET, which was spawned out of this conglomeration of different packet technologies that DARPA initiated, has already had a pretty dramatic impact on both the military and the commercial world as far as I can tell. You can't pick up a trade press article anymore without discovering that somebody is doing something with TCP/IP, almost in spite of the fact that there has been this major effort to develop international standards through the international standards organization, the OSI protocol, which eventually will get there. It's just that they are taking a lot of time. 


Once again, like the original standards-makers, the organizational efforts follow an idea that is already working in practice. The recent news that Ford and GM are adopting Tesla’s electric vehicle charging standard is a good example. Tesla raced ahead to fill the need, and now even its most ardent competitors are falling in line. Disruptive standards-making falls somewhere on the spectrum between de facto and voluntary consensus—elements of both strategies mixed with heavy doses of entrepreneurial hubris. [p]


This brings up the second important question in standards-making: Who’s it for? And what's the minimum viable buy-in needed to get a network of users started?


The internet kicked off the next great era of standards-making. The World Wide Web would follow, starting with a small team of outsiders at CERN who got the damn thing working. This method of disruptive standards-making has caught on quickly amongst software developers. In fact, it has evolved into an entirely new idea: open source software (OSS). 


By almost every metric imaginable, OSS is thriving. In 2022, GitHub released a report on the staggering influence. More than 90% of companies are relying on OSS in some way. More than 90M developers are on the GitHub platform and they collectively made more than 400M contributions to OSS projects in 2022 alone. And it’s big business, too. Roughly a third of Fortune 100 companies have dedicated open source program offices. 


But more than any statistic, just ask any software developer if and how they use OSS. You’ll likely get an effusive endorsement of both the philosophy and a list of favorite projects. 


The rise of OSS represents a full-on standards movement. Engineers build reputations through their contributions to the commons in the same way that the early standards-entrepreneurs would gain status for their efforts. It's prestigious work that raises the tide for all boats. [q]
Standards in the Real World


Unfortunately, and despite trying, the standards movement happening in software has hardly moved off the screen. There have been valiant attempts to create an open hardware movement—and we were an active part of that—but it’s remained niche. [r]The thinking went: take what's working about open source software, like version control and transparency, and do the same for electronics and mechanical designs. There was an entire batch of open hardware companies that tried to build businesses through open sharing. Arduino is perhaps the most famous example. They led a revolution in microcontrollers by inviting amateur electrical engineers to prototype every conceivable idea. However, the business side of Arduino was a struggle. Most open hardware companies didn’t succeed. Or when they did, the openness of their hardware played a minimal role. Makerbot, the 3D printing company which grew out of the RepRap open source project, was the most public example of having to hand back their open source bonafides. It turned out that supply chains and non-recurring engineering costs didn't match the ideals or expectations of open source. At any reasonable scale, and with anything beyond hobbyist tools, the math and operational realities couldn't square. 


Like with OSS, the best measure of impact is a simple survey of an engineer. Ask any mechanical or electrical engineer how open source hardware has impacted their work and you’re likely to get a blank stare. It simply hasn’t caught on. 

The open hardware practitioners have kept the discussion alive for the past decade, through waves of attention and momentum. Progress has been made, especially in addressing the licensing challenges of open source hardware, but there’s still room for improvement. The opportunity to create a modern standards movement in the realm of atoms — as opposed to just bits — remains ripe for interpretation and invention. And there are good examples of enabling standards in the non-digital realm. However, the projects that succeeded didn't go through any open hardware playbook, but rather by stumbling onto the disruptive standards-making process.


The Cubesat design[s] was created by a pair of professors at Cal Poly and Stanford, Jordi Puig-Suari and Bob Twiggs, who were interested in helping their students get their experiments into space. Discouraged by the high price of launch, the two conceived of a simple, standardized satellite design — 10 cm by 10cm by 10cm — that could work with a common mechanism, the P-Pod launcher, to piggyback on commercial launches. The simple design turned out to be transformative. NASA and commercial providers agreed to carry the small payloads. It started, as Puig-Suari and Twiggs hoped, as a wonderful platform for student experimentation, but it wasn't long before entrepreneurial minds realized they could take advantage of the spec. Within a few years, commercial prototypes were flying, and the Cubesat standard had become the foundation of a new generation of companies, like Planet Labs, that were building large constellations of tiny satellites. The design was a critical and underrated component in the current space technology renaissance.


The MIDI connector is another example.[t] The emergence of new, digital musical instruments in the 1980s created a need for an interoperability standard. One of the early makers of synthesizers, Dave Smith, the founder of Sequential Circuits, sought to fill the gap. In 1981, at the AES conference, Smith presented a paper titled “Universal Synthesizer Interface” as a proposal to make sure these new instruments could play nice together. After trying and failing to gather consensus amongst all the largest synthesizer makers — Roland, Korg, Yamaha — he found one willing partner: Ikutaru Kakehashi, the founder of the Roland Corporation. 


“We sat down, just a small group of us, and we just said: let’s just do it. Forget everybody else, nobody else is interested in it, let’s just do it.”


The two worked together over the next year to develop the spec. At the following year’s show, they demoed their new synthesizers: Roland’s JP6 and Sequentials’ Prophet 6000. The rest of the market quickly fell in line. It wasn’t perfect. There were bugs and missing features that drew complaints and demands for fixes. 


“Sure, if we had gone through a standards committee and if we had spent 5 years developing MIDI, none of those things would have happened, so we kind of let that sort of thing — all the little details get fixed in the marketplace.”


The initial rollout storms eventually passed and MIDI became the standard. More importantly, even though they didn’t know it, Smith and Kakehashi had proven the new model of disruptive standards-making. It doesn’t require everyone’s buy-in up front — just enough people to get something working, even barely. And if it works, it works. 


Start with a small group of true believers and gradually work outwards.
Rethinking Ocean Connectivity


When we got a second chance at creating a ocean technology standard, we skipped the open hardware rhetoric and modeled ourselves on the CubeSat and MIDI examples. And, more importantly, this time we had a bigger team. 


OpenROV had merged to create Sofar Ocean Technologies and the company was planning for a new, modular technology architecture that would allow us to make flexible additions and changes to our smart buoy and mooring system. Want to add a new dissolved oxygen sensor? No problem. Need temperature sensors at various depths throughout the water column? Easy. The new design would allow us to quickly adapt the configuration to suit customer needs. We had identified the right problem and were on a path to solving it for ourselves. That’s also when we—notably Evan Shapiro (Sofar’s CTO), Tim Janssen (Sofar’s CEO), Eric and me—started thinking bigger. 

Our team began having informal discussions about turning our internal plans outward—moving from an internal connectivity scheme to trying to influence an industry-wide shift. 

It was a lofty goal, and one that made very little sense for a startup like us to pursue. But we all agreed that, like the CubeSat and MIDI teams before us, we had rightly identified the bottleneck that was holding everyone back: connectors. Eric had developed a new type of underwater connector design using a screw mechanism, face-seal o-rings, and separate wire terminals. We had pressure tested the design down to full ocean depth, and it was simple enough that it could be manufactured for just a few dollars. Creating a standard was barely possible. If we were going to attempt it, we needed help. 


Also, one small-but-important aspect of our project needed addressing before we were ready to take it out into the world: the name. Eric had been calling the connector design “One Plug” throughout the early development process, but it was painfully inadequate. It was bland and even worse: there were two plugs! The main aspect of the design required all the power and data to happen over two wires and connection points. One Plug had to go. And after a prolonged naming exercise, Evan broke the stalemate by suggesting the common name of the most numerous fish in the world: Bristlemouth. We all laughed and agreed. 


The next challenge wouldn’t be as straightforward: we needed partners. 
How Standardization Efforts Fail


Carl F. Cargill made a career as a standards theorist and consultant. He has written multiple books and several journal articles about the art and science of standards-making. Scholarly but not overly academic, Cargill’s perspective was born from direct experience, as he spent his career as a practicing standards strategist at a variety of Silicon Valley companies like Sun, Adobe, and Netscape. 


One of his most important contributions to the field was a paper that inverted the basic premise. [u]Instead of asking what factors made a standard succeed, he approached the question from the opposite: Why Standardization Efforts Fail.


The paper articulated a harsh reality: the main ways that standards efforts fail are not in the standards-making process itself. The conceptualizing, writing, and implementing is only half—or probably less—of the story. There are two other failure modes that were more likely to contribute to missing standards. The first is on the front end: a failure to launch. Any number of problems can stall a project at this pre-conceptualization stage: a lack of enthusiasm from potential sponsors, the originator lacking charisma and vision, or even a fear of anti-trust and anti-competition accusations from market players. There’s also the real possibility of opposition. In the early stages of the process, someone or some organization might feel that they have something to lose from the creation of a standard, and it becomes their mission to stall or derail the momentum, which is often not hard to do. Standards can be fragile ideas in the earliest stages. 

The second big failure mode is on the back end. Once the standard is complete, it must eventually find healthy adoption and implementation in the market in order to sustain and grow. Startups know they must find a product/market fit in order for their companies to be successful. Standardizers, too, must find market fit in order to survive. There are any number of ways to miss the market: being too late[v], incompatible implementations, or just plain ignorance. Cargill also highlights the perverse outcome of the standard being used to manipulate and manage the market. While broad adoption can construe this outcome as a success, the network effects of standards can sometimes be a sort of rent seeking from predatory organizations.


The danger zone for standards is the starting and the sticking. And, if you get those two right, the sustaining becomes the next challenge. [w][x]


Viewed through this perspective, the booming OSS movement becomes explainable with basic economic logic. The cost of both starting a project and finding the right initial adopters has fallen dramatically. Thanks to enabling platforms like Github, there is no friction. Starting and sharing OSS projects has become so easy that the discussion has turned almost entirely to the challenge of how to sustain these projects and maintainers. 

As a startup company proposing a new hardware standard, the economic friction—the costs of the added time needed to develop and support the standard, as well as the costs of the actual, physical components—was all too acute. There were two ways of dealing with the cost. The first was spreading it out over a long period of time and being frugal, just as the CubeSat project had done. But there were no guarantees that this would match the timeline of the company—the unique assemblage of talent needed to develop it—or the market need. The alternative path was to seek philanthropic support to help make the economics work.


Surveying the options for funding this type of project yielded few results and, thankfully, a rapidly growing effort to change that reality.


So far, the best philanthropic model for encouraging new disruptive standards has been an idealistic notion of paying it forward.[y] An example is Willow Garage started by Scott Hassan. Hassan recognized the critical role that open source software played in his business success. By building off the stack of open tools, he was able to quickly get his software company, eGroups, off the ground and serving customers without having to build everything from scratch. He theorized that robotics needed a similar boost — contributions at the core operating system layer that would make building applications easier for everyone.  He decided to apply a portion of his startup winnings to the idea. Willow Garage ended up developing and making core contributions to ROS, the Robot Operating System, which is now installed and running on more than half of the new commercial robots that are sold every year. It’s another software standard, but getting closer to the physical world.


Mark Shuttleworth, founder of Canonical which publishes and provides support for the Linux-based Ubuntu software, also recognized the important role that open source software played in his business success, too. He set up the Shuttleworth Foundation to push the next wave of open innovation. The foundation underwrites fellow[z]ships for people and projects that are applying the principles of open source software to new arenas. They've funded people building tools for open biology and open industrial machinery to varying degrees of effectiveness. After twenty years of paying it forward, the Shuttleworth Foundation is now winding down, but it left an interesting mark.


Now traditional philanthropy is giving way to a host of radical new ideas and strategies for funding public goods.


Over the past decade, blockchains and cryptoeconomics have shaken up the standards-making[aa] world. Since the Bitcoin whitepaper was published in 2008 — introducing a vision for peer-to-peer electronic cash as well as the concept of a blockchain — there has been a surge of protocol development, with many tied to a token that starts and sustains the project. By turning protocols into stores of value, crypto is underwriting an array of useful new technical standards. 


If nothing else, crypto has already produced a historical anomaly worth noting: the standardizers got rich and famous! Several have created modest fortunes as standards entrepreneurs, which is almost never the case. The creators usually garner acclaim from technical crowds while follow-on entrepreneurs (or a generation of entrepreneurs) get rich in the newly enabled frontiers. Crypto has those follow-on entrepreneurs, too, but it’s unique in the way it rewards the initiators. 


The crypto set has reshaped the discussion about how best to fund public goods through their tools, but also through their philosophy and philanthropy. The Funding the Commons event series, hosted by Protocol Labs, has been a nexus point for these ideas and experiments. New mechanisms like quadratic funding and retroactive public goods funding are being tested in the Web3 world, and there are early efforts to scale and translate these mechanisms to other arenas. 

Amidst the noise of exchange crashes and meme coins, it’s easy to lose sight of the long story of techno-cooperation. The soul of crypto is an idealistic promise: a new and better way to fund and sustain the technological commons, including enabling standards. There are open questions about how far cryptoeconomics can go. Can it move beyond the screen in a way the open source movement hasn’t? Will it rebuild the web or create new network states? We are amidst this grand experiment. [ab][ac]


The third question in standards-making might be: what tool will you use to start and sustain the effort? How will you underwrite the work?


We decided to try and make our own path. 
[ad]


Our team, led by Tim, hatched a plan to give the Bristlemouth standard the best possible chance of success. We brought together a group of potential partners—ocean philanthropists and senior officials from government research agencies—and pitched them on supporting the development. Like everyone who works in the industry, these groups were keenly aware of the interoperability problem.


Our pitch was both technical and economical. We relied heavily on the CubeSat analogy and borrowed loosely from the emerging economic discipline of “market shaping”. We proposed a combination of two mechanisms: a patent buyout and an advance market commitment (AMC). Both ideas were proposed by the Nobel Prize-winning economist Michael Kremer decades ago. AMCs provide a meaningful-sized purchase order predicated on some capability or price performance, with the goal of moving market players into unoccupied territories. They have caught on in public health, and were made famous by the Gates Foundation and GAVI’s $1.5B purchase order for pneumococcal vaccines that have saved an estimated 700,000 lives. The concept of AMCs has gained momentum in philanthropic circles, having been used to accelerate the deployment of COVID-19 vaccines as well as kickstart the nascent market for negative emissions technologies. Patent buyouts are just like they sound: a government or public-spirited entity buys out a patent from a private company in order to spur innovation in the broader sector. The mechanism is less well known and still missing a notable success story. 


Our proposal had two phases. First, underwrite the time and development needed to get the technology ready for outsiders: designing for ease of use, field testing, and documentation. It’s not quite what Kremer described in his original patent buyout paper, which he envisioned as a tool to remove monopolistic positions, but it’s close enough to warrant acknowledgement. In our scenario, it simply covers the realistic costs of preparing a product like this for outside consumption—making it easy enough for any engineer to pick up and use. Call it a Patent Buyout Lite. 


For the second phase, once we had met acceptable development milestones, we asked our partners to pre-buy a set number of kits—development boards, connectors, and waterproof housings—that could be sent out to students, startups, and researchers who wanted to start building with Bristlemouth. We’re trying to follow the CubeSat path to market adoption by focusing on new entrants. They are the group most in need of the solution and the groups with the least invested in legacy systems. Of course, we’re warmly welcoming established ocean technology companies to use the standard, but we’re spending more time recruiting the upstarts. The goal with the development kits was to remove any barriers to potential adoption from both old and new players. And, again, not exactly the use that Kremer envisioned for his AMC model, but close enough to cite for inspiration.


We’re here now. The first Bristlemouth development kits are on their way out the door. And the ultimate destiny of the new standard remains to be built. 


One reality is already certain: it will take time. For hardware-related standards, the uptake can take much longer than market-driven products. Potential adopters look for signs of a solid foundation before they’re willing to jump in. Is the spec changing? Do the economics make sense? Does it work?

Jordi Puag-Suari, co-inventor of the CubeSat standard, reflected on the decade-long journey to seeing true adoption: 
When we went back and looked at launch activity, It took about ten years for things to really take off. There were a few launches before that but it wasn’t instantaneous. People needed to gain some comfort that these standards are going to be around for a while. [ae]They are going to exist when I finish my satellite in a few years and then they started building.
Standardizers must have patience baked into their model, as normal economic cycles might not match. But the lengthy process shouldn’t hold us back from telling these stories and documenting these projects, even when they’re just making their way into the world. 

An Open Invitation[af]


Robin Sloan, writer and media inventor, recently spent time reflecting on the question: “What do you want from the internet, anyway?”


His answers sent him down a path of uncovering the early discussions of the Internet Engineering Task Force (IETF) and culminated with him deciding to build out his own idea for a protocol, which he’s calling Spring ’83. It’s a stripped down version of the behemoth we have today. It’s an idea, Sloan hopes, that will inspire other creators and interesting people to follow and learn from each other, without all the nonsense. He made a point to comment on the satisfying nature of the work: 


Before I go further, I want to say: I recommend this kind of project, this flavor of puzzle, to anyone who feels tangled up by the present state of the internet. Protocol design is a form of investigation and critique. Even if what I describe below goes absolutely nowhere, I’m very glad to have done this thinking and writing. I found it challenging and energizing.


Sloan tapped into the thrill of standardizing. [ag][ah][ai]We touched it, too. There are no promises of riches or even success, but there is a sort of personal power that comes from shaping the tool that shapes the tools. It’s rarefied air — a Highlander-like competition taking place above the realm of startups and FAANGs that quietly determines the direction of technology. [aj][ak][al]


Like Sloan, I recommend this challenge[am]. Encouraging awareness and understanding of the importance of standards is a meager first step. More experimentation is needed to spark new disruptive standards or, even better, inspire a full-on standards movement. There are vast new frontiers to be reached through better techno-cooperation. 






________________


On Credit
[an]
For the Bristlemouth project, I deserve almost none. I was bit player at every stage, documenting the progress and doing research when needed. I was basically the court stenographer. 


For the technical aspects of the project, Evan Shapiro should get most of the credit for vision and leadership. But also Charles Cross and Alvaro Prieto for the architecture contributions. And also Eric Stackpole for the early design and later testing. Or Zack Johnson. Or any number of Sofar engineers who shouldered leadership for the project at various points. From my vantage point, this is truly a team effort. 


On the management side, Tim Janssen should get the credit for organizing all the parties and rallying them around a common vision. Tim would be quick to acknowledge the crucial role of Oceankind and ONR and DARPA in making this real. And he’d be right. 

And there’s a section portion of credit that should be withheld and given to the early adopters who come next. Joining and building on a project like this is an underrated form of leadership. 

That’s a funny thing about helping a standard come to life: it requires this exact formula of credit distribution. It’s fully-ddecentralized heroism. Everyone has to believe. 

So despite my peripheral role, I absolutely felt fully invested. If this essay seems overly biographic, blame it on my enthusiasm. But absolutely give any positive credit to everyone else. 


________________
[1] See On Credit section at the end of the essay.
[2] The best book on the story is Greenwich Time and the Longitude.
[a]2 total reactions
Tim Beiko reacted with ➕ at 2023-07-24 19:09 PM
Timber Schroff reacted with ➕ at 2023-07-24 11:14 AM
[b]Is there an example you can use that's more standards related instead of atoms vs. bits, given that could also be true in non-open-standards projects
[c]To avoid the book-review vibe here, perhaps lead this history section with a contemporary example, such as the EU forcing Apple to respect standards on phone chargers, to motivate where that kind of tension came from. Then the book discussion feels like a smooth segue.
[d]Reminded me of part of Nadia's essay:


"Protocological thinking only appeared with the onset of industrialization, or what James Beniger called “The Control Revolution”: a need to regain control after a sudden explosion of information
Ex. the use of standardized forms, time zones"
[e]So good
[f]While this works great as an example in this essay, it feels like an obvious place to try and "backlink" to Timber's piece in a broader SoP publication/output/etc
[g]Standardization converts uncertainty to risk. Exchanging uncertainty for risk enables better quantification of expected returns to a particular project, which is an input to both insurability (history of steam boiler insurance --> nuclear design is fascinating!), and capitalization at impersonal scale. Better quantification of returns attracts capital due to reducing variance in expected returns, and facilitation of comparison across competing uses of capital. 


Standards are intimately bound up in the processes of insuring and financing our large scale collective endeavours, and therefore unsurprisingly insurers and investors foment and influence standards adoption in deep ways.
[h]More thriving commerce is also in fact a goal I think. It’s what drives more private-sector driven initiatives and certain government moves like the creation of the interstate commerce commission… not exactly a standards move, but a kind of strong protocol move.
[i]Standards reduce transaction costs which can increase and scope and depth of a given market. If facilitating voluntary transactions is the task a market is supposed to perform, one could shoehorn this functional benefit into "performance"
[j]/xkcd-standards-meme :-)
[k]This might be a good place to re-emphasize the bits vs. atoms distinction - adding a web API is much easier than creating a new type of sensor
[l]runon sentence btw, probably useful to break it up
[m]I like this a lot.
[n]Great point, well said. Had a similar thought that I'll probably develop in my own essay: protocols have 2 failure modes, positive (protocol failure creates a new problem) and negative (commons doesn't materialize or desired transactions/coordination just don't happen)
[o]This work was symptomatic of a larger movement around ideas related to international cooperation and progress. Cf. work of Paul Otlet: https://en.wikipedia.org/wiki/Paul_Otlet
[p]1 total reaction
Tim Beiko reacted with ➕ at 2023-07-24 19:18 PM
[q]I feel like standards and OSS are highly, but not entirely, correlated: "web1" was heavily standards based, even though most of the products weren't OS. OTOH, if you look at things like node or python packages, you _wish_ there was more of an effort towards standardization, even if most of it is OS.
[r]reminds how different furniture companies are rarely interoperable at all. IKEA, with internal standards, often doesn't use the same stuff you find at a hardware store.
[s]1 total reaction
Tim Beiko reacted with ➕ at 2023-07-24 19:21 PM
[t]Same comment as above, for readers, what is a MIDI probably needs to be defined in laymans terms prior to addressing the casestudy
[u]Feels like you haven’t established this as the basic premise before this point yet. Perhaps a very brief discussion of what makes your successful examples successful before you dive into the failure frame?
[v]or being too early :D
[w]visual here would be cool
[x]I feel this entire essay is very ripe for added visual components! cc: all-affiliates :-)
[y]ETH community has been pushing for models of retroactive public goods grants. Might be applicable here
[z]You’re mentioning a lot of examples in passing, and I think maybe a tabulation of their highlight features might help? What’s the effort, is there a foundation, did they launch, some measure (even if subjective judgment by you) on success/failure…
[aa]Not sure if crypto has shaken standards **making**. Definitely funding, and OSS software in general, but ETH and BTC's standardizations processes are literally copied from Python :-)
[ab]This section feels a little awkwardly bolted on, like an afterthought. Perhaps delve into one example issue in the blockchain world, like say block-size debates, and how they’re resolved (hard forks etc) and comment on the unique features of the techno-environment a little? Right now it’s all valuation commentary, no details. I’d shoot for 50-50 or drop the section. More generally, forking seems like a topic to touch on in both the OSS and crypto sections. The ease of forking in software, and the typically lower cost of not having a universal standard, was missing in, for eg rail gauge standards. Some software domains like networking suffer a lot from non-standardization, but in others, promiscuous forking arguably creates a healthier ecosystem if there’s no good reason for strong standardization.
[ac]+1. I'd say the strongest standards-aligned example would be ENS, which actually did create a useful standard (human readable crypto addresses), which interop with other standards (DNS, Bitcoin, etc.), and where the token can (eventually) claim a share of the economic activity (selling ENS names) + has been used to fund public goods.
[ad]You’re sort of doing an interleaving of your story and commentary kinda unconsciously. Consider making that structure explicit and clean. I’ve often used the strategy of sections separated by *** or -1– where odd-numbered sections are personal narrative, and even-numbered ones are impersonal commentary/analysis. Works quite nicely when you feel strongly about a subject, but also need to make objective points about it.
[ae]This could be a whole section in and of itself :-)
[af]i think i want to see this whole section take up more room in the whole essay? this feels like the call to action to everyone to play with making your own protocols and shaping the standards that are already around you
[ag]2 total reactions
Nadia Asparouhova reacted with 👍 at 2023-07-06 09:54 AM
Tim Beiko reacted with 👍 at 2023-07-24 19:30 PM
[ah]+1, this paragraph is invigorating!
[ai]Thank you all for the feedback. It's very interesting. Every reviewer has made this exact suggestion, which has me paying close attention.


I will say, though, part of me wants to keep it at the end. Not because I'm attached to any of my writing, but because I think there's something important to the exclamation mark being at the end. 


The actual feeling of satisfaction from standardizing is only available after you pass through a very specific portal: oblivious –> aware -> bored ->  intrigued -> engaged -> enthralled. 


I want the essay to be engaging but I also like the idea of it being true to the experience.
[aj]3 total reactions
Nadia Asparouhova reacted with 👍 at 2023-07-06 09:54 AM
Aaron Z. Lewis reacted with 👍 at 2023-07-18 12:16 PM
Tim Beiko reacted with 👍 at 2023-07-24 19:30 PM
[ak]++ (esp if we go magazine route)
[al]Yes. If standards are "boring," invoking your story makes it lively and interesting. I'd make it even more front-and-center throughout this piece!
[am]more than recommend - you _took it on_, which can be a good way to re-frame the piece as per others' comments.
[an]Just put the text of this section in footnote 1 instead of using footnote 1 to point to it. No issue with long footnotes/endnotes.

Outlining V2 [Semi-Public]


Quick Reference
* Research Agenda

   * There are existing institutions in society (right kids?) that already have power
   * There are also subcultures that form, die, and reform without necessarily gaining power, but which sometimes make their own attempts to make it to the institutionally entrenched level
   * New technological media layers cause new information behaviors that do not correspond to the credit cultures of existing institutions. this is causing epistemological crisis
   * Shifts that result in new institutions coming into power often result from significant epistemological crisis (NEED EVIDENCE)
   * So: as we look at new types of institutions that are evolving in tandem with recent technologies, what can we observe about the “credit cultures” that are forming? How is power accruing to different “knowledge communities” in similar or different ways?
   * Research Questions

      * Why has the internet failed to engender communities of scientific and intellectual inquiry that rival legacy knowledge institutions?
      * What additional context (besides knowledge) needs to exist in a knowledge community for a credit culture to form?
      * What infrastructure needs to be in place for them to flourish?
      * What sorts of institutions can be built on different technological substrates?
      * How do networks of institutions with strong credit cultures give rise to an epistemology?
      * What is the relationship between these communities and institutions and their individual members’ identities?
      * Definition of Credit Culture

 Credit Culture: a ****set of protocols that define a community of knowledge over space and time.

         * Certifying and legitimating knowledge
         * Assessing progress
         * Preserving and maintaining knowledge practices
         * Assigning reputation, codifying provenance and propriety


The Evolution of Credit Cultures
Outlining V2 + Drafting
Emoji Legend
📝 = section that needs further prose development.
🧩 = piece of the argument. current length is a result of ordering the argument, not necessarily how long the section needs to be.
🐍 = needs evidence, do not trust! prioritize getting evidence for these points above other tasks.
Brown text = tentative points that are unfinished, includes commentary.
________________


📝 Introduction
         * The internet has yet to produce communities of scientific and intellectual inquiry that rival the success of legacy knowledge institutions
         * What has it managed to produce? Movements, blogs, swarms — new types of organization or “social form.”
         * These new social forms exhibit very interesting information behaviors. They manage to organize around new types of memory—memes and lore—which display a suitedness to the properties of the internet
         * Kei quote
         * But these new information behaviors on new media layers do not interface well with older institutions
         * In general, the forms of legitimacy endogenous to these media formats are quite different
         * Some things never change, and there are parts of legitimacy structure that remain the same: that which is most reproduced has the most longevity
         * But the way the work is produced is very different
         * What is the most “legitimate” or trustable online material you can find? Is it a blog? A tweet? A youtube essay? A substack?
         * Link rot and link drift, Dorian argues, are some reasons for the spurious aura around internet media
         * But it’s more than that. It’s also the conversational nature of the way online media is produced
         * It’s possible for an existing institution of repute to produce knowledge and upload it to the internet. But this is, largely, a one-way street. It’s hard for information produced online to “compile” back to something recognized in a legacy knowledge context.
         * More and more collaboration is happening online, but a bigger issue is socialization and cultural development as a whole. Online is moving faster and taking precedence over prior institutional forms.
         * There is increasing conflict and pressure for reconciliation between these two knowledge environments, which are, properly understood, different epistemic environments
         * For existing knowledge institutions have practices and traditions that make them a “complete” epistemic environment. In contrast, institutions on the internet do not.
         * Through this inquiry, we identify what we call a “credit culture”: a set of protocols that define a community of knowledge over space and time
         * What can we observe about the nascent “credit cultures” that are forming on the internet? How is power accruing to different knowledge communities in similar or different ways? What could make internet knowledge communities more successful?
Part 1
         * The institutional repository of the world that internet migrants were born into: law, academia, science, medicine, business, trades.
         * 🧩 The importance of institutions
         * institutions form the bridge between past, present, and future
         * institutions constrain and supply certain forms of human behavior [North]
         * communities of practice know how to get stuff done and train people to do it [Wenger & Lave]
         * Give some examples of both civilization-critical and life-enriching legacy institutions throughout
         * 🧩 Defining scope: which institutions do we care about? Knowledge communities
         * knowledge communities generate questions and answers that embed wisdom and drive progress
         * These communities are responsible for shaping the world of thought and intellectual inquiry via philosophy, and share extensions into the world of practice
         * Discuss the relationship of knowledge communities to communities of practice. E.g. theoretical psychology and clinical practice
         * We aren’t interested in communities of pure practice which persist through the reproduction of tradition. In short we care about communities which persist over space and time through using written or recorded media.
         * Discuss formal properties of the types of knowledge communities we’re interested in
         * 🐍 Maybe something about how written / print media has shaped knowledge communities over time, and or shaped today’s institutions. This could start with Plato’s Phaedrus, or could be the link to some of the “bureaucracy” references [von Mises, Weber, Castells, Graeber]. This part sets up the introduction of “credit culture.”
         * 🧩 The idea of “credit cultures”
         * We are proposing a name for the protocols that define a community of knowledge over space and time and allow for its continuity: a “credit culture.”
         * 📝 Unpacking and justifying use of the term “credit.”
         * Philologically, the “cred–” root is at the heart of many of the salient features of knowledge communities that persist: credentials, credibility, crediting, credence
         * Explanation of the components of credit cultures. We will dig into each in turn in a later section.
         * Certifying and legitimating knowledge
         * Assessing progress
         * Preserving and maintaining knowledge practices
         * Assigning reputation, codifying provenance and propriety
         * Why think of these as “protocols?”
         * Protocols are a sort of formal rule system that are distinct from traditions, laws, and norms. They aren’t laws because they mostly aren’t enforced by the state (although the requirements of professional associations may apply); copyright and patents are a rare case historically speaking. They aren’t norms or traditions because they don’t live entirely within the social realm; they’re more sociotechnical infrastructure.
         * Pivot
         * 🧩 Why focus on credit cultures?
         * successful institutions are definitionally those which obtain resources and achieve entrenched privileges. the most successful are able to rewrite themselves - legally and behaviorally - into society’s definition. Eg: In the way that it is hard for us to imagine a world without the banking and credit money system.
         * Just as credit cultures form a part of sustained institutions, institutions that entrench themselves entrench credit cultures as valid ways of producing knowledge. An entrenched institutional ecology = an entrenched epistemology
         * 🐍 shifts that involve new institutions coming into power often ((result from, produce, or coincide with?)) significant epistemological crisis
         * describing the current moment
         * new technological media layers are causing new information behaviors that do not correspond to the credit cultures of existing institutions. this itself is causing epistemological crisis and the rise of uncorrelated institutions
         * At the same time, we see a big shift in the material conditions for intellectual work. This point requires further development. Historically this has produced dynamic periods of intellectual creativity, at least in philosophical fields [collins]
         * However, this change is also challenging some of the core components that made prior credit cultures viable.
         * Therefore, we think it’s important to take a deep look at the composition of a credit culture. What makes specific knowledge communities work over decades? And what kind of institutional ecology does that lead to? What can we observe about knowledge communities that are forming on the internet?
         * So we will look at new types of institutions that are evolving in tandem with recent technologies, what can we observe about the “credit cultures” that are forming? How is power accruing to different “knowledge communities” in similar or different ways?
Part 2 - Comparing old & new
         * Let’s look at some already-institutionalized (legacy) knowledge communities

         * 🧩 LKIs as communities of practice [wenger & lave]. We start with the CofP framework because it introduces the contextual matter of knowledge institutions as essential to the social practice and the reproduction of the institution

         * DEPRECATED ANGLE 🧩 What does this context consist of? An institutional description of university knowledge work with examples woven in. The entire right column of the research questions chart goes here. In order of most local to most abstract:

            * the local specificity of the collaborative “work site” [garfinkel]
            * shared types of questions, shared types of tools, shared language worlds [kuhn, Williams]
            * regular in-person meetings and scientific cabals [collins, price]
            * shared social practice and constant learning process toward identity of mastery [wenger & lave]
            * discrete boundaries between individuals and organizations [ong, stanley]
            * funding mechanisms and shared objectives which strengthen these “lanes” [stanley]
            * 🧩 ****************ALT: the protocols of successful intellectual communities.

               * Certifying and legitimating knowledge
               * Kuhn: shared paradigms
               * Garfinkel: the local specificity of the collaborative work site / shop floor
               * Ong, Stanley, Turner: discrete boundaries between organizations & fields
               * Assessing progress
               * Lewis Ross: philosophical communities “converge” on answers.
               * Collins: dialectical synthesis and growth of abstraction
               * Kuhn: Shared types of questions and shared methods for answering them. Validating a proposed paradigm with increased levels of specificity
               * Bloom: Poets swerve from their influences when dealing with the same subject matter, then fully integrating influences as mature poets. unsure whether to include this but worth putting in
               * Preserving and maintaining knowledge practices
               * Wenger & Lave: shared social practice and constant learning process toward identity of mastery
               * Collins: influential scholars come from lineages of influential scholars
               * Assigning reputation, codifying provenance and propriety
               * De Solla Price: the role of papers
               * Collins: importance of publishing
               * 🧩 Epistemology and the construction of (knowledge) fields

                  * 🐍 How does—or did—an epistemology arise from the legacy institutional ecology?
                  * 🐍 Something about the premier status of “scientific” knowledge
                  * rise of the royal society and then what replaced the royal society?
                  * science and industry?
                  * 📝 Return to the notion of “credit cultures” and show how the evidence supports our definition

                  * 🧩 What institutions are “native” to the internet?

                     * New, different types of media—social media, forums, algorithms, and networked publics [boyd, kelty]
                     * New, different types of organizing logic—peer production, fandoms, and swarms. [benkler, jenkins etc, rafa]
                     * Here we can introduce a few of the IKCs as examples that we’ll go through in more detail later
                     * EA as an example of an IKC that effectively maintains knowledge and a shared canon to its benefit
                     * Briefly cite a few fandom related examples
                     * [reference] “Points of Contact Between Activism, Populism, and Fandom on Social Media” for #freebrittney
                     * [reference] “Exploring The Dynamic Between Online Social Infrastructure and Online Community of Practice in Social Media Fandom” for general discussion of how fans develop communities across online social infrastructure
                     * Internet-native institutions assume their shape by embracing the most obvious formal properties of the web: speedy distribution, replication, scalability, user-generated content
                     * brainstorming
                     * there wasn’t toooooo much that was useful from the “how do intellectual communities progress” paper but I thought it was interesting they discussed discrepancy between theories of progress that prioritize novelty vs theories of progress that prioritize convergence [passage below]
                     * “Next, one difference in emphasis between debates about scientific and philosophical progress is the importance accorded to novelty and convergence. Philosophy of science shows a preoccupation with discovery. Conversely, regarding philosophy, the focus is on consensus on the correct views. A comprehensive theory of progress will do justice to the importance of both novelty and of convergence. And finally, progress seems to be a gradable notion—intellectual communities can make partial progress (and regress) relative to some intellectual end. More precisely, progress is gradable both with respect to the scope of novel contributions and to convergence. Some novel discoveries contribute more than others, and an intellectual community can converge on the truth to greater or lesser extents. Each of these thoughts should be accounted for in the framework we use to think about progress.”
                     * Made me wonder about internet-specific determinations of what information is important (emphasis on spreadable and repetition over lineage progression and novelty) and if that leads to emphasis on convergence as part of what “progress” looks like
                     * 📝 Brief comparison again to the “legacy” institutions.
                     * these differences can be only partially accounted for by technological “structuring” or determinism [winner, turner, i forget who my favorite references on this are]

                     * 🧩 Brief discussion of technological vs social determinism. what is our perspective on this?

                     * 🧩 Information behavior of knowledge communities

                        * LKIs deal with knowledge in a qualitatively different way than internet knowledge communities (IKCs)
                        * LKIs: enshrined and certified knowledge, synthesized and published.
                        * IKCs: diffuse knowledge that people attach themselves to in brief affiliational ways, ongoing and in-process. Work and ideas are published and given a false sense of finality, creating a sense that things are “always-in-progress”
                        * “Languaging” into existence and creative misprision (Striphas, boyd, warner, Williams)
                        * Infrastructure: what kind of media support the work in distributed contexts?
                        * Collapsed knowledge contexts: the opposite of a functioning community of practice [boyd, meyrowitz]
                        * Identity: no milestones, no identities of mastery, issues of “blocked access” (wenger & lave)
                        * 🧩 What sort of institutions can be built? Other types of knowledge communities and why they only comprise some ingredients

                           * 📝 Swarms [rafa], Open source [kelty], Fandoms, Wikis, Algorithmic similarity clusters, “Big accounts”.
                           * Alt: what is the design space for a new credit culture?
                           * What are the ingredients of a credit culture within this new space (especially ones that differ)
                           * The role of lifestyle philosophies [collins], and what else is close to evolving?
                           * Open areas for new research to go on
                           * Some examples of organizations that are trying to build proper institutional KCs today

                              * DeSci, Progress Studies
                              * Despite successes in peer production institutions, IKCs have not succeeded in creating robust credit cultures, with one or two notable exceptions. We will explore why and how that might be corrected

                                 * when we say robust do we mean their outputs can compete for legitimacy w those of legacy institutions
Case Studies
                                 * Rationalism and the rise of effective altruism
                                 * Counter case study: accelerationism
                                 * I currently think we should do the case studies first so we have better examples to work through when we discuss IKCs properly. But not entirely convinced
                                 * might be cool to talk to more web 1.0 communities about communities that didn’t survive
                                 * there’s probably some chartmaking we can do depending how many we collect
📝 Conclusion
                                 * Toward an internet-native credit culture, and random prognostication and commentary

                                 * We want new credit cultures to emerge. It would be good if this were to happen. However, the new credit culture is undergoing a break with previous knowledge cultures, because print media publications and archiving are being superseded by different forms of media

                                 * The current historical moment is an impasse because the epistemology of the older credit culture is being actively challenged, but the new infrastructure has yet to develop ways of certifying knowledge. In the absence of this, only the instrumental reasoning of tech companies seems ‘valid’

                                 * There are attempts to take over older credit cultures and their attached institutions - but not yet formal knowledge validating systems of their own

                                    * they can also attempt backwards-compatibility with legacy institutions to leverage legacy influence
                                    * Building this may at first seem like endeavoring to build “useless” knowledge - in the way that “science” was originally seen as a gentlemanly activity performed by civilized men, not a profession to itself. It may seem useless to begin with. But it’s necessary for the other benefits it may later provide

                                    * AI is not a solution* this is a footnote

                                       * LLMs have the power to consolidate … they basically produce things that feel “legitimate” through breadth of inputs but is really no better a mishmash than a human could produce
                                       * only statistically meaningful not semantically meaningful ***
                                       * chatgpt can’t use epistemic status tag that rationalists came up w
                                       * the way the gpt UIs are designed make it seem like the knowledge is static or reproducible
                                       * mcluhan argument we like - there are faster forms of media

                                       * epistemological crisis being the imperative for change

                                       * baconian science - bacon invented current scientific method. has to be based on in the world evidences vs other types of scientists wanting to base it on logical inference

                                          * this was a shift that had nothing to do with the shift in media technology
                                          * Evan… protocol labs efforts, they’re an interesting player because they’re implicitly interested in building a new credit culture on new media rails (desci culture)

                                             * eth foundation same level as protocol labs
                                             * desci question….. we’re talking about “science” but science is different from other types of intellectual inquiry. it operates pretty differently. social sciences bears more resemblance to philosophy than to science. We shouldn’t necessarily exclude science as a theme from this work, even though the requirements for validating scientific knowledge are more strenuous than other types of intellectual production
                                             * (also code)
                                             * the desci question could be an interesting case study. its the most direct attempt to do what we’re talkimbout
                                             * see researchhub drama
                                             * noodling on… ethereum foundation, the funders of this program - what they’re hoping to get out of it. we’re not really answering their questions of “how do we steward the protocol and understand what it’s for” we’re most suited to comment on the project and the ethereum foundation’s role

                                                * what do we want to communicate to them? what is their strategic position as funders of an intellectual culture through ppl getting rich on crypto
                                                * how should they be thinking about fostering intellectual work that contributes to their overall goals

Top Line Comments from Feedback:
* Framework “makes sense”
* Very insightful on the protocol encoding and relationship to different structures (encoding of social protocols changes the structure of the org
* Overall, the piece is broader than swarms, although swarms is the primary case-study. Need to think about reframing, maybe a different title / intro ---- or need to go from swarm to broader framework instead of framework to swarm
* Need to restructure writing since right now it’s progressively unfolding and the reader “doesn’t know what we’re going to talk about” (the flow isn’t intuitive, even if the scope is tighter than the first version)


  



~^~~^~~^~


Welcome to the Swarm[a]


A text message, a like, or a post on social media may appear insignificant on its own. However, these actions generate transformative waves as they converge and compound in our networked world. We are in the swarm[b], a social formation born in an algorithmic ecology. And as a result? We’re racing to create technologies to herd this collective spirit. Its power will be harnessed.


~^~~^~~^~


The sky turned lilac before the hurricane’s landfall; an omen of Juracán, spirits of the wind summoned by the indigenous deity Guabancex[c]. With some luck, the hurricane would have passed above or below the Island. Puerto Rico was not lucky in 2017. Hurricane María ripped straight through the Island with winds of over 200km per hour.[1] Thousands died in the aftermath. Only Juracán knows the final count.[2]


María’s landfall was at 6:15 am local time on September 22nd, 2017. It was mythical: landfall happened one minute after sunrise. The Puertorican diaspora living around the world waited in anticipation.[3] Many watched their social feeds for posts from family and friends. But the feed was silent. There was a total communication blackout. Rumors of the damage crawled from phone to phone. Jorge, a Puerto Rican living in Berlin and working in climate technologies, imagined the challenges locals might face in the coming days and weeks. He tapped into his previous social activism experience and sought an opportunity to contribute.


Jorge noticed in his Facebook feed that Puerto Ricans living in the US were beginning to share drop-off points for mutual-aid supplies. It seemed the diaspora’s anxiety was being channeled into support. A collective direction began to emerge: get supplies to those in need. There was no banner or organizing institution, just posts about where to go and visual memes with lists of useful supplies. Jorge’s feed became a cacophony of activity. How could he help?


By the end of the day in Berlin, while Hurricane María was still ravaging the Island, Jorge had begun a public spreadsheet[4] aggregating drop-off points found on Facebook. He shared the link with various mutual-aid groups in the hopes of accelerating donations.[5] Immediately, Pablo Benson, a social activist and Puerto Rican, saw Jorge’s spreadsheet. Pablo forwarded the link to the spreadsheet to others in his network. A message flew from New York to Berlin via Facebook chat:  “We’re creating a team and we want to use your spreadsheet.”


Jorge and Pablo had met briefly over a decade ago. After a quick exchange on Whatsapp, Pablo invited Jorge to join daily coordination meetings. In the following days, the spreadsheet grew to include over a hundred drop-off locations around the US. A civilian-led supply chain of aid emerged. Memes reinforced demand; lists of what to donate filled social media feeds.[d][e] Online and offline, simple memes were transforming spectators to volunteers.


Later on, collaborations such as this one would have compounding consequences.[f] The Puerto Rican network would support the development of physical Centers for Mutual Support (CAM in Spanish) on the Island,[6] join the 2019 protests to oust the governor, and become core to the campaign for a new political party[g][h].


A new type of networked social formation, a digital swarm, formed from the diaspora and spread beyond its origins to provide aid in response to the deadly tempest.[i]
“The light of a candle
Is transferred to another candle—
Spring twilight.” 
Yosa Buson
________________


~^~~^~~^~
(in) Formations


Groups of people, such as crowds and companies, have been extensively studied and theorized.[7][j],[8][k] Their power is well known; they shape history. Crowds demand war, address civil rights, and deliver workplace safety. Companies[l][m][n][o][p] deliver groundbreaking innovations like our global network of communication[q].[9] These social formations achieve feats far beyond the capabilities of any individual, demonstrating what is referred to as collective intelligence.[10]


Each type of social formation is shaped by the social protocols[r] underlying its identity and collaboration mechanics. Formal organizations rely on social protocols which are explicit.[s] The protocols they adhere to are often encoded and strict, such as a chain-of-command or Git version control. A Company could be defined as a formal socia[t][u]l form [v][w]built upon a portfolio of explicit social protocols. They have a brand, adhere to regulations and coordinate production through management[x].[11] 
[y]


In contrast, informal social groups are defined by implicit social protocols, relying instead on[z] norms and behavioral contagion for coordination. A Crowd is an example of an informal social form, most often physical. It has common emotions shaped by peer-to-peer mimicry and aligned through ad-hoc event broadcasts.[12] Crowds and their kin are characterized by the absence of explicit social protocols; they are a world of vibes.
[aa]


What social formation best describes Jorge and Pablo’s experience? It’s clear they were not part of a company. They were online working together, but it wasn’t an open-source project with version control either. Some might argue that the term “social movement” is more accurate. Similar to the emergent phase of social movements, the online civilian response to Hurricane María had no holistic cooperation structure or banner under which to organize[ab][ac]. Were they part of a crowd? Some might say so[ad][ae]. However, there were two main differences: coordination was mediated by an algorithm, and there wasn’t any visible gathering.[af][ag][ah][ai]


If the response to Hurricane María wasn’t a crowd, company, or social movement, what was it?[aj]


The response included the diaspora but also people exposed to diffuse call-to-actions across multiple social media feeds.[ak] Participants had no consistent home or third space for gathering, physical or digital. Moreso, the response had no collective name, brand, central knowledge base, or paid software[al]. Small teams, like Jorge and Pablo’s[am], were acting independently yet in broad alignment; decoupled and attuned.[an]
[ao][ap]


It is now clear that the response was networked.[13] Collaboration patterns existed within and between team hubs, with broader bursts of alignment. Each team adopted its own coordination process, and teams connected with others at key handoff points. People were connected, not gathered. As we will see, Jorge and Pablo’s experience hints at the new social formations we should expect this century.[aq]


The response to Hurricane María was, in some ways, what danah boyd described in 2010 as a [ar][as][at][au]“networked public… restructured by networked technologies… simultaneously a space and a collection of people.” The social formation was “transformed by networked media, its properties, and its potential.”[14] However, once we look at the broader landscape of group entities emerging online, the limits of boyd's definition begin to emerge.


First, new categories of social formations must account for human(ish) and non-human agents. While true that Jorge and Pablo’s network was a “collection of people,” similar formations include autonomous and semi-autonomous participants like bots. People gathered in group messaging services, like Discord or Telegram, often include automated bots that actively participate in moderation and content creation. Now, with chatbots powered by artificial intelligence services such as OpenAI’s GPT4, the distinction between our digital avatars and bots is even more opaque. Bots, such as ones for moderation and conversational support, are already becoming proactive participants in our networks[av][aw][ax].


Second, social formations are networks of actions and content. The term “public” [ay]suggests that the unit of participation is the agent itself. But online agents, humans, and bots alike, publish content that exists independently of themselves. Memes seem to take on a life of their own. For example, someone might post a comment about an experience at a restaurant. This, in turn, is aggregated automatically by social media algorithms and social media listening tools. The post is now part of a trend. As an observer we can ask: is the post, or the original poster[15] part of the online formation? They would be coupled if authorship was reserved, but what if it's not?[az]


Posts are routinely downloaded, cropped, and remixed[ba]. Authorship is lost while the content travels across social media sites and personal blogs. Today, the broadcast action can be decoupled from the creator. Consequently, we should reframe social formations as action[bb][bc]-centric instead of people-centric.


Online formations have a diversity of agents, humans and bots, co-creating a multidimensional network of content actions. It is a new social formation, unique from a networked public.[bd]


By accounting for a plurality of agents such as humans and bots, and looking at a multi-dimensional network of actions, we can then define the new social form: an online formation[be][bf]. Unlike a networked public, the nodes of the networks are made of content actions, and the participants extend beyond human-only entities.


An online formation is a network of actions.
Actions are performed by networked agents, humans or otherwise.[16]


The definition has two direct consequences. Firstly, people interacting online become a type of online formation themselves. We will call this an avatar, a term used by game designers and online researchers today.


An avatar is a set of networked actions attributed to an agent.[17] 


[bg]
Secondly, broadcasted actions attributed to an agent are part of multiple online formations, depending on the observer. The relationships are relative and shaped by algorithmic aggregators–whether we intended them to be or not.


  

Image: Online formations are defined by the observer. Algorithms group together our broadcasted actions and present them to others. Each person perceives online formations differently based on the actions they encounter.
________________
~^~~^~~^~
The shapes of our networked spirits


There is ample evidence of online formations with hundreds and thousands of avatar participants. The ubiquitous scale is impressive. Millions likely spark in and out of existence every year. If we look carefully, we can get a sense of their magnitude, archetypes, and lifecycles.


At Reddit there are >60k new subreddits created every month[bh].[18] Facebook Groups likely dwarf this volume, with over 10x the number of users as Reddit. Fandoms of super-celebrities like BTS, Taylor Swift, and OneDirection count with millions of participants each.[19] Disinformation campaigns are reported during every election campaign.[20] Crypto airdrop farms have thousands of subscribers.[21]


Those spending too many hours on social media, the terminally online, have even developed specific language to speak about these formations, hinting at various archetypes. Terms like raids, farms, online communities, and fandoms may sound familiar.[22] And yet, many online formations go unnamed. If the response to Hurricane María was an online formation, can we define an archetype for it? If so, we could understand the source of its social power.[bi][bj]


[ADD VISUAL: Word cloud of “very online terms” surrounding online formations]


Echoes of these novel terms are found in the news and contemporary academic research journals. Online formations have been responsible for shaping political discourse, enacting protests around the globe, and serving as a source of credibility for misinformation campaigns. Below are a few news clips and journal entries that talk about some online formations with well-known names: Black Lives Matter, #MeToo, Occupy, QANON, and Postrats (short for post-rationalists).


BLM




https://www.brookings.edu/articles/how-george-floyd-changed-the-online-conversation-around-black-lives-matter/
	#MeToo




The Guardian: https://www.theguardian.com/world/2017/oct/20/women-worldwide-use-hashtag-metoo-against-sexual-harassment
	Occupy WallStreet


Academic Article: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3667169/
	QANON




Misinformation Campaign Academic Article (Check with Renée?)
	Postrats




NYT: This C.S. Lewis Novel Helps Explain the Weirdness of 2023
	Image: Collage of news and journal articles mentioning online formations


These formations are part of a category that Peter Limberg, online culture researcher, calls “memetic tribes.”[23] Per Limberg’s research, called “Meme to Vibe: A Philosophical Report,”[24] these online formations are defined by their “living philosophies” which have a “distinct vibe, with a definite sense of ingroup.” In other words, the actions associated with these formations fall under a single identity.


As Limberg points out, a tribe is primarily defined by its cultural lore and assets.[25] Following this, formal social protocols like managers, are either absent or ignored. Operationally, tribe participation is fluid and fragmented. Those who participate define their own way to contribute. [bk]External infrastructure, such as platform algorithms and media cycles address coordination and distribution.


[bl][bm]
Similar to these, but with a stronger presence of encoded social protocols are online communities. These include professional networks, fandoms, and hobby groups. Celebrity fandoms are common and studied deeply. Celebrity fandoms include Taylor Swift’s ‘Swifties’ and K-pop superfan group called the ‘BTS Army.’ Fantasy world fandoms include communities like ‘Potterheads,’ which are fans of Harry Potter.


Communities have known influencers like the memetic tribes, but also have structured knowledge hubs, gathering spaces, and tighter scopes related to content creation. Potterheads, for example, create and gather content on specific websites about the Harry Potter universe. In many cases, communities have partially or fully documented jargon, vision statements, and codes of conduct. They also have channels of mobilization. It is not a coincidence that One Direction, a British boy band, holds the record for most Billboard awards in the Top Duo/Group category. Fans coordinated online to mass-purchase their music and stream their songs 24/7 for weeks.


In addition, the knowledge hubs, influencers, and content canon–for example, Taylor Swift’s music, performances, and personal life events–allow communities to adopt persistent identities, sometimes outlasting the celebrity’s life. One Direction superfans, called Directioners, are still expanding their knowledge base and attending community-organized events, even though One Direction disbanded in 2015.[26]


In recent years we’ve seen new categories of online formations become mainstream: Blockchain organizations known as “DAOs” (decentralized autonomous organizations). These might be considered a natural progression from communities, leveraging decentralized financial tools and code known as smart contracts. Unlike memetic tribes, “DAOs” and their descendants adopt uniform code-enforced social protocols. For example, they may have internal reputation scoring, fund distribution rules, and democratic governance models.


Table: Virtual Organization Examples


Virtual Organization
	Gitcoin
	Bored Ape Yacht Collective (BAYC)
	MakerDAO
	Friends with Benefits (FWB)
	Organization Description
	Protocol for funding public goods.[27]
	Luxury brand community, similar to Supreme.
	Decentralized finance organization managing >$4.5B in assets.
	Online social community with over 3,000 members.
	Participation Boundary Protocol
	Proof of Personhood and Governance Tokens
	Digital Membership Pass (NFT) and Governance Tokens
	Digital Governance Tokens
	Digital Membership Tokens and Written Application
	Governance Protocol
	Quadratic Governance
	Dictatorship 
(Parent Company: YugaLabs)
	Democratic Governance[28]
	Committee Governance[29]
	

As a result of the software-encoded social protocols, these virtual organizations can delineate participation boundaries[bn][bo]. For example, membership and voting power are based on digital token ownership, allowing the organization to create automated grant programs. Gitcoin, for example, uses quadratic voting[30] to distribute funding for digital public goods. In other cases, virtual organizations use governance as an entertainment process and rely on explicit creator dictatorships for direction.
[bp]


Besides blockchain-based online formations, we’ve also seen the formalization of digital farming activities. These include gold farming[31], social influence farming,[32] and crypto-airdrop farms.[33] All are online formations where the participants perform specific tasks, a playbook, in exchange for digital currency or the opportunity to receive them. Unlike memetic tribes and blockchain organizations, these don’t have a brand but have ongoing instructions on what tasks to complete. They are virtual farms, often surviving in grey legal areas.[34]


But the experience of Jorge and Pablo doesn’t align with either of these archetypes: tribes, communities, virtual organizations, or farms.[bq][br][bs] They were similar to memetic tribes, but only inasmuch that participants have a large amount of agency in terms of how to participate. Unlike memetic tribes, Jorge and Pablo didn’t have “a vibe” or cultural assets such as niche symbols or influencers. Instead, they were more like gold-farming telegram groups, without an overarching collective identity[bt]. The combination of task agency and lack of identity hints at a distinct archetype, unlike the previous ones we’ve discussed. Jorge and Pablo were inside a fluid and fragmented organization–a minimally protocolized entity. We will call this a swarm[bu][bv].[bw][bx]


Together, we can plot these five archetypes–memetic tribes, communities, virtual organizations, farms, and swarms–along two primary axes: identity persistency and collaboration structure. 


Framework: Online Formation[by] Archetypes
  

Image: 2x2 framework used to understand online formation archetypes. There are two axes: identity persistency and collaboration sources (internal vs. external).


In this framework, identity refers to the ability to apply a name to the formation[bz]. Persistent identities have a name that is agreed upon by internal participants and external observers alike. As the label implies, it is persistent across perspectives and over a significant time period (months, years, decades). Shifting identities refer to the opposite. It is an inability to apply a single name. This may be either due to conflicting internal factions or rapid identity evolutions.[35]


The second axis is collaboration structure. This refers to how the online formation aligns the actions which compose them. In high-agency models, like those of memetic tribes and swarms, alignment relies on external algorithms and informal peer-to-peer agreements. It is based on pattern recognition and constrained by the ecosystem in which it lives. Here, the spread of behavior requires confirmation from multiple sources before individuals are willing to act themselves.[36] Actions are mimicked if they go viral or are approved by peers. For example, Jorge’s spreadsheet didn’t have an approval process, but others sought to mimic the success of the spreadsheet in helping others donate materials.[ca]


Table: Online Formation Archetypes, Characteristics, and Examples
Category
	(1) Memetic Tribe
	(2) Online Community
	(3) Virtual Organization
	(4) Farm
	(5) Swarm
	Identity
	Persistent
	Persistent
	Persistent
	Fluid
	Fluid
	Collaboration
Structure
	High Agency, w/ Algorithmic Alignment
	Mixed (Some Agency, Some Process)
	Internally Defined Processes
	Internally Defined Processes
	High Agency, w/ Algorithmic Alignment
	Key Example
	Meme-based groups, like QANON
	Fandoms: Swifties, Potterheads
	Blockchain Orgs: Gitcoin, FWB, MakerDAO
	World of Warcraft Gold Farms
	Hurricane María Response
	

In contrast, online formations with uniform and well-defined collaboration operations rely on central documentation, managerial roles, and other internal processes[cb][cc]. Their collaboration is constrained by internal preference. For example, blockchain organizations create onboarding workflows and internal reputation scoring systems. Some run a regular census and have a membership database. Similarly, influence farms have lists of specific actions which manipulate social algorithms to their advantage without being penalized.








[PENDING PARAGRAPH DISCUSSION RANKING RE: PROTOCOL ENCODING]








[PENDING PARAGRAPH OF CONCLUSION / TRANSITION: Focus on Swarm]










Visual: Online[cd][ce] Formations[cf] sorted based on social protocol encoding
  

Image: Online formation archetypes can be sorted, roughly, based on the reliance on implicit and explicit social protocols for cooperation. Swarms are minimally protocolized; they are reliant on implicit social protocols. In contrast, virtual orgs have their social protocols encoded into their code infrastructure.


________________


~^~~^~~^~
Swarm Power[cg][ch]


Since the inception of their collaboration, Jorge and Pablo used what they had learned from previous online experiences. Pablo started daily coordination calls, mimicking successful behaviors that he had participated in during the aftermath of Hurricane Sandy in 2012.[37] Jorge, in the meantime, created a vetting process to ensure that drop-off points gathered in the spreadsheet were active and up-to-date: “We had to make sure the information was accurate.” It’s easy to get distorted information that’s been passed between one too many people. To ensure high-quality information, Jorge made direct calls to the drop-off organizers and received calls from supply donors. In parallel, the Facebook feed would be filled with images showing lists of items to donate, and items to avoid. These visuals, a common tactic in crowdfunding supplies, would be further shared on other social media channels: WhatsApp, Instagram, and Twitter.


  
  

Image: Visuals of donation memes in English and Spanish. On the right, we can see the Facebook group name included in the banner: “Boricuas en la Diáspora: Alivio Post-María” (English Translation: Puerto Ricans in the Diaspora: Post-María Relief).




The combination of public feeds, daily calls, and meme-style visuals led to the creation of a civilian-based supply chain of donations. This is a clear demonstration of power. Hundreds of volunteers across the US had successfully done what the local government couldn’t: mobilize aid supplies immediately[ci][cj]. They had neither budget, approvals, consultants, nor corporate managers. In the beginning, supplies were gathered together at the drop-off locations and shipped via mail. As the scale of donations increased, volunteers prepared them in pallets and sent them via plane. Weeks later, to ensure the supplies were not being confiscated or hoarded by government officials, the supplies were accompanied by volunteers who hand-delivered them to various locations around Puerto Rico.


The achievement of the swarm is even more impressive when we account for internal factions[ck]. Dozens of groups emerged, complementing hundreds of Facebook groups that already existed where Puertoricans in the diaspora gathered to exchange job opportunities, sell goods, and ask for advice. Factions followed custom processes based on their priorities, resources, and experience. Jorge and Pablo’s group used daily townhall-style video calls and focused on delivering supplies without governmental support. Others, like Francisco (radio host on the Island) gathered information from phone calls and shared it with governmental emergency response organizations like the Federal Emergency Management Agency, known as FEMA. Some factions converged over time. For example, multiple civilian-led supply-gathering initiatives ended up using aid centers set up by FEMA as distribution centers.[cl]  


Jorge went to Puerto Rico a few weeks after the initial response. What he found amazed him. He visited local aid centers to offer his help, but they were fully staffed. His help was no longer needed. Soon after that, the working group between Pablo and Jorge disbanded. But the digital relationships between them and the network of content they created would persist. Similar organizational tactics would be deployed in the following years. The power of online formations would lead to the resignation of Puerto Rico’s governor in the summer of 2019 and a new political party.


Image: 2019 Protests in Puerto Rico, credit: Isaak Gonzalo


~^~~^~~^~


"Go to the ant, you sluggard! 
Consider her ways and be wise, 
which having no captain, overseer, or ruler, 
provides her supplies in the summer, 
and gathers her food in the harvest" 
Proverbs 6:6-11, The Bible, King James Version


~^~~^~~^~


Social movements over the past century, such as the civil rights movements, have successfully channeled the power of the public to drive institutional changes. Researchers and theorists, such as Blume [ADD MORE REFERENCES HERE] have outlined how social movements manage momentum in two steps. First, they define objectives framed in contrast (or support) of targetted institutions[cm][cn]; they act on a political intention.[38] Then, they adopt a common organizational framework[co].[39]


However, the power of online formations is derived from the network, not just the size and organization of a movement.[cp][cq] To understand this source of power we can look at the online formation archetype of swarms, which do not have central managerial structures[40] or objectives. 


Swarms rely on external technologies, like social media algorithms, for the distribution of tactics and coordination. This is similar to guerrilla warfare, where coordinated cells use broadcast technologies–such as peer communication, radio, newspaper, and TV–to learn about successful strategies and tactics. In both cases–for swarms and guerrillas–achievements and blunders are reported publicly. In the case of the hurricane’s response, successful aid delivery stories were shared on Facebook and on Television. The ecosystem is the infrastructure for communication.


Swarms share another commonality with guerrillas. They gain focus through the adoption of an emergent narrative, a plausible promise, not collective objectives. Participant actions address an associated “enjeu” (stake). Jorge and Pablo sought to get supplies to those in need. Their actions addressed the issue of Hurricane María’s devastation. As we can see, there are no metrics or objectives associated with the promise, only a direction. In addition, the promise was not understood as a collective call-to-action, but one that they, as individuals, could act on.


In the words of John Robb, researcher of modern guerrilla warfare, “This promise is the central connection between all the members in the community. Each member can have specific motivations that are substantially different from any of the others. In the case of warfare, these alternative motivations can be patriotism, hatred of occupation, ethnic bigotry, religious fervor, tribal loyalty, or what have you. It doesn’t matter as long as they agree with the plausible promise.”[41]


These two characteristics of swarms–ecosystem as communication, and a shared promise– provide swarms with increased maneuverability and, as a result, power.[cr][cs] They become rapid shapeshifters, adapting quickly to whatever challenges they encounter. Like a crowd, their participants have common emotions. Like a guerrilla cell, their actions are aligned, but uncoupled.[ct][cu] Unlike social movements, there are no holistic organizational protocols; the algorithmic broadcast and peer-based gossip networks are enough. Unlike a crowd, the swarm is not gathered and, therefore, difficult to contain.[cv]


It is worthwhile to note that the frame[42] of their promise does not require an institutional backdrop. This increases the flexibility and breadth of scenarios a swarm may appear in. Swarms can emerge to address aid after a catastrophic weather event, like in the case of Hurrican María, but can also appear in far stranger scenarios.
[cw]


In 2022, a swarm emerged on TikTok. An unknown amount of participants, likely in the millions, started playing a decentralized game to collect imaginary coins called doubloons[cx]. Anyone could decide to participate in the game.[43] As social media users browsed short-form videos on the feed, they would encounter memes that gave them instructions to add or remove doubloons from their inventory. Participants kept track of their doubloons, as well as other game items, on their own personal notes and then created memes that others might encounter. The game lasted a few months, with various “rules” emerging to manage the supposed inflation problem.[44]


In summary, swarms can survive and thrive despite not having internal infrastructure, uniform social protocols, or a target “them.” In other words, they are minimally protocolized entities. Those who try to control the swarm end up herding shadows.


However, a swarm’s survival can be fragile[cy][cz]. Swarms are dependent on the two mechanisms that contribute to the alignment of participation: swarms delegate their communication reliability to the networked platforms they live in, and do not have central control of the promises they seek. [da]If either system is disrupted–communication or promise–the swarm will dissipate. On the other hand, enhancing communication or strengthening the image of the promise can lead to additional scale, and influence.[db]
  

Image: At the most basic level, online formations rely upon the infrastructure that provides their networked communication. Successfully signaling participation in the “Promise” (depicted by the various shapes) helps formations connect and act collaboratively.


________________


~^~~^~~^~
Alignment Technologies


There is a meme about the American Central Intelligence Agency (CIA) that surfaces on social media every few months. It outlines eight tactics for “General Interference with Organizations and Production.” For anyone who’s worked in a business setting, the meme often causes laughter; the tactics are experiences most employees encounter daily. For example, point number three states: “When possible, refer all matters to committees, ‘for further study and consideration.’ Attempt to make the committees as large as possible–never less than five.” Those working in corporate or enterprise companies likely remember how difficult it is to achieve anything with more than five people.  


The common theme across all the tactics is simple: slow down the decision-making process of the organization by extending and complicating communication. This prevents the organization from taking action and achieving its objectives. It prevents the entity from maneuvering in the face of market changes or internal conflict.[dc]


Image: Text meme focused on interfering in organizations. Shared by Martin Weigel, Head of Planning at  Wieden+Kennedy Amsterdam, tweeted the following image from “The CIA’s’ Simple Sabotage Field Manual’ (1944)


We can consider these tactics a type of bureaucratic technology that interferes with collective behavior[dd]. Other management technologies can mitigate the risks of interference; leadership and product management playbooks increase the production and responsiveness of an organization. What other technologies could there be besides these examples of communication sabotage and management? Under this umbrella of technologies for influence, we can also include content moderation, interface design, incentive programs, as well as status games. Together they can execute strategies of influence to align the social formation.[de][df][dg]


Alignment Technology: A technique or method to direct content creation, user behaviors, and network development.[45]


What alignment technologies are emerging to influence online formations? Large social media platforms point the way. They have been exploring alignment technologies rigorously to fight misinformation, violence, and illegal content[dh][di]. Although the results seem dubious.[dj]


Framework: Alignment Technologies in Social Media Today
  
[dk][dl]
Note: The framework includes common interventions in social media ecosystems. A social media ecosystem may be a platform, such as Facebook or Twitter, but it may also be a private space.


Governments, malicious state actors, and even online communities have, in turn, attempted the coopt the technologies to their advantage.


A few months before Hurricane María, another event took place across the Atlantic Ocean. In London, terrorists deliberately drove a truck into pedestrians on London Bridge, and then began stabbing people in the popular tourist area called Borough Market. Eventually, they were shot and killed by the police. Eight people were killed. The Islamic State claimed responsibility for the attack.


A lesser-known part of this event was the government’s social media and physical propaganda response to calm citizens and organize volunteers. The UK government has detailed plans to encourage the emergence of ‘spontaneous volunteers’ and help manage the public response.[46] This is an alignment technology as well. However, instead of relying on direct intervention, such as moderation or software policies, propaganda relies on attunement. Social media feeds would be flooded with specific messages of support and cross-cultural support. Like Jorge and Pablo’s memes about items to donate, the UK government used hashtags– #TurnToLove, #ForLondon and #LoveWillWin–to help a promise emerge and propagate across the captive audience.


Together, we can begin to form a shape of technologies used to manage online formations, some based on direction, and others based on attunement[dm].


Framework: Alignment Technologies of Attunement
  

 
Alignment technologies can stimulate the creation, direction, and destruction of online formations.


Other Section Notes:
* Online, the stewards of our digital spaces enact a new portfolio of design strategies. These can stimulate virality, dampen it, or flat-out destroy online formations like swarms. The strategies today include moderation, algorithmic changes, banning, and content adjustments like community notes. All in all, these are alignment technologies that ensure actions on social media do not interfere in revenue generation, such as advertisements and content sponsorships.
* It’s important to note that these technologies can be enacted from the outside of a formation, or within it. Online formations, such as communities, virtual organizations, and farms, assign moderators and install helpful bots. It’s common, for example, to go through proof-of-humanity gates like CAPTCHA tests.[47]
* Social media organizations have worked tirelessly to intervene with malicious actors, often acting in a swarm-like fashion. However, the interventions have been only partially effective. If we look at the two primary levers, we can understand how current technologies influence them, and where there may be opportunities for better-influencing technologies.
* First, swarms require a symbiotic relationship with external systems of engagement. Second, swarms depend on the alignment of actions towards a uniform plausible promise. The external system serves as a broadcast communication channel and the promise as a frame to interpret how to contribute. The combination of frame and broadcast allows any observer to attune to the swarm and contribute permissionlessly.
* A variety of technologies and software design choices have been made to avoid 
* In Jorge and Pablo’s case, the plausible promise was achieved,[48] and social media algorithms began prioritizing other content.[49]




~^~~^~~^~
In the Swarm


Over five years later, Jorge and Pablo are still browsing Facebook. They’re also on Twitter, Instagram, and TikTok. Like many of us, they are broadcasting their actions into the networked world and taking part in dozens of ephemeral online formations: swarms, communities, and virtual organizations. 


Beyond this, Jorge and Pablo represent a new generation of leaders. They are learning about managing within the swarm and creating inspiring promises to mobilize new networks. 


Careful attention to our environment points the way to innovative capabilities and a new generation of managerial toolkits to take on the global challenges we are encountering in this new long century.


Together, we are in the swarm.
________________
Appendix / Graveyard
List of opportunities for further research:
* Factions and their expression in different forms
* List of implicit and explicit social protocols
* Process of morphing between one formation archetype and another
* Moderation, and other technologies, as a part of protocols… or are they a means to preserve existing protocols?
* Examples of transition from IRL crowd into online formation, and vice-versa
* Technologies used by online formations and how they differ from previous formal organizations (e.g. companies, institutions)


Summer of Protocols: Graveyard Draft 2 July 11th 2023












________________
[1] Hurricane María was the strongest hurricane striking Puerto Rico since 1899. Public infrastructure collapsed and caused an island-wide blackout for months. According to ABC News: “it took 328 days, or roughly 11 months, for the island to restore power to all of the customers who lost it during the hurricane, which marked the longest blackout in U.S. history.”
[2] The initial count of deaths acknowledged by the Trump Administration was initially 65. In 2018, Governor Rosselló acknowledged the results of the George Washington University study which raised the estimation to 2,975 people. Another study, from Harvard University, estimates that more than 4,645 deaths occurred as a result of the Hurricane.
[3] As of July 2023, the diaspora population accounted for ~9M Puertoricans. About 10% of the local population, or ~400k people, emigrated from 2010 to 2020.
[4] The spreadsheet is still publicly available here: Puerto Rico Relief / Donation Drop-off Locations (Public)
[5]  Government officials would be informed that almost all of the local emergency supplies (83%) had already been deployed to the U.S. Virgin Islands to address issues from Hurricane Irma from two weeks prior. Pending source.
[6] The Centers of Mutual Support provide food and other resources to local residents in need. They are an evolution of a community center, focused on scale and political activism.
[7] Informal social formations include crowds, masses, publics, and social movements[dn]. Formal social formations[do] include, but are not limited to, teams, tribes, clans, organizations, institutions, corporations. Formal social formations[dp] adhere to specific communication, identity, and coordination protocols.
[8] Theorists and researchers include, but are not limited to: Gustave Le Bon, Elias Canetti, Marshal McLuhan, Blumer, Jürgen Habermas, Nancy Fraser, Michael Warner, Bernays, danah boyd, Charles Tilly, Sidney Tarrow, Alberto Melucci, Alain Touraine.
[9] Management refers to Max Boisot’s definition: “managerial coordination requires that information-bearing data flows through communication channels between centres of authority and centres of task execution.” for more information you can read “Generating knowledge in a connected world: The case of the ATLAS experiment at CERN” Management Learning 42(4) 447–457, Sage Publications
[10] Collective intelligence refers to the ability of a group or collective to solve problems, make decisions, and generate knowledge that surpasses the capabilities of individual members. Collective intelligence can be observed in various contexts, such as social insects like ants, where the collective behavior of the colony enables efficient decision-making and problem-solving. In the context of human society, collective intelligence can be seen in collaborative projects, open-source movements, and crowd wisdom.
[11] Pending: academic definition of company
[12] Pending: academic definition of crowd
[13] David F. Ronfeldt outlines the evolution into networked societies in “Institutions, Markets, and Networks: A Framework about the Evolution of Societies” (RAND, DRU-590-FF, December 1993).
[14] For more information, you can read danah boyd’s 2010 research paper "Social Network Sites as Networked Publics: Affordances, Dynamics, and Implications." In Networked Self: Identity, Community, and Culture on Social Network Sites (ed. Zizi Papacharissi), pp. 39-58.
[15] Original poster is a term referring to the initial person or agent who created the content. It is often abbreviated to “OP” when referenced on social media platforms and other forums.
[16] Under this framing, the definition of a crowd becomes the accumulation of interconnected actions with a common emotion. Notice how this definition is in contrast to a group of people with a common emotion. 
[17] The actions attributed to a person may or may not actually be theirs. The accumulation of attributed actions over time, or the avatar, becomes an interactive, social representation of a user in a virtual world or online platform. Example actions include handles (profile names), pictures, posts, and digital inventory within software like video games or social media.
[18] Pending:  https://backlinko.com/reddit-users#reddit-statistics
[19] Pending: Fandom sizes
[20] Pending: Benkler Y., Tilton C., Etling B., Roberts H., Clark J., Faris R., Kaiser J., Schmitt C. (2020). Mail-in voter fraud: Anatomy of a disinformation campaign. Berkman Center Research Publication.
[21] Pending: CryptoMama Telegram Group
[22] Other terms include, but are not limited to: squads, virtual organizations, online scenes, Twitter neighbourhoods, vibe tribes, raids, DAOs, PFP Communities, NFT Communities.
[23] There is a rich corpus of non-academic research regarding online formations. Examples include The Association of Internet Researchers, donotresearch.net, newmodels.io, and otherinter.net.
[24] Peter Limberg’s research was published in March 2023 and is publicly available on Substack at https://lessfoolish.substack.com/p/meme-to-vibe-a-philosophical-report 
[25] Cultural assets include uniform digital identifiers and memes such as hashtags, group names, digital homes, “Big Name” influencers, and ultimate aims (i.e. telos).
[26] Add link to OneDirection wiki and fandom debates
[27] Gitcoin has allocated more than $50M in funding since inception.
[28] In reality this was oligarchic since there are “whales” who hold large amount of tokens
[29] Add description of quadratic governance
[30] Probably need to add definition of quadratic voting
[31] Gold farming emerged in the early 2000s in response to massive-online-role-playing-games (MMORPGs), like World of Warcraft.
[32] Social influence farming include actions by people who are paid to like, share, or reply to social media comments. It is also done without payment by fandoms in the hopes to manipulate social algorithms and increase the visibility of the content of a target celebrity. Farms often include both human and bot actions.
[33] Crypto airdrop farms include actions by people that aim to accumulate reputation points in the hopes of a future digital asset reward. For example, users may automate a high volume of transactions in new product to fake participation, trying to be rewarded with crypto-assets by the company.
[34] Add note on child labor, international law, and terms & conditions of social media and video games.
[35] Beyond a name, group identity also includes common beliefs, values, symbols, rituals, language, norms, etiquette, goals, and interests. In the case of online formations we will focus on name persistence and consistency across observer perspectives.
[36] This behavioral spread pattern is known as complex contagion. For more information I would recommend the book by Damon Centola: “How Behavior Spreads: The Science Of Complex Contagions” Princeton University Press (2018).
[37] “The storm resulted in the deaths of 44 City residents and inflicted an estimated $19 billion in damages and lost economic activity across the New York City. Most significantly, over 69,000 residential units were damaged, and thousands of New Yorkers were temporarily displaced.” via NYC GOV: https://www.nyc.gov/site/cdbgdr/about/About%20Hurricane%20Sandy.page (Accessed July 12th 2023)
[38] Need to add social movement theory about contrasting to institutions
[39] Need to add note on social movement organizational structures
[40] Managerial structures include all communication control processes. These may be hierarchical chains of command or “flat” management systems based on consensus or democracy.
[41] From John Robbs book “Brave new war: the next stage of terrorism and the end of globalization” published by John Wiley & Sons, Inc.
[42] Add footnote about social movement framing alignment theory: Diagnostic, Prognostic, Motivational
[43] Hello fellow traveler, please collect 12 doubloons.
[44] The doubloon swarm follows all the characteristics of the previously defined archetype. The main coin of the game has shifting names (e.g. not even the spelling of “doubloon” is agreed upon) and the game itself has no agreed name. In addition, the coordination happened first on TikTok but rapidly expanded to include Facebook, Twitter, and Tumblr. Exposure to social media algorithms, which were pushing relevant memes for engagement, provided the backdrop for swarm coordination.
[45] A more comprehensive definition might be “Tools, features, systems, and methods employed to influence the formation, growth, direction, and dissipation of social groups, often with the aim to optimize participant behavior, community norms, content creation and sharing, and platform safety, all while ensuring the alignment of these behaviors with the influencer’s broader goals, such as revenue generation or political change.”
[46] Full report from the Cabinet Office “Planning the coordination of spontaneous volunteers in emergencies” via https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/828201/20190722-Planning-the-coordination-of-spontaneous-volunteers-in-emergencies_Final.pdf 
[47] The term "CAPTCHA" stands for "Completely Automated Public Turing test to tell Computers and Humans Apart." The program is designed to protect websites and other digital spaces against malicious bots by generating and grading tests that humans can pass but current computer programs cannot.
[48] Mostly achieved comment. It’s not necessary that a specific objective was reached but more so that the urgency for the promise dissipated
[49] By December 2017, the news cycle had shifted focus to what?
[a]Swarm grabs my attention. I love this word. It is not used enough. Online formation? Stick to swarm!
[b]Question that came up for me as I read this: Are we in "the swarm" (as in there is one primary or overarching swarm), or are we, through our social media actions referenced in the previous sentence, in "a swarm"?
[c]beautiful illustration
[d]These facts beg for a dangling sentence of foreshadowing analysis to me.  The compounding consequences in the next sentence didn't just come from the locations and memes, they came from the people who were empowered to engage (by locations and memes!), because .... ? It made helping easy? Fun? Exciting? How did it spread out of the bounds of the diaspora? Not all questions to answer here, but if you can allude to the mechanisms it would be satisfying. 


"Some made helping easy, some helped easily, the rapid response channeled empathy into action" ... 


Something like that? It isn't coming together quite right for me.
[e]Yeah good observation. Might need to talk through it in person! You are correct tho, I feel the gap
[f]Shout out social capital formation and its benefits! 


The transition from initial swarm to social ties that translate into real world outcomes in this example (reading sentences ahead) is super cool!
[g]This also suggests that constraining the purpose of swarms is inherently more difficult than in more structured organizations.
[h]Yes. I really should do like a full research program here. Because it's like we have so many hints but limited ethnographic study / evidence
[i]Need to rework this so that it can summarize what we're about to talk about. Missing also hints at technologies here of alignment.
[j]Formal organizations have deliberately articulated rules. At significant scale, they tend to have a constitution delineating decision making rules, special authorities, and those subject to the organization's authority.
[k]I would also note Buchanan, Coase, and Williamson on this list.
[l]Company has become the more familiar vernacular term, although the private organizational literature often uses the term "firm". 


See, e.g., Coase's "The Nature of the Firm" (1937).
[m]What do you think in terms of the term, maybe I should say "firm" throughout
[n]Maybe I could say "Private and public firms deliver..."
[o]Would Organizations fit? It includes private, public, associations, etc.
[p]Issue with "organizations" is that it's too broad -- can be almost anything (incl. a team)
[q]should this be "communication technologies"? re: the innovation is the communication technologies that enable networks to form, or is the innovation referenced the actual network?
[r]What is a social protocol?
[s]"Organizations become formalized by making organizational protocols explicit." Perhaps ?


Or is 'social protocols' a better fit than organizational protocols?
[t]mirror the next paragraph more closely "is an example of a formal social formation" might be more readable.
[u]formal social formation is such a mouthful. I might need to change it to something like "casual" and "structured" need to think about the language
[v]as a type of formal social formation? (see footnote comment!) (Type here implies that company is a subcategory, seem correct?)
[w]Yes, that's right. You also have "states" "institutions" "cooperatives" etc.
[x]These are some of the explicit social protocols that comprise a company? So things that have a brand, regulation, management, they might equal company or always equal company? 


"explicit social protocols such as; a brand, adherence to regulations, coordinated by management"
[y]In my chosen vernacular, formal social groups have been explicitly constituted. Informal social groups don't have clearly articulated secondary rules, or constitutions.
[z]Composed of? built upon?  comprised of?


Is this a logically necessary connection / definition? If so informal social groups don't 'have' implicit social protocols they 'are defined by' implicit social protocols perhaps. 


Building towards the great 'world of vibes' end of this paragraph, I think the logical structure is 


Informal social groups are defined by implicit social protocols, they are comprised of normas and behavioral contagation for coordination. 




(I am being lead that this structure, by mirroring the prior paragraph, is going to relate the following concepts to eacother intentionally: Brand & emotion, regulation & mimicry, management & broadcast (or is ad hoc doing the work?)


So this leads me to expect more on those pairings.
[aa]It is also much more difficult for crowds to exert control over their "members" when such members are not present in the crowd itself and directly subject to the normative response of fellow crowd members. The bonds of contract and jurisdiction have a temporal span that swarms cannot typically match...
[ab]Social movements don't necessarily have these things either, especially when they launch. I thought social movements (BLM, occupy wallstreet) started as swarms and ossified into social movements. If so adjusting this paragraph to imply the relation might be part of a life cycle somehow seems good.
[ac]Interestingly, from This is an Uprising, they distinguish two kinds of social movements, one more structured and deliberate and one more in the spirit of a swarm, which they call "whirlwind moments". Agree with the comment above that many social movements start as swarms and then create structures to form a cohesive movement network, primarily organized around shared purpose, explicit principles or implicit norms, and channels for communication/information sharing. Sharing an excerpt here from something I've written previously: https://docs.google.com/document/d/14B9AY7PsI7gl5Ft3i1REV6fpR1ZnARFksADm_XL1vnI/edit?usp=sharing
[ad]Asking and answering a question in the form doesn't feel in line with the high quality of the rest of your writing.
[ae]Such a good observation.
[af]Is a crowd a type of swarm? A swarm a type of crowd? Are they two different things?
[ag]I think a crowd becomes a swarm when it's networked and mediated by an algorithmic medium. So they are two different things
[ah]Neat that you include algorithmic medium, as with animals swarms they are believed to be governed by an "instinctual algorithm"
[ai](EDIT - nevermind - I see you get into "networked" below)


Also, is it worth mentioning social ties here? 


Coordination was mediated by algorithms, but also likely a good portion was mediated by existing weak and strong ties (ex. Pablo reaching out to (a presumably weak tie) Jorge and pulling him into the spreadsheet.
[aj]Can this question appear sooner? It helps situate all the background / exploratory information you give me in the first section. Even at the risk of a little repetition tossing some foreshadowing into that first paragraph including the word 'swarm' keeps people focused on the thing they're about to learn about. That seems to draw more interest in 'what are companies' that I might be otherwise tempted to skim past because I know a comparison is coming down the pipe.
[ak]Use the word diaspora here if you're going to use it up top. Maybe hammer down that diaspora means people with Puerto Rican heritage in that intro. 


I think the phrase 'In terms of participation' is weak here. The work being done is to explain that it expanded and grew. The word diffuse is associated with dilute (the amount remains the same, just spread it around).


Perhaps "The surprising cooperation lead by the diaspora rapidly grew beyond those of Puerto Rican heritage, the call to action amplified itself across social media feeds in an acceleration of impact."


Then ditch the 'as a result' in the next sentence if so.
[al]If at all possible this could be a great link to other work in the SOP - a google sheet is much like a wallet / signature / standard / connector / email in it's support of larger protocols that do good things!
[am]Maybe mention shifting identity and constrained by external sources here in some covert way. 


Small teams without an overarching identity, like Jorge and Pablo's, were acting independently yet in broad alignment; decoupled an attuned. 


Also the previous paragraph has 'no collective name/ brand' and that is close conceptually to shifting identity, so calling it out in the same language here might be nice.
[an]1 total reaction
Chance McAllister reacted with 👍 at 2023-07-24 13:12 PM
[ao]The output of these two (and their teams) depicted a swarm, but I'm wondering if their highly personal and centralized decision-making is itself also categorizable as a swarm.
[ap]So I think they were "in the swarm" and I think, with more research haha, we will find out that participants in a swarm have a really high degree of agency and centralization in execution
[aq]1 total reaction
Steve Powers reacted with 😍 at 2023-07-18 11:41 AM
[ar]Admittedly, my attention waned from this paragraph through the end of the section on Formations. I kept wanting to hear a definition of a swarm... rather than discussion on definitions of other terms as you have here and in the following paragraphs
[as]In continuing to read, honestly feel that you could just cut straight to the next section from here, by simply (and straightforwardly) defining the term "online formations" at the beginning of the following section
[at]Fair fair - I think I'm trying to "reference the academics" too hard haha. I think the proposal would be stronger if I just define the thing first and provide the reference as contrast instead of anchore
[au]yes I agree!
[av]Are the ex-ante-human-defined moderation activities of autobots sufficient to categorize a bot as "proactive"?
[aw]I think so, they contribute content based on simple scripts but are responsive. There's conversational chat where you don't even know it's a bot today
[ax]I wonder if there's a specific product name / ontology I can specify without calling out any company in particular
[ay]maybe say "boyd's use of 'public'"... found myself searching back to where "public" was referenced, as it was more than a paragraph earlier
[az]Even if authorship is preserved, how many people independently consider the identity of the reviewer once the review has auto-populated - if anything, they care about the number of other reviews as opposed to the actual identity...
[ba]Remix culture might itself merit analysis under the analytical framework you're developing here...
[bb]Why not content-centric? Is action-centric necessarily consonant with content-centric?
[bc]So I'm struggling with this actually. Because technically, people take actions which may not be "content" So think about automated smart contracts that "do a thing in the future"


So, I think technically it should be action-centric to account for content and whatever else exists. Buttttt action-centric is not as "this makes sense as a reader" as content-centric
[bd]feels like this paragraph should go after the following paragraph, after you've already introduced the term "an online formation"
[be]I don't have a better nomenclature per se, but online as a modifier seems to reduce the very real world impacts that a "digital swarm" can actually induce, as your motivating premise directly betokens...
[bf]agree with the above -- online is merely where much of the communication happens, but I see these formations as spanning both online and IRL connection, communication, and action. or maybe you are trying to distinguish "online formation" as something that only forms online? In which case would be helpful to clarify this, e.g., 'online formations are those where the catalyst for organizing originates online'
[bg]It might be interesting to in contrast consider broadcasted actions that are by definition not reducible to a single agent, even if the agents involved in the broadcast are identifiable.
[bh]Do we know how fast they are abandoned? what's the median lifecycle length
[bi]Is this information susceptible to a flow chart? Online formations seems to be the largest umbrella?
[bj]Yeah! I could do like a word cloud visual. All the "Very online" jargon circling around "Online Formations"
[bk]This seems like an essential point, it feels like it should get more treatment as a defining feature.
[bl]I'd enjoy a paragraph here leading into the next 6 paragraphs. Perhaps a sub heading. Give me the core concepts I should be paying attention to before giving me the things that support those concepts.


Here are some points I think could be in it:


The combination of task agency and lack of identity hints at a distinct archetype, unlike the previous ones we’ve discussed.


But the experience of Jorge and Pablo doesn’t align with either of these archetypes: tribes, communities, virtual organizations, or farms


Identity and collaboration structure
[bm]Yes - I'm glad to read this. I was re-reading this in the morning and realizing that this section needs to "tell the reader what they're about to read" more clearly
[bn]Is it no longer a swarm if you have clear participation boundaries dictated by an organization controller?
[bo]Yeap - under this taxonomy at least
[bp]One of the most notable features of blockchain-enabled online organizations is the ability to create actions that require the input of more than one agent (multi-sigs). Given your framing, this innovation might merit further interrogation...
[bq]So this comment here is the reason that the above pages exist? I feel like it should be the lead into talking about the above pages.


Tell me what you are going to tell me. 
Tell me.
Tell me what you told me.
[br]I just replied to an earlier comment that gave me the same exact feedback. Great stuff, will restructure a bit here
[bs]See my later comment about this
[bt]Did the participants in the Hurricane Maria swarm share an offline collective identity before the creation of the swarm? Like Puerto Rican diaspora? I wonder if the already existing shared perspective is relevant. (Apologies if you address this elsewhere.)
[bu]So the online formation is a more general category within which swarms reside alongside other sub-category types?


If so, this ontological move should be previewed earlier.
[bv]Ok cool - I think you're right. It's like I need to say "welcome to the swarm" but then go up a level to social formations and say something like "but what exactly is a swarm" then, again go to online formations and say "oh and here is why its a specific thing"
[bw]now reaching the definition for swarm 11 pages in after much curiosity and waiting, and it is derived from what it is not rather than what it is. 2 question: can you define a swarm based on what it is, rather than what it is not? And, can you get to this definition much earlier in the paper? at least as a foreshadowing for what will come next? e.g., the classic advice, "tell them what you're gonna tell them, then tell them, then tell them what you told them" -- as a reader I would have benefitted from more foreshadowing at the outset for what a swarm is and what I was going to encounter through the paper (tell them what you're gonna tell them)
[bx]It feels like the piece is actually much more taxonomical, beyond just swarms. I think you can keep what you have without cutting anything by simply making the highest-level title and the introductory gambit *not* about swarms. The piece could be about "how protocols structure online formations (or don't)" and you can use swarms as a case study..?
[by]are swarms only online? in other words, are swarms really a type of online formation?
[bz]Maybe it isn't just 'ability to apply a name' but the diversity of legitimate names used, especially named used internally to the formation.
[ca]I love this paragraph
[cb]You tend to see a fundamental governance document (or set of documents) in order to stably obtain these outputs. Hence, formally constituting as precedent to formally organizing...
[cc]Yeah, kinda like a start-up starts with a memorandum / GTM plan
[cd]A-ha! It's not just exit costs for swarms - it's also ENTRY costs!
[ce]Heh. I recall the churl who spouted this verbiage...
[cf]https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4039320
[cg]https://law.mit.edu/pub/governanceasconflict/release/1
[ch]Quite enjoyed this section, would've loved to see it earlier in the piece, as whereas the previous sections were much more theoretical, this section was much more practical
[ci]Puerto Rico isn't exactly known for high levels of government capacity. It might be useful to think through what it would have taken for a large aid organization to run this process as the counterfactual by which to assess the power of the swarm that did assemble...
[cj]Yeah I think swarms emerge in complex environments where needs are not met / opportunity. There's like a societal arbitrage.
[ck]Factions can carry a negative normative connotation. I lack a better word to hand, but wanted to note...
[cl]Complementarity with an agency of the federal government is quite interesting. Are swarms more likely to be complementary to or competitive to formally constituted organizations?
[cm]Institutions don't have a commonly accepted definition, including within the discipline itself!
[cn]Maybe I should let the academics fight over this sentence tho :D
[co]Do swarms thus either decohere or metamorphosize into an organization?
[cp]Core analytical moves like this would ideally be previewed earlier to the reader.
[cq]Ohhhh good idea, like get some foreshadowing. Maybe an exercise I could do is write out these main thesis and then cross-check they're previewed in the intro / flow
[cr]At this level of ubiquity in human social orders, there are very rarely strict gains, but instead tradeoffs associated with a given form of organization. They offer greater maneuverability and the specific expressions of power that come with that, but clearly at some kind of cost on other margins...
[cs]Yeah, I think the right framing might not be "increased maneuverability" but like "a different type"
[ct]When have guerrilla movements been especially successful (Che & Castro in Cuba?)? Do we even have a clear lens on attempted and failed guerrilla movements?
[cu]I'm relying on John Robb's expertise here, kinda building on his conclusions. So maybe let me quote him directly instead of adding a statement without my own evidence
[cv]Are swarms therefore viral?
[cw]BUT this makes them far harder to engineer or constructively influence once spurred into existence?
[cx]They weren't always imaginary...
[cy]Should I mention the ephermerality?


Eric A. Look at: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4039320
[cz]Look at "why private firms aren't democratic but some public ones are"
[da]this is a key sentence
[db]https://media.giphy.com/media/DtuVm6vCmerf2/giphy.gif
[dc]This is the corollary of one of the central premises of https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4039320
[dd]Or simply an assault on the efficiencies of centralized decision-making for many (if not most!) organizations?
[de]Which of these weaken the individual agency that is fundamentally a motive force for the formation of a given digital swarm?
[df]Damn good question.
[dg]You know, I wonder if I could rewrite the meme and close the essay with "General Swarm Enablement" and have a list of tactics, then deep-fry the meme so it looks rugged and surreal like the one at the beginning of this section
[dh]Does your average reader think they have been particularly effective in doing so thus far?
[di]Lol, maybe I should be a bit cheeky here. I could say "dubiously effective technologies" haha
[dj]opportunity for cheeky opinion here
[dk]Work in progress. This is still v0 of trying to categorise alignment technologies. However, it doesn't follow clearly from the online formation discussion.
[dl]The thing is: "the feed" is an alignment technology, like rumors and gossip. So the "ahá" moment needs to be that we can overlay alignment technologies into our current managerial tech.
[dm]Is the former more tractable to constructivist intent?
[dn]not sure I'd place social movements here... I'd argue that most successful social movements are quite deliberately (formally) organized at their core. See "How Change Happens" by Crutchfield or "This is an Uprising"
[do]could include impact networks here
[dp]This is the root of some confusion for me. (15 minutes) Formal social formations is understandable conceptually but difficult to read. I'm left understanding the formalization process is making an implicit thing explicit, and the formation to be an 'organization type' that can be formal or informal, but the shortened use of the word form leaves me working to figure out the context. A solution that comes to mind is to use 'social formations' and steer away from 'social form'